---
title: "Oomycete Data Curation"
subtitle: "Workflow for curating the ASV dataset--generating microeco & phyloseq objects, removing spurious ASVs and low-count/negative control samples."
format:
  html:
    other-links: 
      - text: Data
        href: https://doi.org/10.25573/data.16828063
execute:
  freeze: false
  keep-md: false
---

{{< include include/code/_setup.qmd >}}

{{< include include/code/_curate.qmd >}}

# Overview

On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.  

## Data 

All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.

::: {.callout-note appearance="default" icon=false}

### {{< fa download >}} &nbsp; phyloseq_oo.Rdata

{{< downloadthis files/CURATE/phyloseq_oo.Rdata dname="phyloseq_oo" label="LotuS3 phyloseq data" icon="code-slash" type="link" >}}

:::

</br>

## Citable resources 

#### Packages used in this workflow:

1. R microeco package [@liu2021microeco]. 
2. phyloseq R package [@mcmurdie2013phyloseq].
3. miaverse [@felix2024mia]

</br>

## References

::: {#refs}
:::

# Workflow

## Required Packages

```{r}
#| message: false
#| results: hide
#| code-fold: true
#| code-summary: "Click here for required R packages."

set.seed(919191)
library(microeco)
library(mia)
library(phyloseq)
library(microbiome)
```

```{r}
#| echo: false
#| eval: true
#rm(list = ls())
load("page_build/oo_curate.rdata")
```

## Review Results

The LotuS3 pipeline produces numerous output files  but for our purposes there are four specific files we are interested in:

1. `phyloseq.Rdata`
2. `OTU.fna`
3. `OTU.txt`
4. `hiera_BLAST.txt`

We will mainly work from the `phyloseq.Rdata` (renamed `phyloseq_oo.Rdata`) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis. 

```{r}
#| echo: true
#| eval: false
load("working_files/LOTUS3/oo/phyloseq_oo.Rdata")
physeq
```

```
phyloseq-class experiment-level object
otu_table()   OTU Table:         [ 12091 taxa and 41 samples ]
sample_data() Sample Data:       [ 41 samples by 14 sample variables ]
tax_table()   Taxonomy Table:    [ 12091 taxa by 7 taxonomic ranks ]
phy_tree()    Phylogenetic Tree: [ 12091 tips and 12089 internal nodes ]
```

```{bash}
#| echo: false
#| eval: true
cp working_files/LOTUS3/oo/phyloseq_oo.Rdata files/CURATE
```

```{r}
#| echo: false
#| eval: false
#| comment: this is for fixing two column names in the sample_data rather than rerunning the pipeline. 
library(speedyseq)
physeq <- physeq %>% 
      rename_sample_data(I7_Index = index) %>%
      rename_sample_data(I5_Index = index2) 
```

```{r}
#| echo: false
#| eval: false
min_read_ps <- min(readcount(physeq))
max_read_ps <- max(readcount(physeq))
total_reads_ps <- sum(readcount(physeq))
mean_reads_ps <- round(mean(readcount(physeq)), digits = 0)
median_reads_ps <- median(readcount(physeq))
total_asvs_ps <- ntaxa(physeq)
singleton_ps <- tryCatch(ntaxa(rare(physeq, detection = 1, 
                                    prevalence = 0)), 
                         error=function(err) NA)
singleton_ps_perc <- 
  tryCatch(round((100*(ntaxa(rare(physeq, detection = 1, prevalence = 0)) /
  ntaxa(physeq))), digits = 3), error=function(err) NA)
sparsity_ps <- 
  round(length(which(abundances(physeq) == 0))/
          length(abundances(physeq)), digits = 3)

ds_metrics <- data.frame(rbind(min_read_ps, max_read_ps, total_reads_ps, 
                               mean_reads_ps, median_reads_ps, total_asvs_ps, 
                               singleton_ps, singleton_ps_perc, sparsity_ps))
```

| Metric                              | Results                             |
|-------------------------------------|-------------------------------------|
| Min. number of reads                | `r ds_metrics[1, ]`                 |
| Max. number of reads                | `r ds_metrics[2, ]`                 |
| Total number of reads               | `r ds_metrics[3, ]`                 |
| Average number of reads             | `r ds_metrics[4, ]`                 |
| Median number of reads              | `r ds_metrics[5, ]`                 |
| Total ASVs                          | `r ds_metrics[6, ]`                 |
| Number of singleton ASVs            | `r ds_metrics[7, ]`                 |
| Any ASVs sum to 1 or less?          | `r isTRUE(ds_metrics[7, ] >= 1)`    |
| Percent of ASVs that are singletons | `r ds_metrics[8, ]`                 |
| Sparsity                            | `r ds_metrics[9, ]`                 |

In this next part of the workflow the goal is to create a *microtable object* using the R package [microeco](https://chiliubio.github.io/microeco_tutorial/) [@liu2021microeco]. The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.

We will also:

-   Remove any ASVs without kingdom level classification.\
-   Revome any contaminants (chloroplast, mitochondria, etc.).\
-   Remove Negative Control (NC) samples.\
-   Remove any low-count samples.

## Sample Data

Before we begin, let's create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:

| Header         | Description                                             |
|----------------|---------------------------------------------------------|
| `SampleID`     | New sample ID based on plot, depth, treatment, & pair   |
| `Plot`         | Experimental plot ID                                    |
| `Treatment`    | Control vs warming                                      |
| `Temp`         | Warming treatment temperature                           |
| `Depth`        | Sampling depth in soil                                  |
| `Pairing`      | Sample pairings                                         |
| `raw_rc`       | No. of raw reads                                        |
| `cutadapt_rc`  | Read count after cutdapt                                |
| `final_rc`     | Read count after LotuS3 pipeline                        |
| `no_asv`       | No. of ASVs                                             |

<br/>

Now I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow. 

```{r}
#| echo: true
tmp_otu <- data.frame(otu_table(physeq))
tmp_otu_sum <- data.frame(colSums(tmp_otu > 0)) %>%
  tibble::rownames_to_column("SampleID")
names(tmp_otu_sum)[2] <- "no_asv"
tmp_read_sum <- data.frame(colSums(tmp_otu)) %>%
  tibble::rownames_to_column("SampleID")
names(tmp_read_sum)[2] <- "final_rc"
tmp_track_reads <- read.table(
    "files/CUTADAPT/oo_cutadapt_track.txt",
    header = TRUE, sep = "\t"
)

tmp_track_reads$SampleID <- stringr::str_replace_all(
  tmp_track_reads$SampleID, "-", "_")
names(tmp_track_reads)[2] <- "raw_rc"
names(tmp_track_reads)[3] <- "cutadapt_rc"
```

Then I want to merge this data with the sample metadata. 

```{r}
#| echo: true
tmp_samp <- data.frame(sample_data(physeq)) %>% 
  tibble::rownames_to_column("SampleID")

sample_tab <- dplyr::left_join(tmp_samp, tmp_track_reads, by = "SampleID") %>% 
  dplyr::left_join(., tmp_read_sum, by = "SampleID") %>% 
  dplyr::left_join(., tmp_otu_sum, by = "SampleID") 
sample_tab <- sample_tab %>%
  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, 
                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)
```

```{r}
readr::write_delim(sample_tab, "files/CURATE/oo_sample_data_full.txt",
    delim = "\t")

sample_tab_trim <- sample_tab[, -c(11:19)]
sample_tab_trim$per_reads_kept <- 
  round(sample_tab_trim$final_rc/sample_tab_trim$raw_rc, digits = 3)
sample_tab_trim <- sample_tab_trim %>% 
  dplyr::relocate(per_reads_kept, .before = no_asv)
```

</br>

```{r}
#| echo: false
#| eval: true
#| label: tbl-make-metadata-table-oo
#| tbl-cap: "Sample metadata including read changes at start and end of workflow."
make_metadata_table(sample_tab_trim)
```

```{r}
#| echo: false
#| eval: false
readr::write_delim(sample_tab_trim, "files/CURATE/oo_sample_data_with_rc.txt",
    delim = "\t")
```

</br>

{{< downloadthis files/CURATE/oo_sample_data_with_rc.txt dname=oo_sample_data_with_rc label="Download sample metadata" icon=table type=primary class=data-button id=shrimp_md >}}

## Prep Data for microeco

Like any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section,  [microeco](https://chiliubio.github.io/microeco_tutorial/basic-class.html#prepare-the-example-data) tutorial.

### A. Taxonomy Table

Here is what the taxonomy table looks like in the dataset.

```{r}
#| eval: false
#| echo: true
tmp_tax <- data.frame(tax_table(physeq))
tmp_tax[1:6, 1:4]
```

```         
       Domain Phylum     Class          Order
ASV163 Fungi  Ascomycota Eurotiomycetes Chaetothyriales
ASV118 Fungi  Ascomycota Eurotiomycetes Chaetothyriales
ASV86  ?      ?          ?              ?
ASV179 ?      ?          ?              ?
ASV132 Fungi  Ascomycota Eurotiomycetes Chaetothyriales
ASV156 Fungi  Ascomycota Eurotiomycetes Chaetothyriales
```

Next we need to add rank definitions (e.g., `k__`, `p__`, etc.) to each classification. 

```{r}
#| echo: true
#| eval: false
tmp_tax <- data.frame(tax_table(physeq))   
tmp_tax <- tmp_tax %>% dplyr::rename("Kingdom" = "Domain")
tmp_tax <- tmp_tax %>%
  dplyr::mutate_all(~stringr::str_replace_all(., "\\?", ""))

tmp_tax$Kingdom <- paste("k", tmp_tax$Kingdom, sep = "__")
tmp_tax$Phylum <- paste("p", tmp_tax$Phylum, sep = "__")
tmp_tax$Class <- paste("c", tmp_tax$Class, sep = "__")
tmp_tax$Order <- paste("o", tmp_tax$Order, sep = "__")
tmp_tax$Family <- paste("f", tmp_tax$Family, sep = "__")
tmp_tax$Genus <- paste("g", tmp_tax$Genus, sep = "__")
tmp_tax$Species <- paste("s", tmp_tax$Species, sep = "__")
```

And now the final, modified taxonomy table.

```{r}
#| echo: true
#| eval: false
tmp_tax[1:6, 1:4]
```

```
       Kingdom  Phylum        Class             Order
ASV163 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales
ASV118 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales
ASV86  k__      p__           c__               o__
ASV179 k__      p__           c__               o__
ASV132 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales
ASV156 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales
```

### B. Sequence Table

Here is what the sequence table looks like in the dataset.

```{r}
#| eval: false
#| echo: true
tmp_otu[1:6, 1:3]
```

```
       P00_D00_000_NNN P01_D00_010_W4A P01_D10_020_W4A
ASV163               0               0               0 
ASV118               0               0               0 
ASV86                0               0               0 
ASV179               0               0               0 
ASV132               0               0               0 
ASV156               0               0               0 
```

```{r}
#| echo: true
#| eval: false
identical(row.names(tmp_otu), row.names(tmp_tax))
```

```
[1] TRUE
```

### C. Sample Table

Here is what the sample table looks like.

```{r}
#| eval: true
#| echo: true
sample_tab[1:6, 1:6]
```

```{r}
#| echo: true
#| eval: false
sample_tab <- sample_tab %>% tibble::column_to_rownames("SampleID")
sample_tab$SampleID <- rownames(sample_tab)
sample_tab <- sample_tab %>% dplyr::relocate(SampleID)
```

## Create a Microtable Object

With these three files in hand we are now ready to create a [microtable object](https://chiliubio.github.io/microeco_tutorial/basic-class.html).

::: callout-tip
A [microtable object](https://chiliubio.github.io/microeco_tutorial/basic-class.html) contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV. 
:::

```{r}
#| echo: true
#| eval: false
sample_info <- sample_tab
tax_tab <- tmp_tax
otu_tab <- tmp_otu
```

```{r}
#| echo: true
#| eval: false
tmp_me <- microtable$new(sample_table = sample_info, 
                         otu_table = otu_tab, 
                         tax_table = tax_tab)
tmp_me
```

```         
2 samples with 0 abundance are removed from the otu_table ...

microtable-class object:
sample_table have 41 rows and 11 columns
otu_table have 12091 rows and 41 columns
tax_table have 12091 rows and 7 columns
```

### D. Add Representative Sequence

We can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.

```{r}
#| echo: true
#| eval: false
rep_fasta <- Biostrings::readDNAStringSet("working_files/LOTUS3/oo/OTU.fna")
identical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))
```

```
[1] TRUE
```

```{r}
#| echo: true
#| eval: false
tmp_fasta_names <- names(rep_fasta)
tmp_rep_fasta <- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]
rep_fasta <- tmp_rep_fasta
tmp_me$rep_fasta <- rep_fasta
identical(row.names(tmp_me$tax_table), names(rep_fasta))
tmp_me$tidy_dataset()
```

```
[1] TRUE
```

```{r}
#| echo: true
#| eval: false
me_asv_raw <- microeco::clone(tmp_me)
me_asv_raw
```

```{r}
#| echo: false
#| eval: true
me_asv_raw
```

> The microeco object `me_asv_raw` contains all data from the LotuS3 pipeline, before any dataset curation. 

## Curate the Data Set

Pretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.

### Remove non-fungal ASVs

First we can have a quick look at the kingdom  level classifications to see what we are dealing with. 

```{r}
#| echo: true
#| eval: true
tmp_k <- me_asv_raw$tax_table %>%
      distinct(Kingdom, .keep_all = TRUE)
tmp_p <- me_asv_raw$tax_table %>%
      distinct(Phylum, .keep_all = TRUE)
```

```{r}
#| echo: false
#| eval: true
#| message: true
#| results: markup
message("-------KINGDOM------------------")
cat(tmp_k$Kingdom, sep = "\n")

message("-------PHYLUM------------------")
cat(tmp_p$Phylum, sep = "\n")
```

1. The first thing to notice is that at the kingdom level we have Fungi, unclassified (or NA), Stramenopila, some non-fungal classifications. We can remove anything that is not classified as Fungi at the kingdom level or Oomycota at the phylum level.  

The easiest thing to do is keep everything we want--in other words, all fungal taxa.

```{r}
#| echo: true
#| eval: false
tmp_no_na <- microeco::clone(tmp_me)
tmp_no_na$tax_table %<>% 
  base::subset(Kingdom == "k__Fungi" | Phylum == "p__Oomycota")
tmp_no_na$tidy_dataset()
```

```{r}
#| echo: true
#| eval: false
me_asv_no_na <- microeco::clone(tmp_no_na)
me_asv_no_na
```

```{r}
#| echo: false
#| eval: true
me_asv_no_na
```

> The microeco object `me_asv_no_na` conatins only ASVs calssified as Archaea or Bacteria. 

### Remove Negative Controls (NC)

Now we need to remove the NC samples *and* ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.

```{r}
#| echo: true
#| eval: false
tmp_nc <- microeco::clone(tmp_no_na)
tmp_nc$sample_table <- subset(tmp_nc$sample_table, Treatment == "Negative")
tmp_nc$tidy_dataset()
```

```
89 taxa with 0 abundance are removed from the otu_table ...
```

```{r}
#| echo: true
#| eval: false
me_asv_nc <- microeco::clone(tmp_nc)
me_asv_nc
```

```{r}
#| echo: false
#| eval: true
me_asv_nc
```

> The microeco object `me_asv_nc` conatins only the negative control sample(s) and associated ASVs. 

Looks like there are `r nrow(me_asv_nc$tax_table)` ASVs in the NC samples from a total of `r sum(me_asv_nc$taxa_sums())` reads.

```{r}
#| echo: true
#| eval: false
nc_asvs <- row.names(tmp_nc$tax_table)
nc_asvs
```

```{r}
#| echo: false
#| eval: true
length(nc_asvs)
```

There are `r length(nc_asvs)` ASVs found in the NC sample. 

Essentially, for each ASV, the code below calculates:

-   The total number of NC samples containing at least 1 read.\
-   The total number of reads in NC samples.\
-   The total number of non-NC samples containing at least 1 read.\
-   The total number of reads in non-NC samples.\
-   The percent of reads in the NC samples and the percent of NC samples containing reads.

```{r}
#| echo: true
#| eval: false
tmp_rem_nc <- microeco::clone(tmp_no_na)
tmp_rem_nc_df <- tmp_rem_nc$otu_table
tmp_rem_nc_df <- tmp_rem_nc_df %>% 
                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)
tmp_rem_nc_df <- tmp_rem_nc_df %>% tibble::rownames_to_column("ASV_ID")
```

```{r}
#| echo: true
#| eval: false
tmp_rem_nc_df <- tmp_rem_nc_df  %>% 
  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains("NNN"))), 
         .after = "ASV_ID")
tmp_rem_nc_df <- dplyr::select(tmp_rem_nc_df, -contains("NNN"))
tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), 
                .after = "total_reads_NC")
tmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] <- list(NULL)
tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(perc_in_neg = 100*(
    total_reads_NC / (total_reads_NC + total_reads_samps)),
                .after = "total_reads_samps")
```

```{r}
#| echo: true
#| eval: false
tmp_rem_nc_df$perc_in_neg <- round(tmp_rem_nc_df$perc_in_neg, digits = 6)

tmp_1 <- data.frame(rowSums(tmp_rem_nc$otu_table != 0))
tmp_1 <- tmp_1 %>% tibble::rownames_to_column("ASV_ID")
tmp_1 <- tmp_1 %>% dplyr::rename("total_samples" = 2)  

tmp_2 <- dplyr::select(tmp_rem_nc$otu_table, contains("NNN"))
tmp_2$num_samp_nc <- rowSums(tmp_2 != 0)
tmp_2 <- dplyr::select(tmp_2, contains("num_samp_nc"))
tmp_2 <- tmp_2 %>% tibble::rownames_to_column("ASV_ID")

tmp_3 <- dplyr::select(tmp_rem_nc$otu_table, -contains("NNN"))
tmp_3$num_samp_no_nc <- rowSums(tmp_3 != 0)
tmp_3 <- dplyr::select(tmp_3, contains("num_samp_no_nc"))
tmp_3 <- tmp_3 %>% tibble::rownames_to_column("ASV_ID")

tmp_rem_nc_df <- dplyr::left_join(tmp_rem_nc_df, tmp_1) %>%
                 dplyr::left_join(., tmp_2) %>%
                 dplyr::left_join(., tmp_3)

tmp_rem_nc_df <- tmp_rem_nc_df %>%
  dplyr::mutate(
    perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),
    .after = "num_samp_no_nc")
```

```{r}
#| echo: true
#| eval: false
nc_check <- tmp_rem_nc_df
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-make-asv-in-nc-table-oo
#| tbl-cap: "Summary of ASVs detected in Negative Control (NC) samples."
make_asv_in_nc_table(nc_check)
```

```{r}
#| echo: false
#| eval: false
write_delim(nc_check, "files/CURATE/oo_asv_in_nc_samples.txt",
    delim = "\t")
```

{{< downloadthis files/CURATE/oo_asv_in_nc_samples.txt dname=oo_asv_in_nc_samples label="Download ASVs in NC samples" icon=table type=primary class=data-button id=asv_in_nc_samples >}}

Looking at these data we can see the NC sample only has 1 read from each of the two ASVs. We can simply ignore these ASVs and just remove the NC sample. We will still go through the process...

```{r}
#| echo: true
#| eval: false
nc_remove <- nc_check %>% 
  dplyr::filter(perc_in_neg > 10 | num_samp_no_nc < 4)
```

```{r}
#| echo: false
#| eval: false
nc_remain <- dplyr::anti_join(nc_check, nc_remove)

rem_nc_reads <- sum(nc_remove$total_reads_NC)
rem_sam_reads <- sum(nc_remove$total_reads_samps)
per_reads_rem <- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), 
                       digits = 3)

ret_nc_reads <- sum(nc_remain$total_reads_NC)
ret_sam_reads <- sum(nc_remain$total_reads_samps)
per_reads_ret <- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), 
                       digits = 3)
```

|          | Total ASVs          | NC reads         | non NC reads      | \% NC reads       |
|----------|---------------------|------------------|-------------------|-------------------|
| Removed  | `r nrow(nc_remove)` | `r rem_nc_reads` | `r rem_sam_reads` | `r per_reads_rem` |
| Retained | `r nrow(nc_remain)` | `r ret_nc_reads` | `r ret_sam_reads` | `r per_reads_ret` |

We identified a total of `r nrow(nc_check)` ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed `r nrow(nc_remove)` ASVs from the data set, which represented `r rem_nc_reads` total reads in NC samples and `r rem_sam_reads` total reads in non-NC samples. Of the total reads removed `r per_reads_rem`% came from NC samples. Of all ASVs identified in NC samples,`r nrow(nc_remain)` were retained because the fell below the threshhold criteria. These ASVs accounted for `r ret_nc_reads` reads in NC samples and `r ret_sam_reads` reads in non-NC samples. NC samples accounted for `r per_reads_ret`% of these reads.

OK, now we can remove the NC samples and any ASVs that met our criteria described above.

```{r}
#| echo: true
#| eval: false
tmp_no_nc <- microeco::clone(tmp_no_na)

tmp_rem_asv <- as.factor(nc_remove$ASV_ID)
tmp_no_nc$otu_table <- tmp_rem_nc$otu_table %>% 
  dplyr::filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)
tmp_no_nc$tidy_dataset()

tmp_no_nc$sample_table <- subset(tmp_no_nc$sample_table, 
                                 Treatment != "Negative")
tmp_no_nc$tidy_dataset()
```

```{r}
#| echo: true
#| eval: false
me_asv_no_nc <- microeco::clone(tmp_no_nc)
me_asv_no_nc
```

```{r}
#| echo: false
#| eval: true
me_asv_no_nc
```

> The microeco object `me_asv_no_nc` does not conatins the negative control sample(s). 

### Remove Low-Count Samples

Next, we can remove samples with really low read counts. I know from looking at these data that the are many low coverage samples. We can take a look at different thresholds to see how many samples will be eliminated. 

```{r}
#| echo: true
#| eval: false
samp_sum <- data.frame(me_asv_no_nc$sample_sums())
```

```{r}
#| echo: false
#| eval: true
#| results: markup
#| message: true
cat(c("--- <1000 reads ---", length(samp_sum[samp_sum[1] < 1000, ]), " samples", sep = "\n"))
cat(c("--- <500 reads ---", length(samp_sum[samp_sum[1] < 500, ]), " samples", sep = "\n"))
cat(c("--- <100 reads ---", length(samp_sum[samp_sum[1] < 100, ]), " samples", sep = "\n"))
```

We can start by eliminating samples with fewer than 100 reads. 

```{r}
#| echo: true
#| eval: false
tmp_no_low <- microeco::clone(tmp_no_nc)
```

```{r}
tmp_no_low$otu_table <- tmp_no_nc$otu_table %>%
          dplyr::select(where(~ is.numeric(.) && sum(.) > 100))
tmp_no_low$tidy_dataset()
```

```
3 taxa with 0 abundance are removed from the otu_table ...
```

Check if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).

```{r}
tmp_row_sums <- rowSums(tmp_no_low$otu_table)
any(tmp_row_sums == 0)
```

```
[1] FALSE
```

```{r}
#| echo: true
#| eval: false
me_asv_no_low <- microeco::clone(tmp_no_low)
me_asv_no_low
```

```{r}
#| echo: false
#| eval: true
me_asv_no_low
```

> The microeco object `me_asv_no_low` does not conatins the low abundance samples. 

Giving us the final microtable object.

```{r}
#| echo: true
#| eval: false
me_asv <- microeco::clone(me_asv_no_low)
```

```{r}
#| echo: false
#| eval: true
me_asv
```

# Summary

Now time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia]. First we do a little formatting to get our data compatible with mia.

```{r}
#| echo: false
#| eval: false
identical(rownames(me_asv_raw$sample_table), colnames(me_asv_raw$otu_table))
identical(rownames(me_asv$sample_table), colnames(me_asv$otu_table))
```


```{r}
#| echo: true
#| eval: false
# https://github.com/microbiome/OMA/issues/202
tmp_counts <- as.matrix(me_asv_raw$otu_table)
tmp_assays <-  SimpleList(counts = tmp_counts)
mia_me_asv_raw <- TreeSummarizedExperiment(assays = tmp_assays,
                                colData = DataFrame(me_asv_raw$sample_table),
                                rowData = DataFrame(me_asv_raw$tax_table))
rm(list = ls(pattern = "tmp_"))
tmp_counts <- as.matrix(me_asv$otu_table)
tmp_assays <-  SimpleList(counts = tmp_counts)
mia_me_asv <- TreeSummarizedExperiment(assays = tmp_assays,
                                colData = DataFrame(me_asv$sample_table),
                                rowData = DataFrame(me_asv$tax_table))
mia_me_asv_raw_summ <- summary(mia_me_asv_raw, assay.type = "counts")
mia_me_asv_summ <- summary(mia_me_asv, assay.type = "counts")
rm(list = ls(pattern = "tmp_"))
```

```{r}
objs <- c("me_asv_raw", "me_asv_no_na", "me_asv_no_cont", 
          "me_asv_no_nc", "me_asv_no_low")

pipe_summary <- summarize_objs(objs)
print(pipe_summary)
```

Here we can see how the total number of ASVs, reads, and samples changed during the curation process. 

```{r}
#| echo: false
#| eval: true
knitr::kable(pipe_summary)
```

And some additional metrics describing the changes. 

| Metric                | Start                                      | End                                    |
|-----------------------|--------------------------------------------|----------------------------------------|
| Min. number reads     | `r mia_me_asv_raw_summ$samples[2]`         | `r mia_me_asv_summ$samples[2]`         |
| Max. number reads     | `r mia_me_asv_raw_summ$samples[3]`         | `r mia_me_asv_summ$samples[3]`         |
| Total number reads    | `r mia_me_asv_raw_summ$samples[1]`         | `r mia_me_asv_summ$samples[1]`         |
| Avg number reads      | `r round(mia_me_asv_raw_summ$samples[5])`  | `r round(mia_me_asv_summ$samples[5])`  |
| Median no. reads      | `r mia_me_asv_raw_summ$samples[4]`         | `r mia_me_asv_summ$samples[4]`         |
| Total ASVs            | `r mia_me_asv_raw_summ$features[1]`        | `r mia_me_asv_summ$features[1]`        |
| No. singleton ASVs    | `r mia_me_asv_raw_summ$features[2]`        | `r mia_me_asv_summ$features[2]`        |
| Avg ASVs per sample.  | `r round(mia_me_asv_raw_summ$features[3])` | `r round(mia_me_asv_summ$features[3])` |

We started off with `r nrow(me_asv_raw$tax_table)` ASVs and `r nrow(me_asv_raw$sample_table)` samples. Screening for `NA` kingdom assignment removed an additional `r nrow(me_asv_raw$tax_table) - nrow(me_asv_no_na$tax_table)` ASVs. After removing the negative controls there were `r nrow(me_asv_no_nc$tax_table)` ASVs and `r nrow(me_asv_no_na$sample_table)`. After removing low-count samples, there were `r nrow(me_asv$tax_table)` ASVs and `r nrow(me_asv$sample_table)` samples remaining.

```{r}
#| echo: false
#| eval: false
#---------Starting data-------------#
tmp_start <- microeco::clone(me_asv_raw)
tmp_start_rc <- data.frame(tmp_start$sample_sums()) %>% 
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("start_rc" = 2)

tmp_start_asv <- data.frame(t(tmp_start$otu_table))
tmp_start_asv <- data.frame(rowSums(tmp_start_asv > 0)) %>%
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("start_asv" = 2)

#---------Final data-------------#
tmp_final <- microeco::clone(me_asv)
tmp_final_rc <- data.frame(tmp_final$sample_sums()) %>% 
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("final_rc" = 2)

tmp_final_asv <- data.frame(t(tmp_final$otu_table))
tmp_final_asv <- data.frame(rowSums(tmp_final_asv > 0)) %>%
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("final_asv" = 2)

#---------Summary data-------------#
tmp_summary <- dplyr::full_join(tmp_start_rc, tmp_start_asv) %>%
  dplyr::left_join(., tmp_final_rc) %>%
  dplyr::left_join(., tmp_final_asv)

readr::write_delim(tmp_summary, 
                   "files/CURATE/oo_final_summary.txt", 
                   delim = "\t"
)

curate_summary <- tmp_summary
removed_samples <- curate_summary$SampleID[is.na(curate_summary$final_rc)]
```

```{r}
#| message: false
#| echo: false
#| eval: true
#| label: tbl-make-final-rc-asv-table-oo
#| tbl-cap: "Beginning and ending read counts & total ASVs by sample."
make_final_asv_rc_table(curate_summary)
```

</br>

We lost `r length(removed_samples)` samples after curating the dataset. 

```{r}
#| message: false
#| echo: false
#| eval: true
print(removed_samples)
```


```{r}
#| echo: false
#| eval: false
#| comment: the following code blocks are for exporting data
```

```{bash}
#| echo: false
#| eval: false
mkdir -p files/SHARE/oo_curated_data/
```

```{r}
#| echo: false
#| eval: false
tmp_path <- "files/SHARE/oo_curated_data/"
tmp_otu_n <- "oo_otu_table.txt"
tmp_tax_n <- "oo_tax_table.txt"
tmp_sam_n <- "oo_sample_table.txt"
tmp_rep_n <- "oo_rep_fasta.fasta"
tmp_me_n <- "oo_me_asv.rds"
tmp_ps_n <- "oo_ps_asv.rds"
```

```{r}
#| echo: false
#| eval: false
tmp_otu <- me_asv$otu_table %>% tibble::rownames_to_column("ASV_ID")
write_delim(tmp_otu, paste(tmp_path, tmp_otu_n), delim = "\t")

tmp_tax <- me_asv$tax_table %>% tibble::rownames_to_column("ASV_ID")
write_delim(tmp_tax, paste(tmp_path, tmp_tax_n), delim = "\t")

write_delim(sample_tab, paste(tmp_path, tmp_sam_n), delim = "\t")

write.fasta(
  sequences = as.list(as.character(me_asv$rep_fasta)),
  names = names(me_asv$rep_fasta),
  file.out = paste(tmp_path, tmp_rep_n)
)
saveRDS(me_asv, paste(tmp_path, tmp_me_n))
ps_asv <- file2meco::meco2phyloseq(me_asv)
saveRDS(ps_asv, paste(tmp_path, tmp_ps_n))
```

```{bash}
#| echo: false
#| eval: false
cd files/SHARE
zip -r oo_curated_data.zip oo_curated_data/
rm -r oo_curated_data/
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(ds_metrics, sample_tab, sample_info, tax_tab, otu_tab, samp_sum,
            me_asv_raw, me_asv_no_na, me_asv_no_cont, me_asv_nc, sample_tab_trim, 
            me_asv_no_nc, me_asv_no_low, me_asv, nc_asvs, nc_check, 
            nc_remove, nc_remain, rem_nc_reads, rem_sam_reads, per_reads_rem, 
            per_reads_ret, ret_nc_reads, ret_sam_reads, mia_me_asv_raw, ps_asv,
            mia_me_asv, mia_me_asv_raw_summ, mia_me_asv_summ, pipe_summary,
            curate_summary, removed_samples, 
            sure = TRUE)
rm(list = ls(pattern = "tmp_"))
save.image("page_build/oo_curate.rdata")
saveRDS(me_asv, "files/CURATE/oo_me_asv.rds")
saveRDS(ps_asv, "files/CURATE/oo_ps_asv.rds")
objects()
```


{{< include include/code/_footer.qmd >}}
