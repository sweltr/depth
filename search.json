[
  {
    "objectID": "primers.html",
    "href": "primers.html",
    "title": "Primer Removal",
    "section": "",
    "text": "This workflow uses cutadapt to remove primer sequences from the four amplicon datasets. Included are links to R scripts, associated processing files, and access to raw fastq files.\nHere is the basic rundown for renaming samples and removing primer pairs. All of the data and scripts you need can be downloaded below.\n\nDownload raw fastq files from figshare using the links in the table below.\nRename fastq files using the rename.sh script and the corresponding lookup table.\nRun the corresponding cutadapt R script to remove primers.\n\nAt this point your samples are ready for ASV calling. For a more detailed explaination of the workflow and access to summary data please see the Workflow section below.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity and consistency we use the abbreviation SSU or ssu to refer to the 16S rRNA dataset. We recognize that the small subunit (SSU) applies to multiple domains but here we specifically refer to Bacteria and Archaea. Further we use the abbreviations ITS or its for general fungi, AMF or amf for Arbuscular Mycorrhizal Fungi, and OO or oo for Oomycetes.\n\n\n\nHere is everything you need to run this workflow–raw data files, fastq renaming scripts and lookup tables, and cutadapt R scripts.\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\n16S rRNA (ssu)\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS (its)\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF (amf)\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete (oo)\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nYou can also download the scripts and lookup tables for each dataset from figshare.\n\n\nPrimer pairs used for each dataset along with primer names and associated references.\n\n\n\n\n\n\nDataset\nForward primer\nReverse Primer\n\n\n\nSSU\nGTGCCAGCMGCCGCGGTAA  515f (Caporaso et al. 2011)\n\nGGACTACHVGGGTWTCTAAT  806r (Caporaso et al. 2011)\n\n\n\nITS\nCTTGGTCATTTAGAGGAAGTAA  ITS1f (Gardes and Bruns 1993)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\nAMF\nAAGCTCGTAGTTGAATTTCG  AMV4-5NF (Sato et al. 2005)\n\nCCCAACTATCCCTATTAATCAT  AMDGR (Sato et al. 2005)\n\n\n\nOomycete\nGGAAGGATCATTACCACA  ITS1oo (Riit et al. 2016)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\n\n\n\n\ncutadapt (Martin 2011)\n\n\n\n\n\n\nCaporaso, J Gregory, Christian L Lauber, William A Walters, Donna Berg-Lyons, Catherine A Lozupone, Peter J Turnbaugh, Noah Fierer, and Rob Knight. 2011. “Global Patterns of 16S rRNA Diversity at a Depth of Millions of Sequences Per Sample.” Proceedings of the National Academy of Sciences 108: 4516–22. https://doi.org/10.1073/pnas.1000080107.\n\n\nGardes, Monique, and Thomas D Bruns. 1993. “ITS Primers with Enhanced Specificity for Basidiomycetes-Application to the Identification of Mycorrhizae and Rusts.” Molecular Ecology 2 (2): 113–18. https://doi.org/10.1111/j.1365-294X.1993.tb00005.x.\n\n\nMartin, Marcel. 2011. “Cutadapt Removes Adapter Sequences from High-Throughput Sequencing Reads.” EMBnet. Journal 17 (1): 10–12. https://doi.org/10.14806/ej.17.1.200.\n\n\nRiit, Taavi, Leho Tedersoo, Rein Drenkhan, Eve Runno-Paurson, Harri Kokko, and Sten Anslan. 2016. “Oomycete-Specific ITS Primers for Identification and Metabarcoding.” MycoKeys 14: 17–30. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nSato, Kouichi, Yoshihisa Suyama, Masanori Saito, and Kazuo Sugawara. 2005. “A New Primer for Discrimination of Arbuscular Mycorrhizal Fungi with Polymerase Chain Reaction-Denature Gradient Gel Electrophoresis.” Grassland Science 51 (2): 179–81. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nWhite, Thomas J, Thomas Bruns, SJWT Lee, John Taylor, et al. 1990. “Amplification and Direct Sequencing of Fungal Ribosomal RNA Genes for Phylogenetics.” PCR Protocols: A Guide to Methods and Applications 18 (1): 315–22.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#raw-data-scripts",
    "href": "primers.html#raw-data-scripts",
    "title": "Primer Removal",
    "section": "",
    "text": "Here is everything you need to run this workflow–raw data files, fastq renaming scripts and lookup tables, and cutadapt R scripts.\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\n16S rRNA (ssu)\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS (its)\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF (amf)\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete (oo)\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nYou can also download the scripts and lookup tables for each dataset from figshare.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#citable-resources",
    "href": "primers.html#citable-resources",
    "title": "Primer Removal",
    "section": "",
    "text": "Primer pairs used for each dataset along with primer names and associated references.\n\n\n\n\n\n\nDataset\nForward primer\nReverse Primer\n\n\n\nSSU\nGTGCCAGCMGCCGCGGTAA  515f (Caporaso et al. 2011)\n\nGGACTACHVGGGTWTCTAAT  806r (Caporaso et al. 2011)\n\n\n\nITS\nCTTGGTCATTTAGAGGAAGTAA  ITS1f (Gardes and Bruns 1993)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\nAMF\nAAGCTCGTAGTTGAATTTCG  AMV4-5NF (Sato et al. 2005)\n\nCCCAACTATCCCTATTAATCAT  AMDGR (Sato et al. 2005)\n\n\n\nOomycete\nGGAAGGATCATTACCACA  ITS1oo (Riit et al. 2016)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\n\n\n\n\ncutadapt (Martin 2011)",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#references",
    "href": "primers.html#references",
    "title": "Primer Removal",
    "section": "",
    "text": "Caporaso, J Gregory, Christian L Lauber, William A Walters, Donna Berg-Lyons, Catherine A Lozupone, Peter J Turnbaugh, Noah Fierer, and Rob Knight. 2011. “Global Patterns of 16S rRNA Diversity at a Depth of Millions of Sequences Per Sample.” Proceedings of the National Academy of Sciences 108: 4516–22. https://doi.org/10.1073/pnas.1000080107.\n\n\nGardes, Monique, and Thomas D Bruns. 1993. “ITS Primers with Enhanced Specificity for Basidiomycetes-Application to the Identification of Mycorrhizae and Rusts.” Molecular Ecology 2 (2): 113–18. https://doi.org/10.1111/j.1365-294X.1993.tb00005.x.\n\n\nMartin, Marcel. 2011. “Cutadapt Removes Adapter Sequences from High-Throughput Sequencing Reads.” EMBnet. Journal 17 (1): 10–12. https://doi.org/10.14806/ej.17.1.200.\n\n\nRiit, Taavi, Leho Tedersoo, Rein Drenkhan, Eve Runno-Paurson, Harri Kokko, and Sten Anslan. 2016. “Oomycete-Specific ITS Primers for Identification and Metabarcoding.” MycoKeys 14: 17–30. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nSato, Kouichi, Yoshihisa Suyama, Masanori Saito, and Kazuo Sugawara. 2005. “A New Primer for Discrimination of Arbuscular Mycorrhizal Fungi with Polymerase Chain Reaction-Denature Gradient Gel Electrophoresis.” Grassland Science 51 (2): 179–81. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nWhite, Thomas J, Thomas Bruns, SJWT Lee, John Taylor, et al. 1990. “Amplification and Direct Sequencing of Fungal Ribosomal RNA Genes for Phylogenetics.” PCR Protocols: A Guide to Methods and Applications 18 (1): 315–22.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#required-packages",
    "href": "primers.html#required-packages",
    "title": "Primer Removal",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick to see required packagesset.seed(919191)\nlibrary(dada2)\nlibrary(ShortRead)\nlibrary(Biostrings)\nlibrary(DECIPHER)",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#rename-fastq-files",
    "href": "primers.html#rename-fastq-files",
    "title": "Primer Removal",
    "section": "Rename fastq Files",
    "text": "Rename fastq Files\nThe first thing we did in this workflow is rename all fastq files so that the names are more informative. Here we generate sample names that contain the sample plot information, the soil depth, warming treatment, and sample pairing. To accomplish this we use a two column tab-delimited file (e.g., ssu_rename.txt) where the first column contains the original name (e.g., SWELTR-9_R1.fastq.gz) and the second column contains the new name (e.g., P03-D00-010-W4B_R1.fastq.gz). You can download this table above.\nThis handy bash script will rename all fastq files.\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nClick here to see the bash code for renaming fastq files.\n\n#!/usr/bin/env bash\n\n# Check arguments\nif [[ $# -lt 2 ]]; then\n    echo \"Usage: $0 /path/to/files/ rename_file.txt\"\n    exit 1\nfi\n\ndata_dir=\"$1\"\nrename_file=\"$2\"\n\n# Validate directory\nif [[ ! -d \"$data_dir\" ]]; then\n    echo \"Error: Directory '$data_dir' not found!\"\n    exit 1\nfi\n\n# Validate rename file\nif [[ ! -f \"$rename_file\" ]]; then\n    echo \"Error: File '$rename_file' not found!\"\n    exit 1\nfi\n\n# Get root name (strip directory and extension)\nbase_name=$(basename \"$rename_file\" .txt)\noutput_file=\"${base_name}_results.txt\"\n\n# Perform renaming inside given directory\nwhile IFS=$'\\t' read -r orig new; do \n    rename -v \"$orig\" \"$new\" \"$data_dir\"/*.fastq.gz\ndone &lt; \"$rename_file\" | tee \"$output_file\"\n\necho \"Results written to: $output_file\"\n\nSimply run as such with the script by passing it a directory containing fastq files and a lookup table.\n\nbash rename.sh /path/to/fastq_files /path/to/rename_table\n\n`SWELTR-1_R1.fastq.gz' -&gt; `P01-D00-010-W4A_R1.fastq.gz'\n`SWELTR-2_R1.fastq.gz' -&gt; `P01-D10-020-W4A_R1.fastq.gz'\n`SWELTR-3_R1.fastq.gz' -&gt; `P01-D20-050-W4A_R1.fastq.gz'\n`SWELTR-4_R1.fastq.gz' -&gt; `P01-D50-100-W4A_R1.fastq.gz'\n`SWELTR-5_R1.fastq.gz' -&gt; `P02-D00-010-C0A_R1.fastq.gz'\n`SWELTR-6_R1.fastq.gz' -&gt; `P02-D10-020-C0A_R1.fastq.gz'",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#cutadapt-workflows",
    "href": "primers.html#cutadapt-workflows",
    "title": "Primer Removal",
    "section": "Cutadapt Workflows",
    "text": "Cutadapt Workflows\nIn each tab below you can find the complete cutadapt workflow for each dataset. The worflows are nearly identical except for the the cutadapt parameters --minimum-length and --maximum-length. The differences are described here:\n\nDifferences in cutadapt parameters for --minimum-length and --maximum-length.\n\n\n\n\n\n\nDataset\nminimum-length\nmaximum-length\n\n\n\nSSU\n200\n300\n\n\nITS\n20\n500\n\n\nAMF\n100\n300\n\n\nOomycete\n20\n500\n\n\n\n \n\n\nSSU\nITS\nAMF\nOomycete\n\n\n\n\n\n\n\n\n\n   1_cut_ssu.R\n\n\n\n R Script for cutadapt  primer removal from the SSU dataset.\n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/SSU/2019\"\nhead(list.files(path))\n\n[1] \"P00-D00-000-NNN_R1.fastq.gz\" \"P00-D00-000-NNN_R2.fastq.gz\"\n[3] \"P01-D00-010-W4A_R1.fastq.gz\" \"P01-D00-010-W4A_R2.fastq.gz\"\n[5] \"P01-D10-020-W4A_R1.fastq.gz\" \"P01-D10-020-W4A_R2.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/ssu_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 1: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"GTGCCAGCMGCCGCGGTAA\"\nREV &lt;- \"GGACTACHVGGGTWTCTAAT\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n              Forward            Complement               Reverse \n\"GTGCCAGCMGCCGCGGTAA\" \"CACGGTCGKCGGCGCCATT\" \"AATGGCGCCGMCGACCGTG\" \n              RevComp \n\"TTACCGCGGCKGCTGGCAC\" \n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GGACTACHVGGGTWTCTAAT\" \"CCTGATGDBCCCAWAGATTA\" \"TAATCTWTGGGVHCATCAGG\" \n               RevComp \n\"ATTAGAWACCCBDGTAGTCC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                              fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                              fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads  240334          0       0       0\nFWD.ReverseReads       1          0       0     110\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n     REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       1          0       0     113\nREV.ReverseReads  228930          0       0       0\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 200,\n                                            \"--maximum-length\", 300,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g GTGCCAGCMGCCGCGGTAA -a ATTAGAWACCCBDGTAGTCC \\\n                         -G GGACTACHVGGGTWTCTAAT -A TTACCGCGGCKGCTGGCAC \\\n                         --times 2 --minimum-length 200 --maximum-length 300 \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed \\\n                         --output Negative_R1.fastq.gz \\\n                         --paired-output Negative_R2.fastq.gz \\\n                         --report full --cores 10 \n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:41], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:41], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/ssu_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 2: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nAs you can see, no primers of any orientation remain in the dataset.\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/ssu_cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n\n   1_cut_its.R\n\n\n\n R Script for cutadapt  primer removal from the ITS dataset.\n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/ITS/2019\"\nhead(list.files(path))\n\n[1] \"P02-D10-020-C0A_R1.fastq.gz\" \"P01-D00-010-W4A_R1.fastq.gz\"\n[3] \"P01-D00-010-W4A_R2.fastq.gz\" \"P01-D10-020-W4A_R1.fastq.gz\"\n[5] \"P01-D10-020-W4A_R2.fastq.gz\" \"P01-D20-050-W4A_R1.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/its_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 3: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"CTTGGTCATTTAGAGGAAGTAA\"\nREV &lt;- \"GCTGCGTTCTTCATCGATGC\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n                 Forward               Complement                  Reverse \n\"CTTGGTCATTTAGAGGAAGTAA\" \"GAACCAGTAAATCTCCTTCATT\" \"AATGAAGGAGATTTACTGGTTC\" \n                 RevComp \n\"TTACTTCCTCTAAATGACCAAG\"\n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GCTGCGTTCTTCATCGATGC\" \"CGACGCAAGAAGTAGCTACG\" \"CGTAGCTACTTCTTGCGTCG\" \n               RevComp \n\"GCATCGATGAAGAACGCAGC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 1\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads  113013          0       0       1\nFWD.ReverseReads       1          0       0   84739\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits,\n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       1          0       0   88094\nREV.ReverseReads  113823          0       0       1\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 20,\n                                            \"--maximum-length\", 500,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g CTTGGTCATTTAGAGGAAGTAA -a GCATCGATGAAGAACGCAGC  \\\n                         -G GCTGCGTTCTTCATCGATGC -A TTACTTCCTCTAAATGACCAAG  \\\n                         --times 2 --minimum-length 20 --maximum-length 500  \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed  \\\n                         --output P01-D00-010-W4A_R1.fastq.gz  \\\n                         --paired-output P01-D00-010-W4A_R2.fastq.gz  \\\n                         --report full --cores 10  \\\n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:41], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:41], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/its_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 4: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nAs you can see, no primers of any orientation remain in the dataset.\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n\n   1_cut_amf.R\n\n\n\n R Script for cutadapt  primer removal from the AMF dataset.\n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/AMF/2019\"\nhead(list.files(path))\n\n[1] \"P00-D00-000-NNN_R1.fastq.gz\" \"P00-D00-000-NNN_R2.fastq.gz\"\n[3] \"P01-D00-010-W4A_R1.fastq.gz\" \"P01-D00-010-W4A_R2.fastq.gz\"\n[5] \"P01-D10-020-W4A_R1.fastq.gz\" \"P01-D10-020-W4A_R2.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/amf_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 5: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"AAGCTCGTAGTTGAATTTCG\"\nREV &lt;- \"CCCAACTATCCCTATTAATCAT\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n               Forward             Complement                Reverse \n\"AAGCTCGTAGTTGAATTTCG\" \"TTCGAGCATCAACTTAAAGC\" \"GCTTTAAGTTGATGCTCGAA\" \n               RevComp \n\"CGAAATTCAACTACGAGCTT\" \n\nREV.orients\n\n                 Forward               Complement                  Reverse \n\"CCCAACTATCCCTATTAATCAT\" \"GGGTTGATAGGGATAATTAGTA\" \"TACTAATTATCCCTATCAACCC\" \n                 RevComp \n\"ATGATTAATAGGGATAGTTGGG\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   47737          0       0       0\nFWD.ReverseReads       0          0       0   33903\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0   33914\nREV.ReverseReads   45205          0       0       0\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 100,\n                                            \"--maximum-length\", 300,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g AAGCTCGTAGTTGAATTTCG -a ATGATTAATAGGGATAGTTGGG  \\\n                         -G CCCAACTATCCCTATTAATCAT -A CGAAATTCAACTACGAGCTT  \\\n                         --times 2 --minimum-length 100 --maximum-length 300  \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed  \\\n                         --output P00-D00-000-NNN_R1.fastq.gz  \\\n                         --paired-output P00-D00-000-NNN_R2.fastq.gz  \\\n                         --report full --cores 10  \\\n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:40], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:40], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/amf_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 6: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       1          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nAs you can see, no primers of any orientation remain in the dataset.\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\n\n\n\n\n\n\n   1_cut_oo.R\n\n\n\n R Script for cutadapt  primer removal from the Oomycete dataset.\n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/OO/2019\"\nhead(list.files(path))\n\n[1] \"P01-D20-050-W4A_R1.fastq.gz\" \"P00-D00-000-NNN_R1.fastq.gz\"\n[3] \"P00-D00-000-NNN_R2.fastq.gz\" \"P01-D00-010-W4A_R1.fastq.gz\"\n[5] \"P01-D00-010-W4A_R2.fastq.gz\" \"P01-D10-020-W4A_R1.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/oo_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 7: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"GGAAGGATCATTACCACA\"\nREV &lt;- \"GCTGCGTTCTTCATCGATGC\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n             Forward           Complement              Reverse \n\"GGAAGGATCATTACCACA\" \"CCTTCCTAGTAATGGTGT\" \"ACACCATTACTAGGAAGG\" \n             RevComp \n\"TGTGGTAATGATCCTTCC\" \n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GCTGCGTTCTTCATCGATGC\" \"CGACGCAAGAAGTAGCTACG\" \"CGTAGCTACTTCTTGCGTCG\" \n               RevComp \n\"GCATCGATGAAGAACGCAGC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   71659          0       0       0\nFWD.ReverseReads       0          0       0   65763\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0   65982\nREV.ReverseReads   69664          0       0       0\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 20,\n                                            \"--maximum-length\", 500,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g GGAAGGATCATTACCACA -a GCATCGATGAAGAACGCAGC \n                         -G GCTGCGTTCTTCATCGATGC -A TGTGGTAATGATCCTTCC \n                         --times 2 --minimum-length 20 --maximum-length 500 \n                         --error-rate 0.1 --no-indels --discard-untrimmed \n                         --output P00-D00-000-NNN_R1.fastq.gz \n                         --paired-output P00-D00-000-NNN_R2.fastq.gz \n                         --report full --cores 10 \n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:40], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:40], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/oo_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 8: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       1          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nAs you can see, no primers of any orientation remain in the dataset.\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\",\n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The SWELTR Depth Study",
    "section": "",
    "text": "Here I provide quick access to data and scripts depending on where you want to start your analysis. I list these in reverse order–beginning with the curated datasets ready for analysis and ending with the raw, unprocessed, fastq data. In each section I provide the relevant data, scripts, and tables. I also link to detailed explanations of each workflow.\n\nData provided here is the output of the Data Curation workflows. These data are ready for analysis.\n\n\n\n\n\n\nCurated Data\n\n\n\n\nQuick links to curated data.\n\n\n\n\n\nDataset\nCurrated Data (zip files)\n\n\n\n16S rRNA (ssu)\n ssu_curated_data \n\n\nITS (its)\n its_curated_data \n\n\nAMF (amf)\n amf_curated_data \n\n\nOomycete (oo)\n oo_curated_data \n\n\n\nThese .zip files contain the processed data in different formats:\n\n\nxyz_otu_table.txt: ASV by sample abundance table.\n\nxyz_tax_table.txt: ASV taxonomic assignments.\n\nxyz_sample_table.txt: Sample data table\n\nxyz_rep_fasta.fasta: Representative ASV fasta sequences.\n\nThese files each contain all four tables descriped above.\n\n\nxyz_me_asv.rds: microtable object from the R microeco package.\n\nxyz_ps_asv.rds: phyloseq object from the R phyloseq package\n\nWhere xyz is the root name corresponding to one of the four datasets; ssu = 16S rRNA, its = ITS, amf = Arbuscular mycorrhiza fungi, oo = Oomycetes.\n\n\n\nI strongly encourage anyone interested in these data to look closely at this table–specifically the samples which have been removed at some point in the pipeline. These are represented by NA in the table. This is especially true for the Oomycete dataset.\n\n\n\n\n\n\nCurated Data Summary Table\n\n\n\nYou can download the table here:\n Download final read/ASV counts in all datasets \nOr view it in-line below.\n\n\n\n\n\nTable 1: final read/ASV counts for each sample in all datasets.\n\n\n\n\n\n\n\n\n\n\nAt this stage fastq files have been renamed and primers removed and ASVs have been called with LotuS3. In this workflow I use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables to create a microeco object. Once that is complete I curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAs of 2025-08-28 I have only done this for the SSU (16S rRNA) dataset. The rest are coming soon.\n\nGrab this data if you:\n\nwant to curate the data yourself or\n\nwant to see the data before any spurious ASVs or samples were removed.\n\n\n\n\n\n\n\n   phyloseq_ssu.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the SSU Data Curation workflow.\n\n\n\nOnce primer removal is complete you can call ASVs with LotuS3. Here you can find all you need to do this–trimmed fastq files (primers removed), mapping files, and LotuS3 commands.\n\nQuick links to trimmed fastq data and mapping files for LotuS3.\n\n\n\n\n\n\nDataset\nTrimmed fastq files\nMapping file\n\n\n\n16S rRNA (ssu)\n\nSSU trimmed fastq files\n\n SSU map \n\n\nITS (its)\n\nITS trimmed fastq files\n\n ITS map \n\n\nAMF (amf)\n\nAMF trimmed fastq files\n\n AMF map \n\n\nOomycete (oo)\n\nOO trimmed fastq files\n\n OO map \n\n\n\nOnce you have the data and mapping files you can run the following commands (assuming you have LotuS3 installed).\n\n# SSU dataset\nlotus3 -i . -map ssu_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer GTGCCAGCMGCCGCGGTAA -reversePrimer GGACTACHVGGGTWTCTAAT -clustering dada2 -refDB SLV -taxAligner lambda -threads 20\n\n# ITS dataset\nlotus3 -i . -map its_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS  -forwardPrimer CTTGGTCATTTAGAGGAAGTAA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh__release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n#AMF dataset\nlotus3 -i .-map amf_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer AAGCTCGTAGTTGAATTTCG -reversePrimer CCCAACTATCCCTATTAATCAT -clustering dada2 -refDB SLV -taxAligner lambda -t 20\n\n# Oomycete dataset\nlotus3 -i . -map oo_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS -forwardPrimer GGAAGGATCATTACCACA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the ASV Inference workflow.\n\n\n\nWant to start at the very beginning? Here are the raw fastq files, fastq renaming script, rename tables, and cutadapt primer removal scripts.\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\n16S rRNA (ssu)\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS (its)\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF (amf)\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete (oo)\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\nSteps to reproduce results:\n\nRun the script rename.sh to rename the fastq files. For example, to change the names of the fastq files in the SSU dataset, simply run the following:\n\n\nbash rename.sh SSU fastq files/ ssu_rename.txt\n\n\nRun the cutadapt script (e.g., 1_cut_ssu.R) using the renamed fastq file directory.\n\n\nRscript 1_cut_ssu.R\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the Primer Removal workflow."
  },
  {
    "objectID": "index.html#access-to-data-scripts",
    "href": "index.html#access-to-data-scripts",
    "title": "The SWELTR Depth Study",
    "section": "",
    "text": "Here I provide quick access to data and scripts depending on where you want to start your analysis. I list these in reverse order–beginning with the curated datasets ready for analysis and ending with the raw, unprocessed, fastq data. In each section I provide the relevant data, scripts, and tables. I also link to detailed explanations of each workflow.\n\nData provided here is the output of the Data Curation workflows. These data are ready for analysis.\n\n\n\n\n\n\nCurated Data\n\n\n\n\nQuick links to curated data.\n\n\n\n\n\nDataset\nCurrated Data (zip files)\n\n\n\n16S rRNA (ssu)\n ssu_curated_data \n\n\nITS (its)\n its_curated_data \n\n\nAMF (amf)\n amf_curated_data \n\n\nOomycete (oo)\n oo_curated_data \n\n\n\nThese .zip files contain the processed data in different formats:\n\n\nxyz_otu_table.txt: ASV by sample abundance table.\n\nxyz_tax_table.txt: ASV taxonomic assignments.\n\nxyz_sample_table.txt: Sample data table\n\nxyz_rep_fasta.fasta: Representative ASV fasta sequences.\n\nThese files each contain all four tables descriped above.\n\n\nxyz_me_asv.rds: microtable object from the R microeco package.\n\nxyz_ps_asv.rds: phyloseq object from the R phyloseq package\n\nWhere xyz is the root name corresponding to one of the four datasets; ssu = 16S rRNA, its = ITS, amf = Arbuscular mycorrhiza fungi, oo = Oomycetes.\n\n\n\nI strongly encourage anyone interested in these data to look closely at this table–specifically the samples which have been removed at some point in the pipeline. These are represented by NA in the table. This is especially true for the Oomycete dataset.\n\n\n\n\n\n\nCurated Data Summary Table\n\n\n\nYou can download the table here:\n Download final read/ASV counts in all datasets \nOr view it in-line below.\n\n\n\n\n\nTable 1: final read/ASV counts for each sample in all datasets.\n\n\n\n\n\n\n\n\n\n\nAt this stage fastq files have been renamed and primers removed and ASVs have been called with LotuS3. In this workflow I use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables to create a microeco object. Once that is complete I curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAs of 2025-08-28 I have only done this for the SSU (16S rRNA) dataset. The rest are coming soon.\n\nGrab this data if you:\n\nwant to curate the data yourself or\n\nwant to see the data before any spurious ASVs or samples were removed.\n\n\n\n\n\n\n\n   phyloseq_ssu.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the SSU Data Curation workflow.\n\n\n\nOnce primer removal is complete you can call ASVs with LotuS3. Here you can find all you need to do this–trimmed fastq files (primers removed), mapping files, and LotuS3 commands.\n\nQuick links to trimmed fastq data and mapping files for LotuS3.\n\n\n\n\n\n\nDataset\nTrimmed fastq files\nMapping file\n\n\n\n16S rRNA (ssu)\n\nSSU trimmed fastq files\n\n SSU map \n\n\nITS (its)\n\nITS trimmed fastq files\n\n ITS map \n\n\nAMF (amf)\n\nAMF trimmed fastq files\n\n AMF map \n\n\nOomycete (oo)\n\nOO trimmed fastq files\n\n OO map \n\n\n\nOnce you have the data and mapping files you can run the following commands (assuming you have LotuS3 installed).\n\n# SSU dataset\nlotus3 -i . -map ssu_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer GTGCCAGCMGCCGCGGTAA -reversePrimer GGACTACHVGGGTWTCTAAT -clustering dada2 -refDB SLV -taxAligner lambda -threads 20\n\n# ITS dataset\nlotus3 -i . -map its_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS  -forwardPrimer CTTGGTCATTTAGAGGAAGTAA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh__release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n#AMF dataset\nlotus3 -i .-map amf_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer AAGCTCGTAGTTGAATTTCG -reversePrimer CCCAACTATCCCTATTAATCAT -clustering dada2 -refDB SLV -taxAligner lambda -t 20\n\n# Oomycete dataset\nlotus3 -i . -map oo_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS -forwardPrimer GGAAGGATCATTACCACA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the ASV Inference workflow.\n\n\n\nWant to start at the very beginning? Here are the raw fastq files, fastq renaming script, rename tables, and cutadapt primer removal scripts.\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\n16S rRNA (ssu)\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS (its)\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF (amf)\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete (oo)\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\nSteps to reproduce results:\n\nRun the script rename.sh to rename the fastq files. For example, to change the names of the fastq files in the SSU dataset, simply run the following:\n\n\nbash rename.sh SSU fastq files/ ssu_rename.txt\n\n\nRun the cutadapt script (e.g., 1_cut_ssu.R) using the renamed fastq file directory.\n\n\nRscript 1_cut_ssu.R\n\n\n\n\n\n\n\nWorkflow\n\n\n\nFor a more detailed explanation please see the Primer Removal workflow."
  },
  {
    "objectID": "curate_oo.html",
    "href": "curate_oo.html",
    "title": "Oomycete Data Curation",
    "section": "",
    "text": "On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAll you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_oo.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)\n\n\n\n\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#data",
    "href": "curate_oo.html#data",
    "title": "Oomycete Data Curation",
    "section": "",
    "text": "All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_oo.Rdata\n\n\n\n LotuS3 phyloseq data",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#citable-resources",
    "href": "curate_oo.html#citable-resources",
    "title": "Oomycete Data Curation",
    "section": "",
    "text": "R microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#references",
    "href": "curate_oo.html#references",
    "title": "Oomycete Data Curation",
    "section": "",
    "text": "Borman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#required-packages",
    "href": "curate_oo.html#required-packages",
    "title": "Oomycete Data Curation",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick here for required R packages.set.seed(919191)\nlibrary(microeco)\nlibrary(mia)\nlibrary(phyloseq)\nlibrary(microbiome)",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#review-results",
    "href": "curate_oo.html#review-results",
    "title": "Oomycete Data Curation",
    "section": "Review Results",
    "text": "Review Results\nThe LotuS3 pipeline produces numerous output files but for our purposes there are four specific files we are interested in:\n\nphyloseq.Rdata\nOTU.fna\nOTU.txt\nhiera_BLAST.txt\n\nWe will mainly work from the phyloseq.Rdata (renamed phyloseq_oo.Rdata) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis.\n\nload(\"working_files/LOTUS3/oo/phyloseq_oo.Rdata\")\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 12091 taxa and 41 samples ]\nsample_data() Sample Data:       [ 41 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 12091 taxa by 7 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 12091 tips and 12089 internal nodes ]\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n0\n\n\nMax. number of reads\n89813\n\n\nTotal number of reads\n336493\n\n\nAverage number of reads\n8207\n\n\nMedian number of reads\n2333\n\n\nTotal ASVs\n192\n\n\nNumber of singleton ASVs\nNA\n\n\nAny ASVs sum to 1 or less?\nFALSE\n\n\nPercent of ASVs that are singletons\nNA\n\n\nSparsity\n0.946\n\n\n\nIn this next part of the workflow the goal is to create a microtable object using the R package microeco (Liu et al. 2021). The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\nWe will also:\n\nRemove any ASVs without kingdom level classification.\n\nRevome any contaminants (chloroplast, mitochondria, etc.).\n\nRemove Negative Control (NC) samples.\n\nRemove any low-count samples.",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#sample-data",
    "href": "curate_oo.html#sample-data",
    "title": "Oomycete Data Curation",
    "section": "Sample Data",
    "text": "Sample Data\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nNew sample ID based on plot, depth, treatment, & pair\n\n\nPlot\nExperimental plot ID\n\n\nTreatment\nControl vs warming\n\n\nTemp\nWarming treatment temperature\n\n\nDepth\nSampling depth in soil\n\n\nPairing\nSample pairings\n\n\nraw_rc\nNo. of raw reads\n\n\ncutadapt_rc\nRead count after cutdapt\n\n\nlotus3_rc\nRead count after LotuS3 pipeline\n\n\nlotus3_asv\nNo. of ASVs\n\n\n\n\nNow I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow.\n\ntmp_otu &lt;- data.frame(otu_table(physeq))\ntmp_otu_sum &lt;- data.frame(colSums(tmp_otu &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_otu_sum)[2] &lt;- \"lotus3_asv\"\ntmp_read_sum &lt;- data.frame(colSums(tmp_otu)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_read_sum)[2] &lt;- \"lotus3_rc\"\ntmp_track_reads &lt;- read.table(\n    \"files/CUTADAPT/oo_cutadapt_track.txt\",\n    header = TRUE, sep = \"\\t\"\n)\n\ntmp_track_reads$SampleID &lt;- stringr::str_replace_all(\n  tmp_track_reads$SampleID, \"-\", \"_\")\nnames(tmp_track_reads)[2] &lt;- \"raw_rc\"\nnames(tmp_track_reads)[3] &lt;- \"cutadapt_rc\"\n\nThen I want to merge this data with the sample metadata.\n\ntmp_samp &lt;- data.frame(sample_data(physeq)) %&gt;% \n  tibble::rownames_to_column(\"SampleID\")\n\nsample_tab &lt;- dplyr::left_join(tmp_samp, tmp_track_reads, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_read_sum, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_otu_sum, by = \"SampleID\") \nsample_tab &lt;- sample_tab %&gt;%\n  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, \n                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)\n\n\nreadr::write_delim(sample_tab, \"files/CURATE/oo_sample_data_full.txt\",\n    delim = \"\\t\")\n\nsample_tab_trim &lt;- sample_tab[, -c(11:19)]\nsample_tab_trim$per_reads_kept &lt;- \n  round(sample_tab_trim$lotus3_rc/sample_tab_trim$raw_rc, digits = 3)\nsample_tab_trim &lt;- sample_tab_trim %&gt;% \n  dplyr::relocate(per_reads_kept, .before = lotus3_asv)\n\n\n\n\n\nTable 1: Sample metadata including read changes at start and end of workflow.\n\n\n\n\n\n\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#prep-data-for-microeco",
    "href": "curate_oo.html#prep-data-for-microeco",
    "title": "Oomycete Data Curation",
    "section": "Prep Data for microeco",
    "text": "Prep Data for microeco\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section, microeco tutorial.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the dataset.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))\ntmp_tax[1:6, 1:4]\n\n       Domain Phylum     Class          Order\nASV163 Fungi  Ascomycota Eurotiomycetes Chaetothyriales\nASV118 Fungi  Ascomycota Eurotiomycetes Chaetothyriales\nASV86  ?      ?          ?              ?\nASV179 ?      ?          ?              ?\nASV132 Fungi  Ascomycota Eurotiomycetes Chaetothyriales\nASV156 Fungi  Ascomycota Eurotiomycetes Chaetothyriales\nNext we need to add rank definitions (e.g., k__, p__, etc.) to each classification.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))   \ntmp_tax &lt;- tmp_tax %&gt;% dplyr::rename(\"Kingdom\" = \"Domain\")\ntmp_tax &lt;- tmp_tax %&gt;%\n  dplyr::mutate_all(~stringr::str_replace_all(., \"\\\\?\", \"\"))\n\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\ntmp_tax$Species &lt;- paste(\"s\", tmp_tax$Species, sep = \"__\")\n\nAnd now the final, modified taxonomy table.\n\ntmp_tax[1:6, 1:4]\n\n       Kingdom  Phylum        Class             Order\nASV163 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales\nASV118 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales\nASV86  k__      p__           c__               o__\nASV179 k__      p__           c__               o__\nASV132 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales\nASV156 k__Fungi p__Ascomycota c__Eurotiomycetes o__Chaetothyriales\nB. Sequence Table\nHere is what the sequence table looks like in the dataset.\n\ntmp_otu[1:6, 1:3]\n\n       P00_D00_000_NNN P01_D00_010_W4A P01_D10_020_W4A\nASV163               0               0               0 \nASV118               0               0               0 \nASV86                0               0               0 \nASV179               0               0               0 \nASV132               0               0               0 \nASV156               0               0               0 \n\nidentical(row.names(tmp_otu), row.names(tmp_tax))\n\n[1] TRUE\nC. Sample Table\nHere is what the sample table looks like.\n\nsample_tab[1:6, 1:6]\n\n                       SampleID Plot  Depth Treatment Temp Pairing\nP00_D00_000_NNN P00_D00_000_NNN  P00 00_000  Negative    N       N\nP01_D00_010_W4A P01_D00_010_W4A  P01 00_010      Warm    4       A\nP01_D10_020_W4A P01_D10_020_W4A  P01 10_020      Warm    4       A\nP01_D20_050_W4A P01_D20_050_W4A  P01 20_050      Warm    4       A\nP01_D50_100_W4A P01_D50_100_W4A  P01 50_100      Warm    4       A\nP02_D00_010_C0A P02_D00_010_C0A  P02 00_010   Control    0       A\n\n\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#create-a-microtable-object",
    "href": "curate_oo.html#create-a-microtable-object",
    "title": "Oomycete Data Curation",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nTip\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV.\n\n\n\nsample_tab &lt;- sample_tab\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_otu\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_tab, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\n2 samples with 0 abundance are removed from the otu_table ...\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12091 rows and 41 columns\ntax_table have 12091 rows and 7 columns\nD. Add Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"working_files/LOTUS3/oo/OTU.fna\")\nidentical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))\n\n[1] TRUE\n\ntmp_fasta_names &lt;- names(rep_fasta)\ntmp_rep_fasta &lt;- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]\nrep_fasta &lt;- tmp_rep_fasta\ntmp_me$rep_fasta &lt;- rep_fasta\nidentical(row.names(tmp_me$tax_table), names(rep_fasta))\ntmp_me$tidy_dataset()\n\n[1] TRUE\n\nme_asv_raw &lt;- microeco::clone(tmp_me)\nme_asv_raw\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 19 columns\notu_table have 192 rows and 39 columns\ntax_table have 192 rows and 7 columns\nrep_fasta have 192 sequences\n\n\n\nThe microeco object me_asv_raw contains all data from the LotuS3 pipeline, before any dataset curation.",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_oo.html#curate-the-data-set",
    "href": "curate_oo.html#curate-the-data-set",
    "title": "Oomycete Data Curation",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove non-fungal ASVs\nFirst we can have a quick look at the kingdom level classifications to see what we are dealing with.\n\ntmp_k &lt;- me_asv_raw$tax_table %&gt;%\n      distinct(Kingdom, .keep_all = TRUE)\ntmp_p &lt;- me_asv_raw$tax_table %&gt;%\n      distinct(Phylum, .keep_all = TRUE)\n\n\n\n-------KINGDOM------------------\n\n\nk__Fungi\nk__\nk__Eukaryota_kgd_Incertae_sedis\nk__Ichthyosporia\nk__Stramenopila\nk__Alveolata\n\n\n-------PHYLUM------------------\n\n\np__Ascomycota\np__\np__Basidiomycota\np__Eukaryota_phy_Incertae_sedis\np__Fungi_phy_Incertae_sedis\np__Ichthyosporia_phy_Incertae_sedis\np__Oomycota\np__Mucoromycota\np__Stramenopila_phy_Incertae_sedis\np__Ochrophyta\np__Dinophyta\n\n\n\nThe first thing to notice is that at the kingdom level we have Fungi, unclassified (or NA), Stramenopila, some non-fungal classifications. We can remove anything that is not classified as Fungi at the kingdom level or Oomycota at the phylum level.\n\nThe easiest thing to do is keep everything we want–in other words, all fungal taxa.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Kingdom == \"k__Fungi\" | Phylum == \"p__Oomycota\")\ntmp_no_na$tidy_dataset()\n\n\nme_asv_no_na &lt;- microeco::clone(tmp_no_na)\nme_asv_no_na\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 19 columns\notu_table have 91 rows and 39 columns\ntax_table have 91 rows and 7 columns\nrep_fasta have 91 sequences\n\n\n\nThe microeco object me_asv_no_na conatins only ASVs classified as Archaea or Bacteria.\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_na)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, Treatment == \"Negative\")\ntmp_nc$tidy_dataset()\n\n89 taxa with 0 abundance are removed from the otu_table ...\n\nme_asv_nc &lt;- microeco::clone(tmp_nc)\nme_asv_nc\n\n\n\nmicrotable-class object:\nsample_table have 1 rows and 19 columns\notu_table have 2 rows and 1 columns\ntax_table have 2 rows and 7 columns\nrep_fasta have 2 sequences\n\n\n\nThe microeco object me_asv_nc conatins only the negative control sample(s) and associated ASVs.\n\nLooks like there are 2 ASVs in the NC samples from a total of 2 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\nnc_asvs\n\n\n\n[1] 2\n\n\nThere are 2 ASVs found in the NC sample.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_na)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(\"NNN\"))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(\"NNN\"))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0))\ntmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\ntmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(\"NNN\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\"))\ntmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(\"NNN\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\ntmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(\n    perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n    .after = \"num_samp_no_nc\")\n\n\nnc_check &lt;- tmp_rem_nc_df\n\n\n\n\nTable 2: Summary of ASVs detected in Negative Control (NC) samples.\n\n\n\n\n\n\n\n\n\n Download ASVs in NC samples \nLooking at these data we can see the NC sample only has 1 read from each of the two ASVs. We can simply ignore these ASVs and just remove the NC sample. We will still go through the process…\n\nnc_remove &lt;- nc_check %&gt;% \n  dplyr::filter(perc_in_neg &gt; 10 | num_samp_no_nc &lt; 4)\n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n0\n0\n0\nNaN\n\n\nRetained\n2\n2\n118212\n0.002\n\n\n\nWe identified a total of 2 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 0 ASVs from the data set, which represented 0 total reads in NC samples and 0 total reads in non-NC samples. Of the total reads removed NaN% came from NC samples. Of all ASVs identified in NC samples,2 were retained because the fell below the threshhold criteria. These ASVs accounted for 2 reads in NC samples and 118212 reads in non-NC samples. NC samples accounted for 0.002% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_na)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  dplyr::filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 Treatment != \"Negative\")\ntmp_no_nc$tidy_dataset()\n\n\nme_asv_no_nc &lt;- microeco::clone(tmp_no_nc)\nme_asv_no_nc\n\n\n\nmicrotable-class object:\nsample_table have 38 rows and 19 columns\notu_table have 91 rows and 38 columns\ntax_table have 91 rows and 7 columns\nrep_fasta have 91 sequences\n\n\n\nThe microeco object me_asv_no_nc does not conatins the negative control sample(s).\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts. I know from looking at these data that the are many low coverage samples. We can take a look at different thresholds to see how many samples will be eliminated.\n\nsamp_sum &lt;- data.frame(me_asv_no_nc$sample_sums())\n\n\n\n--- &lt;1000 reads --- 18  samples \n\n\n--- &lt;500 reads --- 17  samples \n\n\n--- &lt;100 reads --- 14  samples \n\n\nWe can start by eliminating samples with fewer than 100 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\n\n\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt; 100))\ntmp_no_low$tidy_dataset()\n\n3 taxa with 0 abundance are removed from the otu_table ...\nCheck if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).\n\ntmp_row_sums &lt;- rowSums(tmp_no_low$otu_table)\nany(tmp_row_sums == 0)\n\n[1] FALSE\n\nme_asv_no_low &lt;- microeco::clone(tmp_no_low)\nme_asv_no_low\n\n\n\nmicrotable-class object:\nsample_table have 24 rows and 19 columns\notu_table have 88 rows and 24 columns\ntax_table have 88 rows and 7 columns\nrep_fasta have 88 sequences\n\n\n\nThe microeco object me_asv_no_low does not conatins the low abundance samples.\n\nWe will quickly add final read counts and ASVs, then create the final microtable object.\n\ntmp_final_rc &lt;- data.frame(me_asv_no_low$sample_sums()) %&gt;% \n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_rc\" = 2)\n\ntmp_final_asv &lt;- data.frame(t(me_asv_no_low$otu_table))\ntmp_final_asv &lt;- data.frame(rowSums(tmp_final_asv &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_asv\" = 2)\ntmp_final &lt;- dplyr::left_join(tmp_final_rc, tmp_final_asv)\n\nsample_tab &lt;- dplyr::full_join(sample_tab, tmp_final) %&gt;% \n        dplyr::relocate(c(final_rc, final_asv), .after = lotus3_asv)\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)\n\nfull_sample_tab &lt;- sample_tab\n\n\ntmp_samp_tab &lt;- sample_tab[complete.cases(sample_tab), ]\ntmp_tax_tab &lt;- me_asv_no_low$tax_table\ntmp_otu_tab &lt;- me_asv_no_low$otu_table\ntmp_rep_fasta &lt;- me_asv_no_low$rep_fasta\n\n\nme_asv &lt;- microtable$new(sample_table = tmp_samp_tab, \n                         otu_table = tmp_otu_tab, \n                         tax_table = tmp_tax_tab, \n                         rep_fasta = tmp_rep_fasta)\nme_asv$tidy_dataset()\n\n\n\nmicrotable-class object:\nsample_table have 24 rows and 21 columns\notu_table have 88 rows and 24 columns\ntax_table have 88 rows and 7 columns\nrep_fasta have 88 sequences",
    "crumbs": [
      "B. Data Curation",
      "Oomycete Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html",
    "href": "curate_amf.html",
    "title": "AMF Data Curation",
    "section": "",
    "text": "On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAll you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_amf.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024) library(microeco) library(microbiome)\n\n\n\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#data",
    "href": "curate_amf.html#data",
    "title": "AMF Data Curation",
    "section": "",
    "text": "All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_amf.Rdata\n\n\n\n LotuS3 phyloseq data",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#citable-resources",
    "href": "curate_amf.html#citable-resources",
    "title": "AMF Data Curation",
    "section": "",
    "text": "R microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024) library(microeco) library(microbiome)",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#references",
    "href": "curate_amf.html#references",
    "title": "AMF Data Curation",
    "section": "",
    "text": "Borman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#required-packages",
    "href": "curate_amf.html#required-packages",
    "title": "AMF Data Curation",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick here for required R packages.set.seed(919191)\nlibrary(microeco)\nlibrary(mia)\nlibrary(phyloseq)\nlibrary(microbiome)",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#review-results",
    "href": "curate_amf.html#review-results",
    "title": "AMF Data Curation",
    "section": "Review Results",
    "text": "Review Results\nThe LotuS3 pipeline produces numerous output files but for our purposes there are four specific files we are interested in:\n\nphyloseq.Rdata\nOTU.fna\nOTU.txt\nhiera_BLAST.txt\n\nWe will mainly work from the phyloseq.Rdata (renamed phyloseq_amf.Rdata) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis.\n\nload(\"working_files/LOTUS3/amf/phyloseq_amf.Rdata\")\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 883 taxa and 41 samples ]:\nsample_data() Sample Data:        [ 41 samples by 14 sample variables ]:\ntax_table()   Taxonomy Table:     [ 883 taxa by 7 taxonomic ranks ]:\nphy_tree()    Phylogenetic Tree:  [ 883 tips and 881 internal nodes ]:\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n221\n\n\nMax. number of reads\n52244\n\n\nTotal number of reads\n1167700\n\n\nAverage number of reads\n28480\n\n\nMedian number of reads\n31733\n\n\nTotal ASVs\n883\n\n\nNumber of singleton ASVs\n2\n\n\nAny ASVs sum to 1 or less?\nTRUE\n\n\nPercent of ASVs that are singletons\n0.227\n\n\nSparsity\n0.904\n\n\n\nIn this next part of the workflow the goal is to create a microtable object using the R package microeco (Liu et al. 2021). The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\nWe will also:\n\nRemove any ASVs without kingdom level classification.\n\nRevome any contaminants (chloroplast, mitochondria, etc.).\n\nRemove Negative Control (NC) samples.\n\nRemove any low-count samples.",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#sample-data",
    "href": "curate_amf.html#sample-data",
    "title": "AMF Data Curation",
    "section": "Sample Data",
    "text": "Sample Data\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nNew sample ID based on plot, depth, treatment, & pair\n\n\nPlot\nExperimental plot ID\n\n\nTreatment\nControl vs warming\n\n\nTemp\nWarming treatment temperature\n\n\nDepth\nSampling depth in soil\n\n\nPairing\nSample pairings\n\n\nraw_rc\nNo. of raw reads\n\n\ncutadapt_rc\nRead count after cutdapt\n\n\nlotus3_rc\nRead count after LotuS3 pipeline\n\n\nlotus3_asv\nNo. of ASVs\n\n\n\n\nNow I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow.\n\ntmp_otu &lt;- data.frame(otu_table(physeq))\ntmp_otu_sum &lt;- data.frame(colSums(tmp_otu &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_otu_sum)[2] &lt;- \"lotus3_asv\"\ntmp_read_sum &lt;- data.frame(colSums(tmp_otu)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_read_sum)[2] &lt;- \"lotus3_rc\"\ntmp_track_reads &lt;- read.table(\n    \"files/CUTADAPT/amf_cutadapt_track.txt\",\n    header = TRUE, sep = \"\\t\"\n)\n\ntmp_track_reads$SampleID &lt;- stringr::str_replace_all(\n  tmp_track_reads$SampleID, \"-\", \"_\")\nnames(tmp_track_reads)[2] &lt;- \"raw_rc\"\nnames(tmp_track_reads)[3] &lt;- \"cutadapt_rc\"\n\nThen I want to merge this data with the sample metadata.\n\ntmp_samp &lt;- data.frame(sample_data(physeq)) %&gt;% \n  tibble::rownames_to_column(\"SampleID\")\n\nsample_tab &lt;- dplyr::left_join(tmp_samp, tmp_track_reads, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_read_sum, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_otu_sum, by = \"SampleID\") \nsample_tab &lt;- sample_tab %&gt;%\n  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, \n                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)\n\n\nreadr::write_delim(sample_tab, \"files/CURATE/amf_sample_data_full.txt\",\n    delim = \"\\t\")\n\nsample_tab_trim &lt;- sample_tab[, -c(11:19)]\nsample_tab_trim$per_reads_kept &lt;- \n  round(sample_tab_trim$lotus3_rc/sample_tab_trim$raw_rc, digits = 3)\nsample_tab_trim &lt;- sample_tab_trim %&gt;% \n  dplyr::relocate(per_reads_kept, .before = lotus3_asv)\n\n\n\n\n\nTable 1: Sample metadata including read changes at start and end of workflow.\n\n\n\n\n\n\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#prep-data-for-microeco",
    "href": "curate_amf.html#prep-data-for-microeco",
    "title": "AMF Data Curation",
    "section": "Prep Data for microeco",
    "text": "Prep Data for microeco\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section, microeco tutorial.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the dataset.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))\ntmp_tax[1:6, 1:4]\n\n       Domain    Phylum       Class          Order\nASV7   Eukaryota Mucoromycota Glomeromycetes Glomerales\nASV17  Eukaryota Mucoromycota Glomeromycetes Glomerales\nASV245 Eukaryota Mucoromycota Glomeromycetes Glomerales\nASV214 Eukaryota Mucoromycota Glomeromycetes Glomerales\nASV401 Eukaryota Mucoromycota Glomeromycetes Glomerales\nASV57  Eukaryota Mucoromycota Glomeromycetes Glomerales\nNext we need to add rank definitions (e.g., k__, p__, etc.) to each classification.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))   \ntmp_tax &lt;- tmp_tax %&gt;% dplyr::rename(\"Kingdom\" = \"Domain\")\ntmp_tax &lt;- tmp_tax %&gt;%\n  dplyr::mutate_all(~stringr::str_replace_all(., \"\\\\?\", \"\"))\n\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\ntmp_tax$Species &lt;- paste(\"s\", tmp_tax$Species, sep = \"__\")\n\nAnd now the final, modified taxonomy table.\n\ntmp_tax[1:6, 1:4]\n\n       Kingdom      Phylum          Class             Order\nASV7   k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nASV17  k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nASV245 k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nASV214 k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nASV401 k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nASV57  k__Eukaryota p__Mucoromycota c__Glomeromycetes o__Glomerales\nB. Sequence Table\nHere is what the sequence table looks like in the dataset.\n\ntmp_otu[1:6, 1:3]\n\n       P00_D00_000_NNN P01_D00_010_W4A P01_D10_020_W4A\nASV7              1340               4             657 \nASV17                4               0               0 \nASV245               0               0               0 \nASV214               0               0               0 \nASV401               0               0               0 \nASV57              699               0               0 \n\nidentical(row.names(tmp_otu), row.names(tmp_tax))\n\n[1] TRUE\nC. Sample Table\nHere is what the sample table looks like.\n\nsample_tab[1:6, 1:6]\n\n                       SampleID Plot  Depth Treatment Temp Pairing\nP00_D00_000_NNN P00_D00_000_NNN  P00 00_000  Negative    N       N\nP01_D00_010_W4A P01_D00_010_W4A  P01 00_010      Warm    4       A\nP01_D10_020_W4A P01_D10_020_W4A  P01 10_020      Warm    4       A\nP01_D20_050_W4A P01_D20_050_W4A  P01 20_050      Warm    4       A\nP01_D50_100_W4A P01_D50_100_W4A  P01 50_100      Warm    4       A\nP02_D00_010_C0A P02_D00_010_C0A  P02 00_010   Control    0       A\n\n\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#create-a-microtable-object",
    "href": "curate_amf.html#create-a-microtable-object",
    "title": "AMF Data Curation",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nTip\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV.\n\n\n\nsample_tab &lt;- sample_tab\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_otu\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_tab, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 883 rows and 41 columns\ntax_table have 883 rows and 7 columns\nD. Add Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"working_files/LOTUS3/amf/OTU.fna\")\nidentical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))\n\n[1] TRUE\n\ntmp_fasta_names &lt;- names(rep_fasta)\ntmp_rep_fasta &lt;- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]\nrep_fasta &lt;- tmp_rep_fasta\ntmp_me$rep_fasta &lt;- rep_fasta\nidentical(row.names(tmp_me$tax_table), names(rep_fasta))\ntmp_me$tidy_dataset()\n\n[1] TRUE\n\nme_asv_raw &lt;- microeco::clone(tmp_me)\nme_asv_raw\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 883 rows and 41 columns\ntax_table have 883 rows and 7 columns\nrep_fasta have 883 sequences\n\n\n\nThe microeco object me_asv_raw contains all data from the LotuS3 pipeline, before any dataset curation.",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "curate_amf.html#curate-the-data-set",
    "href": "curate_amf.html#curate-the-data-set",
    "title": "AMF Data Curation",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove non-fungal ASVs\nFirst we can have a quick look at the kingdom and phylum level classifications to see what we are dealing with.\n\ntmp_k &lt;- me_asv_raw$tax_table %&gt;%\n      distinct(Kingdom, .keep_all = TRUE)\n\ntmp_p &lt;- me_asv_raw$tax_table %&gt;%\n      distinct(Phylum, .keep_all = TRUE)\n\n\n\n-------KINGDOM------------------\n\n\nk__Eukaryota\nk__\n\n\n-------PHYLUM------------------\n\n\np__Mucoromycota\np__\np__Ascomycota\np__Basidiomycota\np__Cnidaria\np__Arthropoda\np__Ciliophora\np__Porifera\np__NAMAKO-1\np__Kathablepharidae\np__Gracilipodida\np__Peronosporomycetes\np__Nucleariidae and Fonticula group\np__Zoopagomycota\np__LKM15\np__Aphelidea\np__Chytridiomycota\np__Cercozoa\np__Cryptomycota\np__Neocallimastigomycota\n\n\n\nThe first thing to notice in at the kingdom level we only have Eukaryota and unclassified (or NA). We can remove anything with the NA at the kingdom level.\nAt the phylum level, we can see several non-fungal taxa as well as unclassified. We can remove any non-fungal taxa as well as any phylum level NAs.\n\nThe easiest thing to do is keep everything we want–in other words, all fungal taxa.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\n\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Kingdom == \"k__Eukaryota\")\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Phylum == \"p__Mucoromycota\" | \n               Phylum ==  \"p__Ascomycota\" | \n               Phylum ==  \"p__Basidiomycota\" | \n               Phylum ==  \"p__Peronosporomycetes\" | \n               Phylum ==  \"p__Zoopagomycota\" | \n               Phylum ==  \"p__LKM15\" | \n               Phylum ==  \"p__Aphelidea\" | \n               Phylum ==  \"p__Chytridiomycota\" | \n               Phylum ==  \"p__Cryptomycota\" | \n               Phylum ==  \"p__Neocallimastigomycota\")  \ntmp_no_na$tidy_dataset()\n\n\nme_asv_no_na &lt;- microeco::clone(tmp_no_na)\nme_asv_no_na\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 578 rows and 41 columns\ntax_table have 578 rows and 7 columns\nrep_fasta have 578 sequences\n\n\n\nThe microeco object me_asv_no_na conatins only ASVs classified as Fungi.\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by sub-setting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_na)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, Treatment == \"Negative\")\ntmp_nc$tidy_dataset()\n\n527 taxa with 0 abundance are removed from the otu_table ...\n\nme_asv_nc &lt;- microeco::clone(tmp_nc)\nme_asv_nc\n\n\nThe microeco object me_asv_nc conatins only the negative control sample(s) and associated ASVs.\n\nLooks like there are 51 ASVs in the NC samples from a total of 16700 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\nnc_asvs\n\n\n\n [1] \"ASV7\"   \"ASV17\"  \"ASV57\"  \"ASV265\" \"ASV231\" \"ASV304\" \"ASV275\" \"ASV550\"\n [9] \"ASV380\" \"ASV300\" \"ASV4\"   \"ASV633\" \"ASV462\" \"ASV12\"  \"ASV533\" \"ASV98\" \n[17] \"ASV1\"   \"ASV880\" \"ASV130\" \"ASV71\"  \"ASV28\"  \"ASV347\" \"ASV191\" \"ASV51\" \n[25] \"ASV471\" \"ASV110\" \"ASV278\" \"ASV631\" \"ASV155\" \"ASV21\"  \"ASV229\" \"ASV501\"\n[33] \"ASV120\" \"ASV2\"   \"ASV460\" \"ASV646\" \"ASV158\" \"ASV145\" \"ASV93\"  \"ASV276\"\n[41] \"ASV364\" \"ASV638\" \"ASV63\"  \"ASV212\" \"ASV392\" \"ASV474\" \"ASV102\" \"ASV173\"\n[49] \"ASV655\" \"ASV23\"  \"ASV58\" \n\n\nThere are 51 ASVs found in the NC sample. ASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. That said, there are numerous low number ASVs (e.g. ASV1, ASV2, ASV7, etc.) that are found in NC samples.\nWe can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_na)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(\"NNN\"))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(\"NNN\"))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0))\ntmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\ntmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(\"NNN\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\"))\ntmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(\"NNN\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\ntmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n\n\nnc_check &lt;- tmp_rem_nc_df\n\n\n\n\nTable 2: Summary of ASVs detected in Negative Control (NC) samples.\n\n\n\n\n\n\n\n\n\n Download ASVs in NC samples \nLooking at these data we can see that some of the abundant ASVs like ASV1, ASV2, and ASV7 are found in the negative control sample but make up less than 10% of total reads. Other abundant ASVs like ASV21, ASV51, and ASV57 however are more abundant in the NC sample, comprising greater than 10% of total reads. We should remove any ASVs that are relatively abundant in the NC sample, specifically remove any ASVs where…\n\nthe number of reads found in NC samples accounted for more than 10% of total reads\n\nWe can also consider the following criteria to further filtering out potential contamination.\n\nthe percent of NC samples containing the ASV was greater than 10% of total samples.\n\nHowever, since we only have 1 NC sample the second criteria does not make much sense. Consider ASV71–3940 total reads, 1 of which was found in the NC sample, or 0.025%. So that does not meet our first criteria. But ASV71 is only found in 5 samples (including the NC) which means that the percent of NC samples containing ASV71 is 20%, which meets our second criteria and should be removed. But I think we can safely retain ASV71 or any ASV that is only found in a few samples and low abundance in the NC sample.\nSo instead we will remove any ASVs that are represented by &gt;10% of total reads OR found in fewer than 10% of non-NC samples.\n\nnc_remove &lt;- nc_check %&gt;% \n  dplyr::filter(perc_in_neg &gt; 10 | num_samp_no_nc &lt; 4)\n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n20\n10790\n21495\n33.421\n\n\nRetained\n31\n5910\n187965\n3.048\n\n\n\nWe identified a total of 51 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in less than 10% of non-NC samples. Based on these criteria we removed 20 ASVs from the data set, which represented 10790 total reads in NC samples and 21495 total reads in non-NC samples. Of the total reads removed 33.421% came from NC samples. Of all ASVs identified in NC samples,31 were retained because the fell below the threshhold criteria. These ASVs accounted for 5910 reads in NC samples and 187965 reads in non-NC samples. NC samples accounted for 3.048% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_na)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  dplyr::filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 Treatment != \"Negative\")\ntmp_no_nc$tidy_dataset()\n\n\nme_asv_no_nc &lt;- microeco::clone(tmp_no_nc)\nme_asv_no_nc\n\n\n\nmicrotable-class object:\nsample_table have 40 rows and 19 columns\notu_table have 558 rows and 40 columns\ntax_table have 558 rows and 7 columns\nrep_fasta have 558 sequences\n\n\n\nThe microeco object me_asv_no_nc does not conatins the negative control sample(s).\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts—here we set the threshold to 1000 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\n\nCheck if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).\n\ntmp_row_sums &lt;- rowSums(tmp_no_low$otu_table)\nany(tmp_row_sums == 0)\n\n[1] FALSE\n\nme_asv_no_low &lt;- microeco::clone(tmp_no_low)\nme_asv_no_low\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 19 columns\notu_table have 558 rows and 39 columns\ntax_table have 558 rows and 7 columns\nrep_fasta have 558 sequences\n\n\n\nThe microeco object me_asv_no_low does not conatins the low abundance samples.\n\nWe will quickly add final read counts and ASVs, then create the final microtable object.\n\ntmp_final_rc &lt;- data.frame(me_asv_no_low$sample_sums()) %&gt;% \n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_rc\" = 2)\n\ntmp_final_asv &lt;- data.frame(t(me_asv_no_low$otu_table))\ntmp_final_asv &lt;- data.frame(rowSums(tmp_final_asv &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_asv\" = 2)\ntmp_final &lt;- dplyr::left_join(tmp_final_rc, tmp_final_asv)\n\nsample_tab &lt;- dplyr::full_join(sample_tab, tmp_final) %&gt;% \n        dplyr::relocate(c(final_rc, final_asv), .after = lotus3_asv)\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)\n\nfull_sample_tab &lt;- sample_tab\n\n\ntmp_samp_tab &lt;- sample_tab[complete.cases(sample_tab), ]\ntmp_tax_tab &lt;- me_asv_no_low$tax_table\ntmp_otu_tab &lt;- me_asv_no_low$otu_table\ntmp_rep_fasta &lt;- me_asv_no_low$rep_fasta\n\n\nme_asv &lt;- microtable$new(sample_table = tmp_samp_tab, \n                         otu_table = tmp_otu_tab, \n                         tax_table = tmp_tax_tab, \n                         rep_fasta = tmp_rep_fasta)\nme_asv$tidy_dataset()\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 21 columns\notu_table have 558 rows and 39 columns\ntax_table have 558 rows and 7 columns\nrep_fasta have 558 sequences",
    "crumbs": [
      "B. Data Curation",
      "AMF Data Curation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n\n\n\n\nLinks\n\nmetacrobe\nQuarto"
  },
  {
    "objectID": "curate_its.html",
    "href": "curate_its.html",
    "title": "ITS Data Curation",
    "section": "",
    "text": "On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAll you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_its.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)\n\n\n\n\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#data",
    "href": "curate_its.html#data",
    "title": "ITS Data Curation",
    "section": "",
    "text": "All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_its.Rdata\n\n\n\n LotuS3 phyloseq data",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#citable-resources",
    "href": "curate_its.html#citable-resources",
    "title": "ITS Data Curation",
    "section": "",
    "text": "R microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#references",
    "href": "curate_its.html#references",
    "title": "ITS Data Curation",
    "section": "",
    "text": "Borman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#required-packages",
    "href": "curate_its.html#required-packages",
    "title": "ITS Data Curation",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick here for required R packages.set.seed(919191)\nlibrary(microeco)\nlibrary(mia)\nlibrary(phyloseq)\nlibrary(microbiome)",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#review-results",
    "href": "curate_its.html#review-results",
    "title": "ITS Data Curation",
    "section": "Review Results",
    "text": "Review Results\nThe LotuS3 pipeline produces numerous output files but for our purposes there are four specific files we are interested in:\n\nphyloseq.Rdata\nOTU.fna\nOTU.txt\nhiera_BLAST.txt\n\nWe will mainly work from the phyloseq.Rdata (renamed phyloseq_its.Rdata) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis.\n\nload(\"working_files/LOTUS3/its/phyloseq_its.Rdata\")\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 2168 taxa and 40 samples ]:\nsample_data() Sample Data:        [ 40 samples by 14 sample variables ]:\ntax_table()   Taxonomy Table:     [ 2168 taxa by 7 taxonomic ranks ]:\nphy_tree()    Phylogenetic Tree:  [ 2168 tips and 2166 internal nodes ]:\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n3\n\n\nMax. number of reads\n66363\n\n\nTotal number of reads\n608393\n\n\nAverage number of reads\n15210\n\n\nMedian number of reads\n3057.5\n\n\nTotal ASVs\n2168\n\n\nNumber of singleton ASVs\nNA\n\n\nAny ASVs sum to 1 or less?\nFALSE\n\n\nPercent of ASVs that are singletons\nNA\n\n\nSparsity\n0.919\n\n\n\nIn this next part of the workflow the goal is to create a microtable object using the R package microeco (Liu et al. 2021). The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\nWe will also:\n\nRemove any ASVs without kingdom level classification.\n\nRevome any contaminants (chloroplast, mitochondria, etc.).\n\nRemove Negative Control (NC) samples.\n\nRemove any low-count samples.",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#sample-data",
    "href": "curate_its.html#sample-data",
    "title": "ITS Data Curation",
    "section": "Sample Data",
    "text": "Sample Data\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nNew sample ID based on plot, depth, treatment, & pair\n\n\nPlot\nExperimental plot ID\n\n\nTreatment\nControl vs warming\n\n\nTemp\nWarming treatment temperature\n\n\nDepth\nSampling depth in soil\n\n\nPairing\nSample pairings\n\n\nraw_rc\nNo. of raw reads\n\n\ncutadapt_rc\nRead count after cutdapt\n\n\nlotus3_rc\nRead count after LotuS3 pipeline\n\n\nlotus3_asv\nNo. of ASVs\n\n\n\n\nNow I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow.\n\ntmp_otu &lt;- data.frame(otu_table(physeq))\ntmp_otu_sum &lt;- data.frame(colSums(tmp_otu &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_otu_sum)[2] &lt;- \"lotus3_asv\"\ntmp_read_sum &lt;- data.frame(colSums(tmp_otu)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_read_sum)[2] &lt;- \"lotus3_rc\"\ntmp_track_reads &lt;- read.table(\n    \"files/CUTADAPT/its_cutadapt_track.txt\",\n    header = TRUE, sep = \"\\t\"\n)\n\ntmp_track_reads$SampleID &lt;- stringr::str_replace_all(\n  tmp_track_reads$SampleID, \"-\", \"_\")\nnames(tmp_track_reads)[2] &lt;- \"raw_rc\"\nnames(tmp_track_reads)[3] &lt;- \"cutadapt_rc\"\n\nThen I want to merge this data with the sample metadata.\n\ntmp_samp &lt;- data.frame(sample_data(physeq)) %&gt;% \n  tibble::rownames_to_column(\"SampleID\")\n\nsample_tab &lt;- dplyr::left_join(tmp_samp, tmp_track_reads, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_read_sum, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_otu_sum, by = \"SampleID\") \nsample_tab &lt;- sample_tab %&gt;%\n  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, \n                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)\n\n\nreadr::write_delim(sample_tab, \"files/CURATE/its_sample_data_full.txt\",\n    delim = \"\\t\")\n\nsample_tab_trim &lt;- sample_tab[, -c(11:19)]\nsample_tab_trim$per_reads_kept &lt;- \n  round(sample_tab_trim$lotus3_rc/sample_tab_trim$raw_rc, digits = 3)\nsample_tab_trim &lt;- sample_tab_trim %&gt;% \n  dplyr::relocate(per_reads_kept, .before = lotus3_asv)\n\n\n\n\n\nTable 1: Sample metadata including read changes at start and end of workflow.\n\n\n\n\n\n\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#prep-data-for-microeco",
    "href": "curate_its.html#prep-data-for-microeco",
    "title": "ITS Data Curation",
    "section": "Prep Data for microeco",
    "text": "Prep Data for microeco\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section, microeco tutorial.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the dataset.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))\ntmp_tax[1:6, 1:4]\n\n        Domain Phylum     Class           Order\nASV1495 Fungi  Ascomycota ?               ?\nASV1740 Fungi  Ascomycota Sordariomycetes Sordariales\nASV697  Fungi  Ascomycota Sordariomycetes Sordariales\nASV1118 Fungi  Ascomycota Sordariomycetes Sordariales\nASV796  Fungi  Ascomycota Sordariomycetes Hypocreales\nASV118  Fungi  Ascomycota Sordariomycetes ?\nNext we need to add rank definitions (e.g., k__, p__, etc.) to each classification.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))   \ntmp_tax &lt;- tmp_tax %&gt;% dplyr::rename(\"Kingdom\" = \"Domain\")\ntmp_tax &lt;- tmp_tax %&gt;%\n  dplyr::mutate_all(~stringr::str_replace_all(., \"\\\\?\", \"\"))\n\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\ntmp_tax$Species &lt;- paste(\"s\", tmp_tax$Species, sep = \"__\")\n\nAnd now the final, modified taxonomy table.\n\ntmp_tax[1:6, 1:4]\n\n        Kingdom  Phylum        Class              Order\nASV1495 k__Fungi p__Ascomycota c__                o__\nASV1740 k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales\nASV697  k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales\nASV1118 k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales\nASV796  k__Fungi p__Ascomycota c__Sordariomycetes o__Hypocreales\nASV118  k__Fungi p__Ascomycota c__Sordariomycetes o__\nB. Sequence Table\nHere is what the sequence table looks like in the dataset.\n\ntmp_otu[1:6, 1:3]\n\n        P01_D00_010_W4A P01_D10_020_W4A P01_D20_050_W4A\nASV1059               0               0               0 \nASV1495               0               0               0 \nASV1740               2               0               0 \nASV697                0               0               0 \nASV1118               2               0               0 \nASV796                0               0               0 \n\nidentical(row.names(tmp_otu), row.names(tmp_tax))\n\n[1] TRUE\nC. Sample Table\nHere is what the sample table looks like.\n\nsample_tab[1:6, 1:6]\n\n                       SampleID Plot  Depth Treatment Temp Pairing\nP01_D00_010_W4A P01_D00_010_W4A  P01 00_010      Warm    4       A\nP01_D10_020_W4A P01_D10_020_W4A  P01 10_020      Warm    4       A\nP01_D20_050_W4A P01_D20_050_W4A  P01 20_050      Warm    4       A\nP01_D50_100_W4A P01_D50_100_W4A  P01 50_100      Warm    4       A\nP02_D00_010_C0A P02_D00_010_C0A  P02 00_010   Control    0       A\nP02_D10_020_C0A P02_D10_020_C0A  P02 10_020   Control    0       A\n\n\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#create-a-microtable-object",
    "href": "curate_its.html#create-a-microtable-object",
    "title": "ITS Data Curation",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nTip\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV.\n\n\n\nsample_tab &lt;- sample_tab\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_otu\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_tab, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 40 rows and 19 columns\notu_table have 2168 rows and 40 columns\ntax_table have 2168 rows and 7 columns\nD. Add Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"working_files/LOTUS3/its/OTU.fna\")\nidentical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))\n\n[1] TRUE\n\ntmp_fasta_names &lt;- names(rep_fasta)\ntmp_rep_fasta &lt;- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]\nrep_fasta &lt;- tmp_rep_fasta\ntmp_me$rep_fasta &lt;- rep_fasta\nidentical(row.names(tmp_me$tax_table), names(rep_fasta))\ntmp_me$tidy_dataset()\n\n[1] TRUE\n\nme_asv_raw &lt;- microeco::clone(tmp_me)\nme_asv_raw\n\n\n\nmicrotable-class object:\nsample_table have 40 rows and 19 columns\notu_table have 2168 rows and 40 columns\ntax_table have 2168 rows and 7 columns\nrep_fasta have 2168 sequences\n\n\n\nThe microeco object me_asv_raw contains all data from the LotuS3 pipeline, before any dataset curation.",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_its.html#curate-the-data-set",
    "href": "curate_its.html#curate-the-data-set",
    "title": "ITS Data Curation",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove any Kingdom NAs\nHere we can just use the straight up subset command since we do not need to worry about any ranks above Kingdom also being removed.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Kingdom == \"k__Fungi\")\ntmp_no_na$tidy_dataset()\n\n\nme_asv_no_na &lt;- microeco::clone(tmp_no_na)\nme_asv_no_na\n\n\n\nmicrotable-class object:\nsample_table have 40 rows and 19 columns\notu_table have 1718 rows and 40 columns\ntax_table have 1718 rows and 7 columns\nrep_fasta have 1718 sequences\n\n\n\nThe microeco object me_asv_no_na conatins only ASVs classified as Archaea or Bacteria.\n\nSince there no contaminants (mitochondria and/or chloroplasts) and no negative control samples in this dataset we can skip to the process of removing low-count samples.\nRemove Low-Count Samples\n\ntmp_df &lt;- data.frame(me_asv_no_na$sample_sums())\ntmp_df[order(tmp_df[, 1], decreasing = FALSE), ]\n\n [1]     3   969   973  1031  1033  1073  1081  1093  1109  1147  1203  1217\n[13]  1387  1468  1531  1844  2062  2239  2422  2610  2864  3138  3631  3672\n[25]  3985  5152 11773 17358 20741 23785 28631 31723 32527 36160 37610 41556\n[37] 45208 46749 52279 53184\n\n\nWe can see that almost all samples have more to than 1000 reads. Two are slightly below but we can keep those and just remove samples with really low read counts–here we set the threshold to 500 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_na)\ntmp_no_low$otu_table &lt;- tmp_no_na$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 500))\ntmp_no_low$tidy_dataset()\n\nCheck if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).\n\ntmp_row_sums &lt;- rowSums(tmp_no_low$otu_table)\nany(tmp_row_sums == 0)\n\n[1] FALSE\n\nme_asv_no_low &lt;- microeco::clone(tmp_no_low)\nme_asv_no_low\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 19 columns\notu_table have 1718 rows and 39 columns\ntax_table have 1718 rows and 7 columns\nrep_fasta have 1718 sequences\n\n\n\nThe microeco object me_asv_no_low does not conatins the low abundance samples.\n\nWe will quickly add final read counts and ASVs, then create the final microtable object.\n\ntmp_final_rc &lt;- data.frame(me_asv_no_low$sample_sums()) %&gt;% \n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_rc\" = 2)\n\ntmp_final_asv &lt;- data.frame(t(me_asv_no_low$otu_table))\ntmp_final_asv &lt;- data.frame(rowSums(tmp_final_asv &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_asv\" = 2)\ntmp_final &lt;- dplyr::left_join(tmp_final_rc, tmp_final_asv)\n\nsample_tab &lt;- dplyr::full_join(sample_tab, tmp_final) %&gt;% \n        dplyr::relocate(c(final_rc, final_asv), .after = lotus3_asv)\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)\n\nfull_sample_tab &lt;- sample_tab\n\n\ntmp_samp_tab &lt;- sample_tab[complete.cases(sample_tab), ]\ntmp_tax_tab &lt;- me_asv_no_low$tax_table\ntmp_otu_tab &lt;- me_asv_no_low$otu_table\ntmp_rep_fasta &lt;- me_asv_no_low$rep_fasta\n\n\nme_asv &lt;- microtable$new(sample_table = tmp_samp_tab, \n                         otu_table = tmp_otu_tab, \n                         tax_table = tmp_tax_tab, \n                         rep_fasta = tmp_rep_fasta)\nme_asv$tidy_dataset()\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 21 columns\notu_table have 1718 rows and 39 columns\ntax_table have 1718 rows and 7 columns\nrep_fasta have 1718 sequences",
    "crumbs": [
      "B. Data Curation",
      "ITS Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html",
    "href": "curate_ssu.html",
    "title": "SSU Data Curation",
    "section": "",
    "text": "On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.\n\nAll you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_ssu.Rdata\n\n\n\n LotuS3 phyloseq data \n\n\n\n\n\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)\n\n\n\n\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#data",
    "href": "curate_ssu.html#data",
    "title": "SSU Data Curation",
    "section": "",
    "text": "All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.\n\n\n\n\n\n\n   phyloseq_ssu.Rdata\n\n\n\n LotuS3 phyloseq data",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#citable-resources",
    "href": "curate_ssu.html#citable-resources",
    "title": "SSU Data Curation",
    "section": "",
    "text": "R microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nmiaverse (Borman et al. 2024)",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#references",
    "href": "curate_ssu.html#references",
    "title": "SSU Data Curation",
    "section": "",
    "text": "Borman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#required-packages",
    "href": "curate_ssu.html#required-packages",
    "title": "SSU Data Curation",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick here for required R packages.set.seed(919191)\nlibrary(microeco)\nlibrary(mia)\nlibrary(phyloseq)\nlibrary(microbiome)",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#review-results",
    "href": "curate_ssu.html#review-results",
    "title": "SSU Data Curation",
    "section": "Review Results",
    "text": "Review Results\nThe LotuS3 pipeline produces numerous output files but for our purposes there are four specific files we are interested in:\n\nphyloseq.Rdata\nOTU.fna\nOTU.txt\nhiera_BLAST.txt\n\nWe will mainly work from the phyloseq.Rdata (renamed phyloseq_ssu.Rdata) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis.\n\nload(\"working_files/LOTUS3/ssu/phyloseq_ssu.Rdata\")\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 12091 taxa and 41 samples ]\nsample_data() Sample Data:       [ 41 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 12091 taxa by 7 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 12091 tips and 12089 internal nodes ]\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n2\n\n\nMax. number of reads\n196476\n\n\nTotal number of reads\n2300907\n\n\nAverage number of reads\n56120\n\n\nMedian number of reads\n16754\n\n\nTotal ASVs\n12091\n\n\nNumber of singleton ASVs\n39\n\n\nAny ASVs sum to 1 or less?\nTRUE\n\n\nPercent of ASVs that are singletons\n0.323\n\n\nSparsity\n0.725\n\n\n\nIn this next part of the workflow the goal is to create a microtable object using the R package microeco (Liu et al. 2021). The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\nWe will also:\n\nRemove any ASVs without kingdom level classification.\n\nRevome any contaminants (chloroplast, mitochondria, etc.).\n\nRemove Negative Control (NC) samples.\n\nRemove any low-count samples.",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#sample-data",
    "href": "curate_ssu.html#sample-data",
    "title": "SSU Data Curation",
    "section": "Sample Data",
    "text": "Sample Data\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nNew sample ID based on plot, depth, treatment, & pair\n\n\nPlot\nExperimental plot ID\n\n\nTreatment\nControl vs warming\n\n\nTemp\nWarming treatment temperature\n\n\nDepth\nSampling depth in soil\n\n\nPairing\nSample pairings\n\n\nraw_rc\nNo. of raw reads\n\n\ncutadapt_rc\nRead count after cutdapt\n\n\nlotus3_rc\nRead count after LotuS3 pipeline\n\n\nlotus3_asv\nNo. of LotuS3 ASVs\n\n\n\n\nNow I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow.\n\ntmp_otu &lt;- data.frame(otu_table(physeq))\ntmp_otu_sum &lt;- data.frame(colSums(tmp_otu &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_otu_sum)[2] &lt;- \"lotus3_asv\"\ntmp_read_sum &lt;- data.frame(colSums(tmp_otu)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_read_sum)[2] &lt;- \"lotus3_rc\"\ntmp_track_reads &lt;- read.table(\n    \"files/CUTADAPT/ssu_cutadapt_track.txt\",\n    header = TRUE, sep = \"\\t\"\n)\n\ntmp_track_reads$SampleID &lt;- stringr::str_replace_all(\n  tmp_track_reads$SampleID, \"-\", \"_\")\nnames(tmp_track_reads)[2] &lt;- \"raw_rc\"\nnames(tmp_track_reads)[3] &lt;- \"cutadapt_rc\"\n\nThen I want to merge this data with the sample metadata.\n\ntmp_samp &lt;- data.frame(sample_data(physeq)) %&gt;% \n  tibble::rownames_to_column(\"SampleID\")\n\nsample_tab &lt;- dplyr::left_join(tmp_samp, tmp_track_reads, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_read_sum, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_otu_sum, by = \"SampleID\") \nsample_tab &lt;- sample_tab %&gt;%\n  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, \n                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)\n\n\nreadr::write_delim(sample_tab, \"files/CURATE/ssu_sample_data_full.txt\",\n    delim = \"\\t\")\n\nsample_tab_trim &lt;- sample_tab[, -c(11:19)]\nsample_tab_trim$per_reads_kept &lt;- \n  round(sample_tab_trim$lotus3_rc/sample_tab_trim$raw_rc, digits = 3)\nsample_tab_trim &lt;- sample_tab_trim %&gt;% \n  dplyr::relocate(per_reads_kept, .before = lotus3_asv)\n\n\n\n\n\nTable 1: Sample metadata including read changes at start and end of workflow.\n\n\n\n\n\n\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#prep-data-for-microeco",
    "href": "curate_ssu.html#prep-data-for-microeco",
    "title": "SSU Data Curation",
    "section": "Prep Data for microeco",
    "text": "Prep Data for microeco\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section, microeco tutorial.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the dataset.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))\ntmp_tax[1:6, 1:4]\n\n           Domain   Phylum      Class      Order\nASV11944   Bacteria Bacillota   Bacilli    RES148\nASV8741    Bacteria Bacillota_A Clostridia Christensenellales\nASV11355   Bacteria Bacillota_A Clostridia Christensenellales\nASV6303    Bacteria Bacillota_A Clostridia Christensenellales\nASV9863    Bacteria Bacillota_A Clostridia Christensenellales\nASV11366   Bacteria Bacillota_A Clostridia Christensenellales\nNext we need to add rank definitions (e.g., k__, p__, etc.) to each classification.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))   \ntmp_tax &lt;- tmp_tax %&gt;% dplyr::rename(\"Kingdom\" = \"Domain\")\ntmp_tax &lt;- tmp_tax %&gt;%\n  dplyr::mutate_all(~stringr::str_replace_all(., \"\\\\?\", \"\"))\n\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\ntmp_tax$Species &lt;- paste(\"s\", tmp_tax$Species, sep = \"__\")\n\nAnd now the final, modified taxonomy table.\n\ntmp_tax[1:6, 1:4]\n\n          Kingdom     Phylum         Class         Order\nASV11944  k__Bacteria p__Bacillota   c__Bacilli    o__RES148\nASV8741   k__Bacteria p__Bacillota_A c__Clostridia o__Christensenellales\nASV11355  k__Bacteria p__Bacillota_A c__Clostridia o__Christensenellales\nASV6303   k__Bacteria p__Bacillota_A c__Clostridia o__Christensenellales\nASV9863   k__Bacteria p__Bacillota_A c__Clostridia o__Christensenellales\nASV11366  k__Bacteria p__Bacillota_A c__Clostridia o__Christensenellales\nB. Sequence Table\nHere is what the sequence table looks like in the dataset.\n\ntmp_otu[1:6, 1:3]\n\n          P00_D00_000_NNN P01_D00_010_W4A P01_D10_020_W4A\nASV11944                0               1               0   \nASV8741                 0               3               1   \nASV11355                0               2               0   \nASV6303                 0               5               0   \nASV9863                 0               8               1   \nASV11366                0               0               0   \n\nidentical(row.names(tmp_otu), row.names(tmp_tax))\n\n[1] TRUE\nC. Sample Table\nHere is what the sample table looks like.\n\nsample_tab[1:6, 1:6]\n\n                       SampleID Plot  Depth Treatment Temp Pairing\nP00_D00_000_NNN P00_D00_000_NNN  P00 00_000  Negative    N       N\nP01_D00_010_W4A P01_D00_010_W4A  P01 00_010      Warm    4       A\nP01_D10_020_W4A P01_D10_020_W4A  P01 10_020      Warm    4       A\nP01_D20_050_W4A P01_D20_050_W4A  P01 20_050      Warm    4       A\nP01_D50_100_W4A P01_D50_100_W4A  P01 50_100      Warm    4       A\nP02_D00_010_C0A P02_D00_010_C0A  P02 00_010   Control    0       A\n\n\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#create-a-microtable-object",
    "href": "curate_ssu.html#create-a-microtable-object",
    "title": "SSU Data Curation",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nTip\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV.\n\n\n\nsample_tab &lt;- sample_tab\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_otu\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_tab, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12091 rows and 41 columns\ntax_table have 12091 rows and 7 columns\nD. Add Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"working_files/LOTUS3/ssu/OTU.fna\")\nidentical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))\n\n[1] TRUE\n\ntmp_fasta_names &lt;- names(rep_fasta)\ntmp_rep_fasta &lt;- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]\nrep_fasta &lt;- tmp_rep_fasta\ntmp_me$rep_fasta &lt;- rep_fasta\nidentical(row.names(tmp_me$tax_table), names(rep_fasta))\ntmp_me$tidy_dataset()\n\n[1] TRUE\n\nme_asv_raw &lt;- microeco::clone(tmp_me)\nme_asv_raw\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 12091 rows and 41 columns\ntax_table have 12091 rows and 7 columns\nrep_fasta have 12091 sequences\n\n\n\nThe microeco object me_asv_raw contains all data from the LotuS3 pipeline, before any dataset curation.",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "curate_ssu.html#curate-the-data-set",
    "href": "curate_ssu.html#curate-the-data-set",
    "title": "SSU Data Curation",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove any Kingdom NAs\nHere we can just use the straight up subset command since we do not need to worry about any ranks above Kingdom also being removed.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\ntmp_no_na$tidy_dataset()\n\n\nme_asv_no_na &lt;- microeco::clone(tmp_no_na)\nme_asv_no_na\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 12018 rows and 41 columns\ntax_table have 12018 rows and 7 columns\nrep_fasta have 12018 sequences\n\n\n\nThe microeco object me_asv_no_na conatins only ASVs classified as Archaea or Bacteria.\n\nRemove Contaminants\nNow we can remove any potential contaminants like mitochondria or chloroplasts.\n\ntmp_no_cont &lt;- microeco::clone(tmp_no_na)\ntmp_no_cont$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\ntmp_no_cont$tidy_dataset()\n\nTotal 17 features are removed from tax_table ...\n\nme_asv_no_cont &lt;- microeco::clone(tmp_no_cont)\nme_asv_no_cont\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 19 columns\notu_table have 12001 rows and 41 columns\ntax_table have 12001 rows and 7 columns\nrep_fasta have 12001 sequences\n\n\n\nThe microeco object me_asv_no_cont does not conatins contain ASVs calssified as mitochondria or chloroplast.\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, Treatment == \"Negative\")\ntmp_nc$tidy_dataset()\n\n11974 taxa with 0 abundance are removed from the otu_table ...\n\nme_asv_nc &lt;- microeco::clone(tmp_nc)\nme_asv_nc\n\n\n\nmicrotable-class object:\nsample_table have 1 rows and 19 columns\notu_table have 27 rows and 1 columns\ntax_table have 27 rows and 7 columns\nrep_fasta have 27 sequences\n\n\n\nThe microeco object me_asv_nc conatins only the negative control sample(s) and associated ASVs.\n\nLooks like there are 27 ASVs in the NC samples from a total of 359 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\nnc_asvs\n\n\n\n [1] \"ASV9992\"  \"ASV3670\"  \"ASV2401\"  \"ASV7644\"  \"ASV9669\"  \"ASV4853\" \n [7] \"ASV6060\"  \"ASV3886\"  \"ASV1\"     \"ASV8245\"  \"ASV2651\"  \"ASV2615\" \n[13] \"ASV4053\"  \"ASV2791\"  \"ASV4425\"  \"ASV4582\"  \"ASV8443\"  \"ASV4651\" \n[19] \"ASV7994\"  \"ASV5320\"  \"ASV5932\"  \"ASV6596\"  \"ASV4883\"  \"ASV4302\" \n[25] \"ASV1155\"  \"ASV1663\"  \"ASV10187\"\n\n\nThere are 27 ASVs found in the NC sample. ASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(\"NNN\"))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(\"NNN\"))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0))\ntmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\ntmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(\"NNN\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\"))\ntmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(\"NNN\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\ntmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n\n\nnc_check &lt;- tmp_rem_nc_df\n\n\n\n\nTable 2: Summary of ASVs detected in Negative Control (NC) samples.\n\n\n\n\n\n\n\n\n\n Download ASVs in NC samples \nLooking at these data we can see that ASVs like ASV1 are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV6060, ASV9669, and ASV9992 are relatively abundant in NC samples. We decided to remove ASVs if:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  dplyr::filter(perc_in_neg &gt; 10 | perc_in_neg_samp &gt; 10)\n#tmp_rem_asv &lt;- nc_remove$ASV_ID %&gt;% \n#  unlist(strsplit(., split = \", \")) \n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n17\n295\n580\n33.714\n\n\nRetained\n10\n64\n65084\n0.098\n\n\n\nWe identified a total of 27 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 17 ASVs from the data set, which represented 295 total reads in NC samples and 580 total reads in non-NC samples. Of the total reads removed 33.714% came from NC samples. Of all ASVs identified in NC samples,10 were retained because the fell below the threshhold criteria. These ASVs accounted for 64 reads in NC samples and 65084 reads in non-NC samples. NC samples accounted for 0.098% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_cont)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  dplyr::filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 Treatment != \"Negative\")\ntmp_no_nc$tidy_dataset()\n\n\nme_asv_no_nc &lt;- microeco::clone(tmp_no_nc)\nme_asv_no_nc\n\n\n\nmicrotable-class object:\nsample_table have 40 rows and 19 columns\notu_table have 11984 rows and 40 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences\n\n\n\nThe microeco object me_asv_no_nc does not conatins the negative control sample(s).\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts—here we set the threshold to 1000 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\n\nCheck if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).\n\ntmp_row_sums &lt;- rowSums(tmp_no_low$otu_table)\nany(tmp_row_sums == 0)\n\n[1] FALSE\n\nme_asv_no_low &lt;- microeco::clone(tmp_no_low)\nme_asv_no_low\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 19 columns\notu_table have 11984 rows and 39 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences\n\n\n\nThe microeco object me_asv_no_low does not conatins the low abundance samples.\n\nWe will quickly add final read counts and ASVs, then create the final microtable object.\n\ntmp_final_rc &lt;- data.frame(me_asv_no_low$sample_sums()) %&gt;% \n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_rc\" = 2)\n\ntmp_final_asv &lt;- data.frame(t(me_asv_no_low$otu_table))\ntmp_final_asv &lt;- data.frame(rowSums(tmp_final_asv &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\") %&gt;% \n  dplyr::rename(\"final_asv\" = 2)\ntmp_final &lt;- dplyr::left_join(tmp_final_rc, tmp_final_asv)\n\nsample_tab &lt;- dplyr::full_join(sample_tab, tmp_final) %&gt;% \n        dplyr::relocate(c(final_rc, final_asv), .after = lotus3_asv)\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)\n\nfull_sample_tab &lt;- sample_tab\n\n\ntmp_samp_tab &lt;- sample_tab[complete.cases(sample_tab), ]\ntmp_tax_tab &lt;- me_asv_no_low$tax_table\ntmp_otu_tab &lt;- me_asv_no_low$otu_table\ntmp_rep_fasta &lt;- me_asv_no_low$rep_fasta\n\n\nme_asv &lt;- microtable$new(sample_table = tmp_samp_tab, \n                         otu_table = tmp_otu_tab, \n                         tax_table = tmp_tax_tab, \n                         rep_fasta = tmp_rep_fasta)\nme_asv$tidy_dataset()\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 21 columns\notu_table have 11984 rows and 39 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences",
    "crumbs": [
      "B. Data Curation",
      "SSU Data Curation"
    ]
  },
  {
    "objectID": "lotus3.html",
    "href": "lotus3.html",
    "title": "ASV Inference",
    "section": "",
    "text": "This workflow uses the LotuS3 pipeline to infer ASVs, remove chimeric reads, and assign taxonomy. Included are links to R scripts and associated processing files.\n\nHere is everything you need to run this workflow:\n\n\nQuick links to trimmed fastq data and mapping files for LotuS3.\n\n\n\n\n\n\nDataset\nTrimmed fastq files\nMapping file\n\n\n\n16S rRNA (ssu)\n\nSSU trimmed fastq files\n\n SSU map \n\n\nITS (its)\n\nITS trimmed fastq files\n\n ITS map \n\n\nAMF (amf)\n\nAMF trimmed fastq files\n\n AMF map \n\n\nOomycete (oo)\n\nOO trimmed fastq files\n\n OO map \n\n\n\n\nOnce you have the trimmed fastq files, the mapping files, and have LotuS3 installed, you can execute the following commands to run the pipelines.\n\n\n\n\n\n\nNote\n\n\n\nFor the ITS and Oomycete datasets I used a custom formatted taxon DB. See the section on creating a custom formatted database to find the code I used for this.\n\n\n\nSSU\nlotus3 -i . -map ssu_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer GTGCCAGCMGCCGCGGTAA -reversePrimer GGACTACHVGGGTWTCTAAT -clustering dada2 -refDB SLV -taxAligner lambda -threads 20\n\nITS\nlotus3 -i . -map its_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS  -forwardPrimer CTTGGTCATTTAGAGGAAGTAA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\nAMF\nlotus3 -i .-map amf_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer AAGCTCGTAGTTGAATTTCG -reversePrimer CCCAACTATCCCTATTAATCAT -clustering dada2 -refDB SLV -taxAligner lambda -t 20\n\nOomycete\nlotus3 -i . -map oo_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS -forwardPrimer GGAAGGATCATTACCACA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n\nFor a full description of each workflow and summaries of the results, please see the relevant sections below.\n\n\n\n\nLotuS3 An ultrafast and highly accurate tool for amplicon sequencing analysis (Özkurt et al. 2022).\nDADA2 ASV clustering (Callahan et al. 2016).\nVSEARCH v2.30.0 (chimera de novo / ref; OTU alignments) (Rognes et al. 2016).\nPoisson binomial model based read filtering (Puente-Sánchez, Aguirre, and Parro 2016).\nOfftarget removal (against phiX) (Bedarf et al. 2021).\nminimap2 v2.30 used in offtarget aligments (Li 2018).\nLULU multicopy rRNA removal (Frøslev et al. 2017).\nLambda3 taxonomic similarity search (Hauswedell et al. 2024).\nKSGP SSU specific tax database (for SSU workflow) (Grant et al. 2023).\nITSx v1.3 removal of non ITS OTUs (for ITS and Oomycete workflows) (Bengtsson-Palme et al. 2013)\n\nSILVA 16S/18S database (for AMF workflow) (Yilmaz et al. 2014)\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nSeqKit2 for sequence and alignment processing (Shen, Sipos, and Zhao 2024)\n\n\n\n\n\n\n\nBedarf, Janis R, Naiara Beraza, Hassan Khazneh, Ezgi Özkurt, David Baker, Valeri Borger, Ullrich Wüllner, and Falk Hildebrand. 2021. “Much Ado about Nothing? Off-Target Amplification Can Lead to False-Positive Bacterial Brain Microbiome Detection in Healthy and Parkinson’s Disease Individuals.” Microbiome 9 (1): 75. https://doi.org/10.1186/s40168-021-01012-1.\n\n\nBengtsson-Palme, Johan, Martin Ryberg, Martin Hartmann, Sara Branco, Zheng Wang, Anna Godhe, Pierre De Wit, et al. 2013. “Improved Software Detection and Extraction of ITS1 and ITS 2 from Ribosomal ITS Sequences of Fungi and Other Eukaryotes for Analysis of Environmental Sequencing Data.” Methods in Ecology and Evolution 4 (10): 914–19. https://doi.org/10.1111/2041-210X.12073.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nFrøslev, Tobias Guldberg, Rasmus Kjøller, Hans Henrik Bruun, Rasmus Ejrnæs, Ane Kirstine Brunbjerg, Carlotta Pietroni, and Anders Johannes Hansen. 2017. “Algorithm for Post-Clustering Curation of DNA Amplicon Data Yields Reliable Biodiversity Estimates.” Nature Communications 8 (1): 1188. https://doi.org/10.1038/s41467-017-01312-x.\n\n\nGrant, Alastair, Abdullah Aleidan, Charli S Davies, Solomon C Udochi, Joachim Fritscher, Mohammad Bahram, and Falk Hildebrand. 2023. “Improved Taxonomic Annotation of Archaea Communities Using LotuS2, the Genome Taxonomy Database and RNAseq Data.” bioRxiv, 2023–08. https://doi.org/10.1101/2023.08.21.554127.\n\n\nHauswedell, Hannes, Sara Hetzel, Simon G Gottlieb, Helene Kretzmer, Alexander Meissner, and Knut Reinert. 2024. “Lambda3: Homology Search for Protein, Nucleotide, and Bisulfite-Converted Sequences.” Bioinformatics 40 (3): btae097. https://doi.org/10.1093/bioinformatics/btae097.\n\n\nLi, Heng. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.\n\n\nÖzkurt, Ezgi, Joachim Fritscher, Nicola Soranzo, Duncan YK Ng, Robert P Davey, Mohammad Bahram, and Falk Hildebrand. 2022. “LotuS2: An Ultrafast and Highly Accurate Tool for Amplicon Sequencing Analysis.” Microbiome 10 (1): 176. https://doi.org/10.1186/s40168-022-01365-1.\n\n\nPuente-Sánchez, Fernando, Jacobo Aguirre, and Vı́ctor Parro. 2016. “A Novel Conceptual Approach to Read-Filtering in High-Throughput Amplicon Sequencing Studies.” Nucleic Acids Research 44 (4): e40–40. https://doi.org/10.1093/nar/gkv1113.\n\n\nRognes, Torbjørn, Tomáš Flouri, Ben Nichols, Christopher Quince, and Frédéric Mahé. 2016. “VSEARCH: A Versatile Open Source Tool for Metagenomics.” PeerJ 4: e2584. https://doi.org/10.7717/peerj.2584.\n\n\nShen, Wei, Botond Sipos, and Liuyang Zhao. 2024. “SeqKit2: A Swiss Army Knife for Sequence and Alignment Processing.” Imeta 3 (3): e191. https://doi.org/10.1002/imt2.191.\n\n\nYilmaz, Pelin, Laura Wegener Parfrey, Pablo Yarza, Jan Gerken, Elmar Pruesse, Christian Quast, Timmy Schweer, Jörg Peplies, Wolfgang Ludwig, and Frank Oliver Glöckner. 2014. “The SILVA and ‘All-Species Living Tree Project (LTP)’ Taxonomic Frameworks.” Nucleic Acids Research 42 (D1): D643–48. https://doi.org/10.1093/nar/gkt1209.",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  },
  {
    "objectID": "lotus3.html#data-scripts",
    "href": "lotus3.html#data-scripts",
    "title": "ASV Inference",
    "section": "",
    "text": "Here is everything you need to run this workflow:\n\n\nQuick links to trimmed fastq data and mapping files for LotuS3.\n\n\n\n\n\n\nDataset\nTrimmed fastq files\nMapping file\n\n\n\n16S rRNA (ssu)\n\nSSU trimmed fastq files\n\n SSU map \n\n\nITS (its)\n\nITS trimmed fastq files\n\n ITS map \n\n\nAMF (amf)\n\nAMF trimmed fastq files\n\n AMF map \n\n\nOomycete (oo)\n\nOO trimmed fastq files\n\n OO map \n\n\n\n\nOnce you have the trimmed fastq files, the mapping files, and have LotuS3 installed, you can execute the following commands to run the pipelines.\n\n\n\n\n\n\nNote\n\n\n\nFor the ITS and Oomycete datasets I used a custom formatted taxon DB. See the section on creating a custom formatted database to find the code I used for this.\n\n\n\nSSU\nlotus3 -i . -map ssu_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer GTGCCAGCMGCCGCGGTAA -reversePrimer GGACTACHVGGGTWTCTAAT -clustering dada2 -refDB SLV -taxAligner lambda -threads 20\n\nITS\nlotus3 -i . -map its_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS  -forwardPrimer CTTGGTCATTTAGAGGAAGTAA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\nAMF\nlotus3 -i .-map amf_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq.txt -p miSeq -amplicon_type SSU  -forwardPrimer AAGCTCGTAGTTGAATTTCG -reversePrimer CCCAACTATCCCTATTAATCAT -clustering dada2 -refDB SLV -taxAligner lambda -t 20\n\nOomycete\nlotus3 -i . -map oo_miSeqMap.sm.txt -o LOTUS3_ASV -sdmopt sdm_miSeq_ITS.txt -p miSeq -amplicon_type ITS -forwardPrimer GGAAGGATCATTACCACA -reversePrimer GCTGCGTTCTTCATCGATGC -clustering dada2 -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax -taxAligner lambda -t 20\n\n\nFor a full description of each workflow and summaries of the results, please see the relevant sections below.",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  },
  {
    "objectID": "lotus3.html#citable-resources",
    "href": "lotus3.html#citable-resources",
    "title": "ASV Inference",
    "section": "",
    "text": "LotuS3 An ultrafast and highly accurate tool for amplicon sequencing analysis (Özkurt et al. 2022).\nDADA2 ASV clustering (Callahan et al. 2016).\nVSEARCH v2.30.0 (chimera de novo / ref; OTU alignments) (Rognes et al. 2016).\nPoisson binomial model based read filtering (Puente-Sánchez, Aguirre, and Parro 2016).\nOfftarget removal (against phiX) (Bedarf et al. 2021).\nminimap2 v2.30 used in offtarget aligments (Li 2018).\nLULU multicopy rRNA removal (Frøslev et al. 2017).\nLambda3 taxonomic similarity search (Hauswedell et al. 2024).\nKSGP SSU specific tax database (for SSU workflow) (Grant et al. 2023).\nITSx v1.3 removal of non ITS OTUs (for ITS and Oomycete workflows) (Bengtsson-Palme et al. 2013)\n\nSILVA 16S/18S database (for AMF workflow) (Yilmaz et al. 2014)\n\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\nSeqKit2 for sequence and alignment processing (Shen, Sipos, and Zhao 2024)",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  },
  {
    "objectID": "lotus3.html#references",
    "href": "lotus3.html#references",
    "title": "ASV Inference",
    "section": "",
    "text": "Bedarf, Janis R, Naiara Beraza, Hassan Khazneh, Ezgi Özkurt, David Baker, Valeri Borger, Ullrich Wüllner, and Falk Hildebrand. 2021. “Much Ado about Nothing? Off-Target Amplification Can Lead to False-Positive Bacterial Brain Microbiome Detection in Healthy and Parkinson’s Disease Individuals.” Microbiome 9 (1): 75. https://doi.org/10.1186/s40168-021-01012-1.\n\n\nBengtsson-Palme, Johan, Martin Ryberg, Martin Hartmann, Sara Branco, Zheng Wang, Anna Godhe, Pierre De Wit, et al. 2013. “Improved Software Detection and Extraction of ITS1 and ITS 2 from Ribosomal ITS Sequences of Fungi and Other Eukaryotes for Analysis of Environmental Sequencing Data.” Methods in Ecology and Evolution 4 (10): 914–19. https://doi.org/10.1111/2041-210X.12073.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nFrøslev, Tobias Guldberg, Rasmus Kjøller, Hans Henrik Bruun, Rasmus Ejrnæs, Ane Kirstine Brunbjerg, Carlotta Pietroni, and Anders Johannes Hansen. 2017. “Algorithm for Post-Clustering Curation of DNA Amplicon Data Yields Reliable Biodiversity Estimates.” Nature Communications 8 (1): 1188. https://doi.org/10.1038/s41467-017-01312-x.\n\n\nGrant, Alastair, Abdullah Aleidan, Charli S Davies, Solomon C Udochi, Joachim Fritscher, Mohammad Bahram, and Falk Hildebrand. 2023. “Improved Taxonomic Annotation of Archaea Communities Using LotuS2, the Genome Taxonomy Database and RNAseq Data.” bioRxiv, 2023–08. https://doi.org/10.1101/2023.08.21.554127.\n\n\nHauswedell, Hannes, Sara Hetzel, Simon G Gottlieb, Helene Kretzmer, Alexander Meissner, and Knut Reinert. 2024. “Lambda3: Homology Search for Protein, Nucleotide, and Bisulfite-Converted Sequences.” Bioinformatics 40 (3): btae097. https://doi.org/10.1093/bioinformatics/btae097.\n\n\nLi, Heng. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.\n\n\nÖzkurt, Ezgi, Joachim Fritscher, Nicola Soranzo, Duncan YK Ng, Robert P Davey, Mohammad Bahram, and Falk Hildebrand. 2022. “LotuS2: An Ultrafast and Highly Accurate Tool for Amplicon Sequencing Analysis.” Microbiome 10 (1): 176. https://doi.org/10.1186/s40168-022-01365-1.\n\n\nPuente-Sánchez, Fernando, Jacobo Aguirre, and Vı́ctor Parro. 2016. “A Novel Conceptual Approach to Read-Filtering in High-Throughput Amplicon Sequencing Studies.” Nucleic Acids Research 44 (4): e40–40. https://doi.org/10.1093/nar/gkv1113.\n\n\nRognes, Torbjørn, Tomáš Flouri, Ben Nichols, Christopher Quince, and Frédéric Mahé. 2016. “VSEARCH: A Versatile Open Source Tool for Metagenomics.” PeerJ 4: e2584. https://doi.org/10.7717/peerj.2584.\n\n\nShen, Wei, Botond Sipos, and Liuyang Zhao. 2024. “SeqKit2: A Swiss Army Knife for Sequence and Alignment Processing.” Imeta 3 (3): e191. https://doi.org/10.1002/imt2.191.\n\n\nYilmaz, Pelin, Laura Wegener Parfrey, Pablo Yarza, Jan Gerken, Elmar Pruesse, Christian Quast, Timmy Schweer, Jörg Peplies, Wolfgang Ludwig, and Frank Oliver Glöckner. 2014. “The SILVA and ‘All-Species Living Tree Project (LTP)’ Taxonomic Frameworks.” Nucleic Acids Research 42 (D1): D643–48. https://doi.org/10.1093/nar/gkt1209.",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  },
  {
    "objectID": "lotus3.html#create-custom-taxon-db",
    "href": "lotus3.html#create-custom-taxon-db",
    "title": "ASV Inference",
    "section": "Create Custom Taxon DB",
    "text": "Create Custom Taxon DB\nA note on making a custom taxonomy database for the ITS/Oomycete analysis. For this I first downloaded the sh_general_release_dynamic_s_all_19.02.2025_dev.fasta. Then I used a combination of command line code and seqkit (Shen, Sipos, and Zhao 2024) to properly format the database for the LotuS3 pipeline.\n\nconda activate seqkit\ncp sh_general_release_dynamic_s_all_19.02.2025_dev.fasta tmp.fasta\n\n\nseqkit seq --name tmp.fasta --out-file tmp.tax\n\nWhere…\n\n\n\n-n, –name only print names/sequence headers.\n\n\n-o, –out-file string out file (“-” for stdout, suffix .gz for gzipped out) (default “-”).\n\n\n\nsed 's/|[^|]*$/\\t&/; s/\\t|/\\t/' tmp.tax &gt; \\\n                      lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax\n\n\nseqkit replace tmp.fasta  --line-width 0 \n                          --pattern \"refs_singleton\\|.*\" \n                          --replacement \"refs_singleton\" \n                          --out-file tmp1.fasta\nseqkit replace tmp1.fasta --line-width 0 \n                          --pattern \"refs\\|.*\" \n                          --replacement \"refs\" \n                          --out-file tmp2.fasta\nseqkit replace tmp2.fasta --line-width 0 \n                          --pattern \"reps_singleton\\|.*\" \n                          --replacement \"reps_singleton\" \n                          --out-file tmp3.fasta\nseqkit replace tmp3.fasta --line-width 0 \n                          --pattern \"reps\\|.*\" \n                          --replacement \"reps\" \n                          --out-file \n                  lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta \nrm tmp*\n\n\n\n\n-w, –line-width  line width when outputting FASTA format (0 for no wrap) (default 60)\n\n\n- -p, –pattern  search regular expression\n\n\n-r, –replacement  replacement. supporting capture variables\n\n\n-o, –out-file  out file (“-” for stdout, suffix .gz for gzipped out) (default “-”)\n\n\n\nAt this point we now have a formated taxonomy database for the analysis.",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  },
  {
    "objectID": "lotus3.html#individual-workflow",
    "href": "lotus3.html#individual-workflow",
    "title": "ASV Inference",
    "section": "Individual Workflow",
    "text": "Individual Workflow\n\n\nSSU\nITS\nAMF\nOomycete\n\n\n\nHere is the command used to analyze the SSU dataset.\n\nlotus3 -i . \\\n       -map ssu_miSeqMap.sm.txt \\\n       -o LOTUS3_ASV \\\n       -sdmopt sdm_miSeq.txt \\\n       -p miSeq \\\n       -amplicon_type SSU  \\\n       -forwardPrimer GTGCCAGCMGCCGCGGTAA \\\n       -reversePrimer GGACTACHVGGGTWTCTAAT \\\n       -clustering dada2 \\\n       -refDB SLV \\\n       -taxAligner lambda \\\n       -threads 20\n\n\n\n\n\n\n\n   ssu_miSeqMap.sm.txt\n\n\n\n SSU mapping file \n\n\n\nClick here to see the verbose output of LotuS3 pipeline\n00:00:01 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i . -m ssu_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\n          -p miSeq -amplicon_type SSU \n          -forwardPrimer GTGCCAGCMGCCGCGGTAA\n          -reversePrimer GGACTACHVGGGTWTCTAAT \n          -CL dada2 -refDB KSGP\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         bacteria, SSU\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//rdp_gold.fa, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       KSGP\nRefDB location          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\n00:00:01 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (ssu_1)\n00:00:01 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:02 Finished primary read processing with sdm:\n          Reads processed: 4,009,780; 4,009,311 (pair 1;pair 2)\n          Accepted (High qual): 2,542,654; 2,816,397 (80; 253,740 end-trimmed)\n          Accepted (Mid qual): 15;28\n          Rejected: 1,467,242; 1,193,478\n          Dereplication block 0: 67,634 unique sequences (avg size\n          28; 1,908,091 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:01:02 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n\n00:07:45 Found 14840 ASVs, summing to 1700760 reads (dada2)\n00:07:45 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:08:18 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:08:18 Extending and merging pairs of ASV Seeds\n00:08:24 Found 16152 fasta seed sequences based on seed extension\n          and read merging\n00:08:24 Found contaminated 0 ASV's using minimap2 \n       (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:09:55 2749 ASV's removed with LULU\n 00:09:55 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          After filtering 12091 ASV's (2300907 reads) remaining in matrix.\n00:13:31 Assigning taxonomy against \n          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\n          using LAMBDA3\n00:15:03 Calculating Taxonomic Abundance Tables from KSGP assignments\nCalculating higher abundance levels\nAdding 23 unclassified ASV's to output matrices\nTotal reads in matrix: 2300907\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  95  94\nClass   92  79\nOrder   88  77\nFamily  77  71\nGenus   40  60\nSpecies 17  32\n00:15:04 Building tree (fasttree) and aligning (mafft) OTUs\n 00:26:02 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial\n\n\n\nHere is the command used to analyze the ITS dataset.\n\nlotus3 -i . \\\n       -map its_miSeqMap.sm.txt \\\n       -o LOTUS3_ASV \\\n       -sdmopt sdm_miSeq_ITS.txt \\\n       -p miSeq \\\n       -amplicon_type ITS  \\\n       -forwardPrimer CTTGGTCATTTAGAGGAAGTAA \\\n       -reversePrimer GCTGCGTTCTTCATCGATGC \\\n       -clustering dada2 \\\n       -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta \\\n       -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax \\ \n       -taxAligner lambda \\\n       -t 20\n\n\n\n\n\n\n\n   its_miSeqMap.sm.txt\n\n\n\n ITS mapping file \n\n\n\nClick here to see the verbose output of LotuS3 pipeline\n00:00:00 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i. -m its_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq_ITS.txt\n          -p miSeq -amplicon_type ITS \n          -forwardPrimer CTTGGTCATTTAGAGGAAGTAA\n          -reversePrimer GCTGCGTTCTTCATCGATGC \n          -CL dada2 \n          -refDB lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\n          -tax4refDB lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.tax\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq_ITS.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         eukarya, ITS\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//ITS_chimera/uchime_sh_refs_dynamic_original_985_11.03.2015.fasta, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       custom\nRefDB location          lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\nITSx check              Yes\n00:00:00 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (its_1)\n00:00:00 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:00:42 Finished primary read processing with sdm:\n          Reads processed: 2,460,691; 2,460,703 (pair 1;pair 2)\n          Accepted (High qual): 691,801; 851,945 (2,429; 1,991,083 end-trimmed)\n          Accepted (Mid qual): 43;150\n          Rejected: 1,768,458; 1,608,720\n          Dereplication block 0: 10,419 unique sequences (avg size\n          50; 516,768 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:00:42 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:23 Found 2830 ASVs, summing to 477956 reads (dada2)\n00:01:23 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:01:30 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:01:30 Extending and merging pairs of ASV Seeds\n00:01:32 Found 4627 fasta seed sequences based on seed extension\n          and read merging\nNo ref based chimera detection\n00:02:04 ITSx analysis: Kept 2527 identified as ITS1,ITS2, deleted\n          303 ASV's (of 2830 ASV's).\n00:02:04 Found contaminated 0 ASV's using minimap2 \n     (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:02:10 374 ASV's removed with LULU\n00:02:10 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          Contaminants: ITSx.: 288 ASV's removed (49771 reads); \n          After filtering 2168 ASV's (608393 reads) remaining in matrix.\n00:02:10 Assigning taxonomy against \n          lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\n          using LAMBDA3\n 00:02:21 Calculating Taxonomic Abundance Tables from custom assignments\nCalculating higher abundance levels\nAdding 202 unclassified ASV's to output matrices\nTotal reads in matrix: 608393\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  77  79\nClass   75  75\nOrder   71  76\nFamily  66  73\nGenus   40  60\nSpecies 29  47\n 00:02:21 Building tree (fasttree) and aligning (mafft) OTUs\nWARNING:: Phylogenies for ITS amplicons may be inaccurate and \n          should be used with caution.\n00:11:02 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial\nThe following WARNINGS occured:\nWARNING:: Setting \"-tax_group\" to \"eukarya\" as only eukarya and \n          fungi are supported options for ITS.\nWARNING:: Phylogenies for ITS amplicons may be inaccurate and should be \n          used with caution.\n\n\n\nHere is the command used to analyze the AMF dataset.\n\nlotus3 \n       -i . \n       -map amf_miSeqMap.sm.txt \\ \n       -o LOTUS3_ASV \\ \n       -sdmopt sdm_miSeq.txt \\\n       -p miSeq \\\n       -amplicon_type SSU  \\\n       -forwardPrimer AAGCTCGTAGTTGAATTTCG \\\n       -reversePrimer CCCAACTATCCCTATTAATCAT \\ \n       -clustering dada2 \\\n       -refDB SLV \\\n       -taxAligner lambda \\\n       -t 20\n\n\n\n\n\n\n\n   amf_miSeqMap.sm.txt\n\n\n\n AMF mapping file \n\n\n\nClick here to see the verbose output of LotuS3 pipeline\n00:00:00 LotuS 3.03\n          COMMAND\n          perl /home/scottjj/miniconda3/envs/lotus3/bin/lotus3 \n          -i. -m amf_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\n          -p miSeq -amplicon_type SSU \n          -forwardPrimer AAGCTCGTAGTTGAATTTCG\n          -reversePrimer CCCAACTATCCCTATTAATCAT \n          -CL dada2 -refDB SLV\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         bacteria, SSU\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//rdp_gold.fa, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       SILVA\nRefDB location          ~miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//SLV_138.1_SSU.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\n00:00:00 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (amf_1)\n00:00:00 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:00:32 Finished primary read processing with sdm:\n          Reads processed: 1,471,847; 1,470,877 (pair 1;pair 2)\n          Accepted (High qual): 1,121,688; 1,165,993 (1,813; 124,821\n          end-trimmed)\n          Accepted (Mid qual): 301;11\n          Rejected: 350,947; 306,939\n          Dereplication block 0: 13,149 unique sequences (avg size\n          80; 1,050,872 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n\n00:00:32 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:19 Found 1159 ASVs, summing to 1002627 reads (dada2)\n00:01:19 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n\n00:01:26 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:01:26 Extending and merging pairs of ASV Seeds\n\n00:01:28 Found 1529 fasta seed sequences based on seed extension\n          and read merging\n\nNo ref based chimera detection\n00:01:28 Found contaminated 0 ASV's using minimap2 \n        (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:01:36 276 ASV's removed with LULU\n00:01:36 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          After filtering 883 ASV's (1167700 reads) remaining in matrix.\n\nBuilding LAMBDA index anew for \n~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//SLV_138.1_SSU.fasta \n(this only happens the first time you use this ref DB, \nit may take several hours to build)..\n00:03:28 Assigning taxonomy against \n    ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//SLV_138.1_SSU.fasta\n    using LAMBDA3\n00:03:51 Calculating Taxonomic Abundance Tables from SILVA assignments\nCalculating higher abundance levels\nAdding 15 unclassified ASV's to output matrices\nTotal reads in matrix: 1167700\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  75  95\nClass   78  72\nOrder   52  63\nFamily  44  47\nGenus   24  33\nSpecies 17  38\n00:03:51 Building tree (fasttree) and aligning (mafft) OTUs\n00:04:54 LotuS3 finished. Output in:\n          LOTUS3_ASV\n          Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial\n00:00:01 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i . -m ssu_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\n          -p miSeq -amplicon_type SSU \n          -forwardPrimer GTGCCAGCMGCCGCGGTAA\n          -reversePrimer GGACTACHVGGGTWTCTAAT \n          -CL dada2 -refDB KSGP\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         bacteria, SSU\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//rdp_gold.fa, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       KSGP\nRefDB location          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\n00:00:01 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (ssu_1)\n00:00:01 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:02 Finished primary read processing with sdm:\n          Reads processed: 4,009,780; 4,009,311 (pair 1;pair 2)\n          Accepted (High qual): 2,542,654; 2,816,397 (80; 253,740 end-trimmed)\n          Accepted (Mid qual): 15;28\n          Rejected: 1,467,242; 1,193,478\n          Dereplication block 0: 67,634 unique sequences (avg size\n          28; 1,908,091 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:01:02 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n\n00:07:45 Found 14840 ASVs, summing to 1700760 reads (dada2)\n00:07:45 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:08:18 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:08:18 Extending and merging pairs of ASV Seeds\n00:08:24 Found 16152 fasta seed sequences based on seed extension\n          and read merging\n00:08:24 Found contaminated 0 ASV's using minimap2 \n       (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:09:55 2749 ASV's removed with LULU\n 00:09:55 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          After filtering 12091 ASV's (2300907 reads) remaining in matrix.\n00:13:31 Assigning taxonomy against \n          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\n          using LAMBDA3\n00:15:03 Calculating Taxonomic Abundance Tables from KSGP assignments\nCalculating higher abundance levels\nAdding 23 unclassified ASV's to output matrices\nTotal reads in matrix: 2300907\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  95  94\nClass   92  79\nOrder   88  77\nFamily  77  71\nGenus   40  60\nSpecies 17  32\n00:15:04 Building tree (fasttree) and aligning (mafft) OTUs\n 00:26:02 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial\n\n\n\nHere is the command used to analyze the Oomycete dataset.\n\nlotus3 \n       -i . \n       -map oo_miSeqMap.sm.txt \\\n       -o LOTUS3_ASV \\\n       -sdmopt sdm_miSeq_ITS.txt \\\n       -p miSeq \\\n       -amplicon_type ITS  \\\n       -forwardPrimer GGAAGGATCATTACCACA \\\n       -reversePrimer GCTGCGTTCTTCATCGATGC \\ \n       -clustering dada2 \\\n       -refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta \\\n       -tax4refDB lotus3_sh_general_release_dynamic_s_all_19.02.2025_dev.tax \\ \n       -taxAligner lambda \\\n       -t 20\n\n\n\n\n\n\n\n   oo_miSeqMap.sm.txt\n\n\n\n OO mapping file \n\n\n\nClick here to see the verbose output of LotuS3 pipeline\n00:00:00 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i. -m oo_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq_ITS.txt\n          -p miSeq -amplicon_type ITS \n          -forwardPrimer GGAAGGATCATTACCACA\n          -reversePrimer GCTGCGTTCTTCATCGATGC \n          -CL dada2 \n          -refDB lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\n          -tax4refDB lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.tax\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq_ITS.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         eukarya, ITS\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//ITS_chimera/uchime_sh_refs_dynamic_original_985_11.03.2015.fasta, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       custom\nRefDB location          lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\nITSx check              Yes\n00:00:00 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (oo_1)\n00:00:00 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:00:15 Finished primary read processing with sdm:\n          Reads processed: 726,145; 725,721 (pair 1;pair 2)\n          Accepted (High qual): 393,541; 601,307 (2,143; 470,145 end-trimmed)\n          Accepted (Mid qual): 20;184\n          Rejected: 333,118; 125,140\n          Dereplication block 0: 4,931 unique sequences (avg size\n          66; 325,486 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:00:15 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:00:49 Found 599 ASVs, summing to 313556 reads (dada2)\n00:00:49 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:00:53 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:00:53 Extending and merging pairs of ASV Seeds\n--------------------------------------------------------------------------------\n00:00:54 Found 1006 fasta seed sequences based on seed extension\n          and read merging\nNo ref based chimera detection\n00:01:01 ITSx analysis: Kept 260 identified as ITS1,ITS2, deleted\n          339 ASV's (of 599 ASV's).\n00:01:01 Found contaminated 0 ASV's using minimap2 \n   (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:01:03 118 ASV's removed with LULU\n00:01:03 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          Contaminants: ITSx.: 289 ASV's removed (57702 reads); \n          After filtering 192 ASV's (336493 reads) remaining in matrix.\n00:01:03 Assigning taxonomy against \n          lotus2_sh_general_release_dynamic_s_all_19.02.2025_dev.fasta\n          using LAMBDA3\n00:01:11 Calculating Taxonomic Abundance Tables from custom assignments\nCalculating higher abundance levels\nAdding 39 unclassified ASV's to output matrices\nTotal reads in matrix: 336493\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  88  71\nClass   83  65\nOrder   82  70\nFamily  78  62\nGenus   66  49\nSpecies 12  33\n00:01:11 Building tree (fasttree) and aligning (mafft) OTUs\nWARNING:: Phylogenies for ITS amplicons may be inaccurate and should be used with caution.\n \n00:01:25 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial\nThe following WARNINGS occured:\nWARNING:: Setting \"-tax_group\" to \"eukarya\" as only eukarya and fungi are supported options for ITS.\nWARNING:: Phylogenies for ITS amplicons may be inaccurate and should be used with caution.\n00:00:01 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i . -m ssu_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\n          -p miSeq -amplicon_type SSU \n          -forwardPrimer GTGCCAGCMGCCGCGGTAA\n          -reversePrimer GGACTACHVGGGTWTCTAAT \n          -CL dada2 -refDB KSGP\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         bacteria, SSU\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//rdp_gold.fa, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       KSGP\nRefDB location          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\n00:00:01 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (ssu_1)\n00:00:01 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:02 Finished primary read processing with sdm:\n          Reads processed: 4,009,780; 4,009,311 (pair 1;pair 2)\n          Accepted (High qual): 2,542,654; 2,816,397 (80; 253,740 end-trimmed)\n          Accepted (Mid qual): 15;28\n          Rejected: 1,467,242; 1,193,478\n          Dereplication block 0: 67,634 unique sequences (avg size\n          28; 1,908,091 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:01:02 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n\n00:07:45 Found 14840 ASVs, summing to 1700760 reads (dada2)\n00:07:45 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:08:18 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:08:18 Extending and merging pairs of ASV Seeds\n00:08:24 Found 16152 fasta seed sequences based on seed extension\n          and read merging\n00:08:24 Found contaminated 0 ASV's using minimap2 \n       (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:09:55 2749 ASV's removed with LULU\n 00:09:55 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          After filtering 12091 ASV's (2300907 reads) remaining in matrix.\n00:13:31 Assigning taxonomy against \n          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\n          using LAMBDA3\n00:15:03 Calculating Taxonomic Abundance Tables from KSGP assignments\nCalculating higher abundance levels\nAdding 23 unclassified ASV's to output matrices\nTotal reads in matrix: 2300907\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  95  94\nClass   92  79\nOrder   88  77\nFamily  77  71\nGenus   40  60\nSpecies 17  32\n00:15:04 Building tree (fasttree) and aligning (mafft) OTUs\n 00:26:02 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial",
    "crumbs": [
      "A. Processing",
      "ASV Inference"
    ]
  }
]