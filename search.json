[
  {
    "objectID": "primers.html",
    "href": "primers.html",
    "title": "Primer Removal",
    "section": "",
    "text": "This workflow uses cutadapt to remove primer sequences from the four amplicon datasets. Included are links to R scripts, associated processing files, and access to raw fastq files.\nHere is the basic rundown for renaming samples and removing primer pairs. All of the data and scripts you need can be downloaded below.\n\nDownload raw fastq files from figshare using the links in the table below.\nRename fastq files using the rename.sh script and the corresponding lookup table.\nRun the corresponding cutadapt R script to remove primers.\n\nAt this point your samples are ready for ASV calling. For a more detailed explaination of the workflow and access to summary data please see the Workflow section below.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity and consistency we use the abbreviation SSU or ssu to refer to the 16S rRNA dataset. We recognize that the small subunit (SSU) applies to multiple domains but here we specifically refer to Bacteria and Archaea. Further we use the abbreviations ITS or its for general fungi, AMF or amf for Arbuscular Mycorrhizal Fungi, and OO or oo for Oomycetes.\n\n\n\nHere is everything you need to run this workflow–raw data files, fastq renaming scripts and lookup tables, and cutadapt R scripts.\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\nSSU\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nYou can also download the scripts and lookup tables for each dataset from figshare.\n\n\nPrimer pairs used for each dataset along with primer names and associated references.\n\n\n\n\n\n\nDataset\nForward primer\nReverse Primer\n\n\n\nSSU\nGTGCCAGCMGCCGCGGTAA  515f (Caporaso et al. 2011)\n\nGGACTACHVGGGTWTCTAAT  806r (Caporaso et al. 2011)\n\n\n\nITS\nCTTGGTCATTTAGAGGAAGTAA  ITS1f (Gardes and Bruns 1993)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\nAMF\nAAGCTCGTAGTTGAATTTCG  AMV4-5NF (Sato et al. 2005)\n\nCCCAACTATCCCTATTAATCAT  AMDGR (Sato et al. 2005)\n\n\n\nOomycete\nGGAAGGATCATTACCACA  ITS1oo (Riit et al. 2016)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\n\n\n\n\ncutadapt (Martin 2011)\n\n\n\n\n\n\nCaporaso, J Gregory, Christian L Lauber, William A Walters, Donna Berg-Lyons, Catherine A Lozupone, Peter J Turnbaugh, Noah Fierer, and Rob Knight. 2011. “Global Patterns of 16S rRNA Diversity at a Depth of Millions of Sequences Per Sample.” Proceedings of the National Academy of Sciences 108: 4516–22. https://doi.org/10.1073/pnas.1000080107.\n\n\nGardes, Monique, and Thomas D Bruns. 1993. “ITS Primers with Enhanced Specificity for Basidiomycetes-Application to the Identification of Mycorrhizae and Rusts.” Molecular Ecology 2 (2): 113–18. https://doi.org/10.1111/j.1365-294X.1993.tb00005.x.\n\n\nMartin, Marcel. 2011. “Cutadapt Removes Adapter Sequences from High-Throughput Sequencing Reads.” EMBnet. Journal 17 (1): 10–12. https://doi.org/10.14806/ej.17.1.200.\n\n\nRiit, Taavi, Leho Tedersoo, Rein Drenkhan, Eve Runno-Paurson, Harri Kokko, and Sten Anslan. 2016. “Oomycete-Specific ITS Primers for Identification and Metabarcoding.” MycoKeys 14: 17–30. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nSato, Kouichi, Yoshihisa Suyama, Masanori Saito, and Kazuo Sugawara. 2005. “A New Primer for Discrimination of Arbuscular Mycorrhizal Fungi with Polymerase Chain Reaction-Denature Gradient Gel Electrophoresis.” Grassland Science 51 (2): 179–81. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nWhite, Thomas J, Thomas Bruns, SJWT Lee, John Taylor, et al. 1990. “Amplification and Direct Sequencing of Fungal Ribosomal RNA Genes for Phylogenetics.” PCR Protocols: A Guide to Methods and Applications 18 (1): 315–22.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#raw-data-scripts",
    "href": "primers.html#raw-data-scripts",
    "title": "Primer Removal",
    "section": "",
    "text": "Here is everything you need to run this workflow–raw data files, fastq renaming scripts and lookup tables, and cutadapt R scripts.\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\nSSU\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nYou can also download the scripts and lookup tables for each dataset from figshare.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#citable-resources",
    "href": "primers.html#citable-resources",
    "title": "Primer Removal",
    "section": "",
    "text": "Primer pairs used for each dataset along with primer names and associated references.\n\n\n\n\n\n\nDataset\nForward primer\nReverse Primer\n\n\n\nSSU\nGTGCCAGCMGCCGCGGTAA  515f (Caporaso et al. 2011)\n\nGGACTACHVGGGTWTCTAAT  806r (Caporaso et al. 2011)\n\n\n\nITS\nCTTGGTCATTTAGAGGAAGTAA  ITS1f (Gardes and Bruns 1993)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\nAMF\nAAGCTCGTAGTTGAATTTCG  AMV4-5NF (Sato et al. 2005)\n\nCCCAACTATCCCTATTAATCAT  AMDGR (Sato et al. 2005)\n\n\n\nOomycete\nGGAAGGATCATTACCACA  ITS1oo (Riit et al. 2016)\n\nGCTGCGTTCTTCATCGATGC  ITS2 (White et al. 1990)\n\n\n\n\n\n\n\ncutadapt (Martin 2011)",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#references",
    "href": "primers.html#references",
    "title": "Primer Removal",
    "section": "",
    "text": "Caporaso, J Gregory, Christian L Lauber, William A Walters, Donna Berg-Lyons, Catherine A Lozupone, Peter J Turnbaugh, Noah Fierer, and Rob Knight. 2011. “Global Patterns of 16S rRNA Diversity at a Depth of Millions of Sequences Per Sample.” Proceedings of the National Academy of Sciences 108: 4516–22. https://doi.org/10.1073/pnas.1000080107.\n\n\nGardes, Monique, and Thomas D Bruns. 1993. “ITS Primers with Enhanced Specificity for Basidiomycetes-Application to the Identification of Mycorrhizae and Rusts.” Molecular Ecology 2 (2): 113–18. https://doi.org/10.1111/j.1365-294X.1993.tb00005.x.\n\n\nMartin, Marcel. 2011. “Cutadapt Removes Adapter Sequences from High-Throughput Sequencing Reads.” EMBnet. Journal 17 (1): 10–12. https://doi.org/10.14806/ej.17.1.200.\n\n\nRiit, Taavi, Leho Tedersoo, Rein Drenkhan, Eve Runno-Paurson, Harri Kokko, and Sten Anslan. 2016. “Oomycete-Specific ITS Primers for Identification and Metabarcoding.” MycoKeys 14: 17–30. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nSato, Kouichi, Yoshihisa Suyama, Masanori Saito, and Kazuo Sugawara. 2005. “A New Primer for Discrimination of Arbuscular Mycorrhizal Fungi with Polymerase Chain Reaction-Denature Gradient Gel Electrophoresis.” Grassland Science 51 (2): 179–81. https://doi.org/10.1111/j.1744-697X.2005.00023.x.\n\n\nWhite, Thomas J, Thomas Bruns, SJWT Lee, John Taylor, et al. 1990. “Amplification and Direct Sequencing of Fungal Ribosomal RNA Genes for Phylogenetics.” PCR Protocols: A Guide to Methods and Applications 18 (1): 315–22.",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#required-packages",
    "href": "primers.html#required-packages",
    "title": "Primer Removal",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick to see required packagesset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\nlibrary(magrittr)\nlibrary(tidyverse) # contains the following packages: dplyr, readr, forcats, stringr, ggplot2, tibble, lubridate, tidyr, purrr",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#rename-fastq-files",
    "href": "primers.html#rename-fastq-files",
    "title": "Primer Removal",
    "section": "Rename fastq Files",
    "text": "Rename fastq Files\nThe first thing we did in this workflow is rename all fastq files so that the names are more informative. Here we generate sample names that contain the sample plot information, the soil depth, warming treatment, and sample pairing. To accomplish this we use a two column tab-delimited file (e.g., ssu_rename.txt) where the first column contains the original name (e.g., SWELTR-9_R1.fastq.gz) and the second column contains the new name (e.g., P03-D00-010-W4B_R1.fastq.gz). You can download this table above.\nThis handy bash script will rename all fastq files.\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\nClick here to see the bash code for renaming fastq files.\n\n#!/usr/bin/env bash\n\n# Check arguments\nif [[ $# -lt 2 ]]; then\n    echo \"Usage: $0 /path/to/files/ rename_file.txt\"\n    exit 1\nfi\n\ndata_dir=\"$1\"\nrename_file=\"$2\"\n\n# Validate directory\nif [[ ! -d \"$data_dir\" ]]; then\n    echo \"Error: Directory '$data_dir' not found!\"\n    exit 1\nfi\n\n# Validate rename file\nif [[ ! -f \"$rename_file\" ]]; then\n    echo \"Error: File '$rename_file' not found!\"\n    exit 1\nfi\n\n# Get root name (strip directory and extension)\nbase_name=$(basename \"$rename_file\" .txt)\noutput_file=\"${base_name}_results.txt\"\n\n# Perform renaming inside given directory\nwhile IFS=$'\\t' read -r orig new; do \n    rename -v \"$orig\" \"$new\" \"$data_dir\"/*.fastq.gz\ndone &lt; \"$rename_file\" | tee \"$output_file\"\n\necho \"Results written to: $output_file\"\n\nSimply run as such with the script by passing it a directory containing fastq files and a lookup table.\n\nbash rename.sh /path/to/fastq_files /path/to/rename_table\n\n`SWELTR-1_R1.fastq.gz' -&gt; `P01-D00-010-W4A_R1.fastq.gz'\n`SWELTR-2_R1.fastq.gz' -&gt; `P01-D10-020-W4A_R1.fastq.gz'\n`SWELTR-3_R1.fastq.gz' -&gt; `P01-D20-050-W4A_R1.fastq.gz'\n`SWELTR-4_R1.fastq.gz' -&gt; `P01-D50-100-W4A_R1.fastq.gz'\n`SWELTR-5_R1.fastq.gz' -&gt; `P02-D00-010-C0A_R1.fastq.gz'\n`SWELTR-6_R1.fastq.gz' -&gt; `P02-D10-020-C0A_R1.fastq.gz'",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "primers.html#cutadapt-workflows",
    "href": "primers.html#cutadapt-workflows",
    "title": "Primer Removal",
    "section": "Cutadapt Workflows",
    "text": "Cutadapt Workflows\nIn each tab below you can find the complete cutadapt workflow for each dataset. The worflows are nearly identical except for the the cutadapt parameters --minimum-length and --maximum-length. The differences are described here:\n\nDifferences in cutadapt parameters for --minimum-length and --maximum-length.\n\n\n\n\n\n\nDataset\nminimum-length\nmaximum-length\n\n\n\nSSU\n200\n300\n\n\nITS\n20\n500\n\n\nAMF\n100\n300\n\n\nOomycete\n20\n500\n\n\n\n\n\n\nSSU\nITS\nAMF\nOomycete\n\n\n\n\n\n\n\n\n\n   1_cut_ssu.R\n\n\n\n R Script for cutadapt primer removal \n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/SSU/2019\"\nhead(list.files(path))\n\n[1] \"P00-D00-000-NNN_R1.fastq.gz\" \"P00-D00-000-NNN_R2.fastq.gz\"\n[3] \"P01-D00-010-W4A_R1.fastq.gz\" \"P01-D00-010-W4A_R2.fastq.gz\"\n[5] \"P01-D10-020-W4A_R1.fastq.gz\" \"P01-D10-020-W4A_R2.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/ssu_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 1: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"GTGCCAGCMGCCGCGGTAA\"\nREV &lt;- \"GGACTACHVGGGTWTCTAAT\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n              Forward            Complement               Reverse \n\"GTGCCAGCMGCCGCGGTAA\" \"CACGGTCGKCGGCGCCATT\" \"AATGGCGCCGMCGACCGTG\" \n              RevComp \n\"TTACCGCGGCKGCTGGCAC\" \n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GGACTACHVGGGTWTCTAAT\" \"CCTGATGDBCCCAWAGATTA\" \"TAATCTWTGGGVHCATCAGG\" \n               RevComp \n\"ATTAGAWACCCBDGTAGTCC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                              fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                              fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads  240334          0       0       0\nFWD.ReverseReads       1          0       0     110\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n     REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       1          0       0     113\nREV.ReverseReads  228930          0       0       0\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 200,\n                                            \"--maximum-length\", 300,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g GTGCCAGCMGCCGCGGTAA -a ATTAGAWACCCBDGTAGTCC \\\n                         -G GGACTACHVGGGTWTCTAAT -A TTACCGCGGCKGCTGGCAC \\\n                         --times 2 --minimum-length 200 --maximum-length 300 \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed \\\n                         --output Negative_R1.fastq.gz \\\n                         --paired-output Negative_R2.fastq.gz \\\n                         --report full --cores 10 \n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:41], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:41], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/ssu_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 2: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\nRaw read count vs. reads remaining after cutadapt.\n\n\n\n\n\n Download table \n\n\n\n\n\n\n\n\n\n   1_cut_its.R\n\n\n\n R Script for cutadapt primer removal \n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/ITS/2019\"\nhead(list.files(path))\n\n[1] \"P02-D10-020-C0A_R1.fastq.gz\" \"P01-D00-010-W4A_R1.fastq.gz\"\n[3] \"P01-D00-010-W4A_R2.fastq.gz\" \"P01-D10-020-W4A_R1.fastq.gz\"\n[5] \"P01-D10-020-W4A_R2.fastq.gz\" \"P01-D20-050-W4A_R1.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/its_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 3: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"CTTGGTCATTTAGAGGAAGTAA\"\nREV &lt;- \"GCTGCGTTCTTCATCGATGC\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n                 Forward               Complement                  Reverse \n\"CTTGGTCATTTAGAGGAAGTAA\" \"GAACCAGTAAATCTCCTTCATT\" \"AATGAAGGAGATTTACTGGTTC\" \n                 RevComp \n\"TTACTTCCTCTAAATGACCAAG\"\n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GCTGCGTTCTTCATCGATGC\" \"CGACGCAAGAAGTAGCTACG\" \"CGTAGCTACTTCTTGCGTCG\" \n               RevComp \n\"GCATCGATGAAGAACGCAGC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 1\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads  113013          0       0       1\nFWD.ReverseReads       1          0       0   84739\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits,\n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       1          0       0   88094\nREV.ReverseReads  113823          0       0       1\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 20,\n                                            \"--maximum-length\", 500,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g CTTGGTCATTTAGAGGAAGTAA -a GCATCGATGAAGAACGCAGC  \\\n                         -G GCTGCGTTCTTCATCGATGC -A TTACTTCCTCTAAATGACCAAG  \\\n                         --times 2 --minimum-length 20 --maximum-length 500  \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed  \\\n                         --output P01-D00-010-W4A_R1.fastq.gz  \\\n                         --paired-output P01-D00-010-W4A_R2.fastq.gz  \\\n                         --report full --cores 10  \\\n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:41], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:41], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/its_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 4: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\nRaw read count vs. reads remaining after cutadapt.\n\n\n\n\n\n Download table \n\n\n\n\n\n\n\n\n\n   1_cut_amf.R\n\n\n\n R Script for cutadapt primer removal \n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/AMF/2019\"\nhead(list.files(path))\n\n[1] \"P00-D00-000-NNN_R1.fastq.gz\" \"P00-D00-000-NNN_R2.fastq.gz\"\n[3] \"P01-D00-010-W4A_R1.fastq.gz\" \"P01-D00-010-W4A_R2.fastq.gz\"\n[5] \"P01-D10-020-W4A_R1.fastq.gz\" \"P01-D10-020-W4A_R2.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/amf_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 5: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"AAGCTCGTAGTTGAATTTCG\"\nREV &lt;- \"CCCAACTATCCCTATTAATCAT\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n               Forward             Complement                Reverse \n\"AAGCTCGTAGTTGAATTTCG\" \"TTCGAGCATCAACTTAAAGC\" \"GCTTTAAGTTGATGCTCGAA\" \n               RevComp \n\"CGAAATTCAACTACGAGCTT\" \n\nREV.orients\n\n                 Forward               Complement                  Reverse \n\"CCCAACTATCCCTATTAATCAT\" \"GGGTTGATAGGGATAATTAGTA\" \"TACTAATTATCCCTATCAACCC\" \n                 RevComp \n\"ATGATTAATAGGGATAGTTGGG\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   47737          0       0       0\nFWD.ReverseReads       0          0       0   33903\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0   33914\nREV.ReverseReads   45205          0       0       0\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 100,\n                                            \"--maximum-length\", 300,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g AAGCTCGTAGTTGAATTTCG -a ATGATTAATAGGGATAGTTGGG  \\\n                         -G CCCAACTATCCCTATTAATCAT -A CGAAATTCAACTACGAGCTT  \\\n                         --times 2 --minimum-length 100 --maximum-length 300  \\\n                         --error-rate 0.1 --no-indels --discard-untrimmed  \\\n                         --output P00-D00-000-NNN_R1.fastq.gz  \\\n                         --paired-output P00-D00-000-NNN_R2.fastq.gz  \\\n                         --report full --cores 10  \\\n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:40], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:40], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/amf_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 6: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       1          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\", \n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\nRaw read count vs. reads remaining after cutadapt.\n\n\n\n\n\n Download table \n\n\n\n\n\n\n\n\n\n   1_cut_oo.R\n\n\n\n R Script for cutadapt primer removal \n\n\nFirst set the path to the directory containing raw sequence data.\n\npath &lt;- \"/pool/genomics/stri_istmobiome/data/SWELTR/RAW_DATA/OO/2019\"\nhead(list.files(path))\n\n[1] \"P01-D20-050-W4A_R1.fastq.gz\" \"P00-D00-000-NNN_R1.fastq.gz\"\n[3] \"P00-D00-000-NNN_R2.fastq.gz\" \"P01-D00-010-W4A_R1.fastq.gz\"\n[5] \"P01-D00-010-W4A_R2.fastq.gz\" \"P01-D10-020-W4A_R1.fastq.gz\"\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nQuality Scores (raw data)\nLet’s quickly check the quality scores of the forward and reverse reads.\n\np1a &lt;- plotQualityProfile(fnFs[1:41], aggregate = TRUE)\np2a &lt;- plotQualityProfile(fnRs[1:41], aggregate = TRUE)\n\np3a &lt;- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"CUTADAPT/figures/oo_plot_qscores_raw.png\", p3a, width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 7: Aggregated quality score plots for forward (left) & reverse (right) RAW reads.\n\n\n\n\nDefine Primers\nBefore we start the DADA2 workflow we need to run cutadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD &lt;- \"GGAAGGATCATTACCACA\"\nREV &lt;- \"GCTGCGTTCTTCATCGATGC\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients &lt;- function(primer) {\n    require(Biostrings)\n    dna &lt;- DNAString(primer) \n    orients &lt;- c(Forward = dna, \n                 Complement = complement(dna), \n                 Reverse = reverse(dna), \n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients &lt;- allOrients(FWD)\nREV.orients &lt;- allOrients(REV)\n\n\nFWD.orients\n\n             Forward           Complement              Reverse \n\"GGAAGGATCATTACCACA\" \"CCTTCCTAGTAATGGTGT\" \"ACACCATTACTAGGAAGG\" \n             RevComp \n\"TGTGGTAATGATCCTTCC\" \n\nREV.orients\n\n               Forward             Complement                Reverse \n\"GCTGCGTTCTTCATCGATGC\" \"CGACGCAAGAAGTAGCTACG\" \"CGTAGCTACTTCTTGCGTCG\" \n               RevComp \n\"GCATCGATGAAGAACGCAGC\" \nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN &lt;- file.path(path, basename(fnFs)) \nfnRs.filtN &lt;- file.path(path, basename(fnRs))\n\nTime to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum &lt;- 2\nprimerHits &lt;- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits &gt; 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   71659          0       0       0\nFWD.ReverseReads       0          0       0   65763\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits, \n                                fn = fnFs.filtN[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, \n                                fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0   65982\nREV.ReverseReads   69664          0       0       0\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers.\nRemove Primers\nNow we can run cutadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt &lt;- \"/home/scottjj/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut &lt;- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut &lt;- file.path(path.cut, basename(fnFs.filtN))\nfnRs.cut &lt;- file.path(path.cut, basename(fnRs.filtN))\n\nFWD.RC &lt;- dada2:::rc(FWD)\nREV.RC &lt;- dada2:::rc(REV)\n\nR1.flags &lt;- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags &lt;- paste(\"-G\", REV, \"-A\", FWD.RC) \n\n\n\nfor(i in seq_along(fnFs.filtN)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags, \n                                            \"--times\", 2, \n                                            \"--minimum-length\", 20,\n                                            \"--maximum-length\", 500,\n                                            \"--error-rate\", 0.10, \n                                            \"--no-indels\",\n                                            \"--discard-untrimmed\",\n                                            \"--output\", fnFs.cut[i], \n                                            \"--paired-output\", fnRs.cut[i],\n                                            \"--report full\",\n                                            \"--cores\", 10,\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g GGAAGGATCATTACCACA -a GCATCGATGAAGAACGCAGC \n                         -G GCTGCGTTCTTCATCGATGC -A TGTGGTAATGATCCTTCC \n                         --times 2 --minimum-length 20 --maximum-length 500 \n                         --error-rate 0.1 --no-indels --discard-untrimmed \n                         --output P00-D00-000-NNN_R1.fastq.gz \n                         --paired-output P00-D00-000-NNN_R2.fastq.gz \n                         --report full --cores 10 \n\np1_cut &lt;- plotQualityProfile(fnFs.cut[1:40], aggregate = TRUE)\np2_cut &lt;- plotQualityProfile(fnRs.cut[1:40], aggregate = TRUE)\n\np3_cut &lt;- grid.arrange(p1_cut, p2_cut, nrow = 1)\nggsave(\"CUTADAPT/figures/oo_plot_qscores_cut.png\", p3_cut, \n       width = 7, height = 3)\n\n\n\n\n\n\n\n\nFigure 8: Aggregated quality score plots for forward (left) & reverse (right) reads with primers removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\n\n\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]), \n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]), \n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       1          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nFinally, we can use the following code to generate a summary table of read count changes before and after primer removal.\n\ngetN_in &lt;- function(x) sum(getUniques(x))\ntrack_in &lt;- cbind( \n               sapply(fnFs, getN_in), \n               sapply(fnRs, getN_in), \n               sapply(fnFs.filtN, getN_in), \n               sapply(fnRs.filtN, getN_in), \n               sapply(fnFs.cut, getN_in),\n               sapply(fnRs.cut, getN_in))\ncolnames(track_in) &lt;- c(\"in_F\", \"in_R\", \n                        \"pre_filt_F\", \"pre_filt_R\",\n                        \"cut_F\", \"cut_R\")\n\ntrack_in &lt;- data.frame(track_in)\ntrack_in &lt;- tibble::rownames_to_column(track_in, \"SampleID\")\n\ntrack_in &lt;- track_in %&gt;% \n  mutate(SampleID = str_replace_all(SampleID, \"^.*/\", \"\")) %&gt;%\n  mutate(SampleID = str_replace_all(SampleID, \"_R.*\", \"\"))\n\nreadr::write_delim(track_in, \"CUTADAPT/cutadapt_track.txt\", delim = \"\\t\")\n\n\n\n\nRaw read count vs. reads remaining after cutadapt.\n\n\n\n\n\n Download table",
    "crumbs": [
      "A. Processing",
      "Primer Removal"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The SWELTR Depth Study",
    "section": "",
    "text": "1 + 1\n\n\n\nLinks\n\nmetacrobe\nQuarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nLinks\n\nmetacrobe\nQuarto",
    "crumbs": [
      "More",
      "About"
    ]
  },
  {
    "objectID": "lotus3_ssu.html",
    "href": "lotus3_ssu.html",
    "title": "SWELTR Depth 16S rRNA",
    "section": "",
    "text": "This workflow uses LotuS3 to remove primer sequences from the four amplicon datasets. Included are links to R scripts and associated processing files. You can also find workflow outputs–specifically microeco objects as well as standalone ASV tables, taxonomy tables, and sample data.\n\nHere is everything you need to run this workflow–\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\nSSU\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples \n\n\n\n\n\n\nLotuS3 An ultrafast and highly accurate tool for amplicon sequencing analysis (Özkurt et al. 2022).\nDADA2 ASV clustering (Callahan et al. 2016).\nVSEARCH v2.30.0 (chimera de novo / ref; OTU alignments) (Rognes et al. 2016).\nPoisson binomial model based read filtering (Puente-Sánchez, Aguirre, and Parro 2016).\nOfftarget removal (against phiX) (Bedarf et al. 2021).\nminimap2 v2.30 used in offtarget aligments (Li 2018).\nLULU multicopy rRNA removal (Frøslev et al. 2017).\nLambda3 taxonomic similarity search (Hauswedell et al. 2024).\nKSGP SSU specific tax database (Grant et al. 2023).\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).\n\n\n\n\n\nBedarf, Janis R, Naiara Beraza, Hassan Khazneh, Ezgi Özkurt, David Baker, Valeri Borger, Ullrich Wüllner, and Falk Hildebrand. 2021. “Much Ado about Nothing? Off-Target Amplification Can Lead to False-Positive Bacterial Brain Microbiome Detection in Healthy and Parkinson’s Disease Individuals.” Microbiome 9 (1): 75. https://doi.org/10.1186/s40168-021-01012-1.\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nFrøslev, Tobias Guldberg, Rasmus Kjøller, Hans Henrik Bruun, Rasmus Ejrnæs, Ane Kirstine Brunbjerg, Carlotta Pietroni, and Anders Johannes Hansen. 2017. “Algorithm for Post-Clustering Curation of DNA Amplicon Data Yields Reliable Biodiversity Estimates.” Nature Communications 8 (1): 1188. https://doi.org/10.1038/s41467-017-01312-x.\n\n\nGrant, Alastair, Abdullah Aleidan, Charli S Davies, Solomon C Udochi, Joachim Fritscher, Mohammad Bahram, and Falk Hildebrand. 2023. “Improved Taxonomic Annotation of Archaea Communities Using LotuS2, the Genome Taxonomy Database and RNAseq Data.” bioRxiv, 2023–08. https://doi.org/10.1101/2023.08.21.554127.\n\n\nHauswedell, Hannes, Sara Hetzel, Simon G Gottlieb, Helene Kretzmer, Alexander Meissner, and Knut Reinert. 2024. “Lambda3: Homology Search for Protein, Nucleotide, and Bisulfite-Converted Sequences.” Bioinformatics 40 (3): btae097. https://doi.org/10.1093/bioinformatics/btae097.\n\n\nLi, Heng. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.\n\n\nÖzkurt, Ezgi, Joachim Fritscher, Nicola Soranzo, Duncan YK Ng, Robert P Davey, Mohammad Bahram, and Falk Hildebrand. 2022. “LotuS2: An Ultrafast and Highly Accurate Tool for Amplicon Sequencing Analysis.” Microbiome 10 (1): 176. https://doi.org/10.1186/s40168-022-01365-1.\n\n\nPuente-Sánchez, Fernando, Jacobo Aguirre, and Vı́ctor Parro. 2016. “A Novel Conceptual Approach to Read-Filtering in High-Throughput Amplicon Sequencing Studies.” Nucleic Acids Research 44 (4): e40–40. https://doi.org/10.1093/nar/gkv1113.\n\n\nRognes, Torbjørn, Tomáš Flouri, Ben Nichols, Christopher Quince, and Frédéric Mahé. 2016. “VSEARCH: A Versatile Open Source Tool for Metagenomics.” PeerJ 4: e2584. https://doi.org/10.7717/peerj.2584.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#data-scripts",
    "href": "lotus3_ssu.html#data-scripts",
    "title": "SWELTR Depth 16S rRNA",
    "section": "",
    "text": "Here is everything you need to run this workflow–\n\n\nQuick links to raw data and scripts.\n\n\n\n\n\n\n\nDataset\nRaw fastq files\nRename table\nCutadapt R script\n\n\n\nSSU\n\nSSU fastq files\n\n SSU lookup \n SSU cutadapt script \n\n\nITS\n\nITS fastq files\n\n ITS lookup \n ITS cutadapt script \n\n\nAMF\n\nAMF fastq files\n\n AMF lookup \n AMF cutadapt script \n\n\nOomycete\n\nOO fastq files\n\n OO lookup \n OO cutadapt script \n\n\n\n\n\n\n\n\n\n\n   rename.sh\n\n\n\n Bash script to rename samples",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#citable-resources",
    "href": "lotus3_ssu.html#citable-resources",
    "title": "SWELTR Depth 16S rRNA",
    "section": "",
    "text": "LotuS3 An ultrafast and highly accurate tool for amplicon sequencing analysis (Özkurt et al. 2022).\nDADA2 ASV clustering (Callahan et al. 2016).\nVSEARCH v2.30.0 (chimera de novo / ref; OTU alignments) (Rognes et al. 2016).\nPoisson binomial model based read filtering (Puente-Sánchez, Aguirre, and Parro 2016).\nOfftarget removal (against phiX) (Bedarf et al. 2021).\nminimap2 v2.30 used in offtarget aligments (Li 2018).\nLULU multicopy rRNA removal (Frøslev et al. 2017).\nLambda3 taxonomic similarity search (Hauswedell et al. 2024).\nKSGP SSU specific tax database (Grant et al. 2023).\nR microeco package (Liu et al. 2021).\nphyloseq R package (McMurdie and Holmes 2013).",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#references",
    "href": "lotus3_ssu.html#references",
    "title": "SWELTR Depth 16S rRNA",
    "section": "",
    "text": "Bedarf, Janis R, Naiara Beraza, Hassan Khazneh, Ezgi Özkurt, David Baker, Valeri Borger, Ullrich Wüllner, and Falk Hildebrand. 2021. “Much Ado about Nothing? Off-Target Amplification Can Lead to False-Positive Bacterial Brain Microbiome Detection in Healthy and Parkinson’s Disease Individuals.” Microbiome 9 (1): 75. https://doi.org/10.1186/s40168-021-01012-1.\n\n\nBorman, Tuomas, Felix G. M. Ernst, Sudarshan A. Shetty, and Leo Lahti. 2024. Mia: Microbiome Analysis. https://doi.org/10.18129/B9.bioc.mia.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nFrøslev, Tobias Guldberg, Rasmus Kjøller, Hans Henrik Bruun, Rasmus Ejrnæs, Ane Kirstine Brunbjerg, Carlotta Pietroni, and Anders Johannes Hansen. 2017. “Algorithm for Post-Clustering Curation of DNA Amplicon Data Yields Reliable Biodiversity Estimates.” Nature Communications 8 (1): 1188. https://doi.org/10.1038/s41467-017-01312-x.\n\n\nGrant, Alastair, Abdullah Aleidan, Charli S Davies, Solomon C Udochi, Joachim Fritscher, Mohammad Bahram, and Falk Hildebrand. 2023. “Improved Taxonomic Annotation of Archaea Communities Using LotuS2, the Genome Taxonomy Database and RNAseq Data.” bioRxiv, 2023–08. https://doi.org/10.1101/2023.08.21.554127.\n\n\nHauswedell, Hannes, Sara Hetzel, Simon G Gottlieb, Helene Kretzmer, Alexander Meissner, and Knut Reinert. 2024. “Lambda3: Homology Search for Protein, Nucleotide, and Bisulfite-Converted Sequences.” Bioinformatics 40 (3): btae097. https://doi.org/10.1093/bioinformatics/btae097.\n\n\nLi, Heng. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1038/s41596-025-01239-4.\n\n\nMcMurdie, Paul J, and Susan Holmes. 2013. “Phyloseq: An r Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLoS One 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.\n\n\nÖzkurt, Ezgi, Joachim Fritscher, Nicola Soranzo, Duncan YK Ng, Robert P Davey, Mohammad Bahram, and Falk Hildebrand. 2022. “LotuS2: An Ultrafast and Highly Accurate Tool for Amplicon Sequencing Analysis.” Microbiome 10 (1): 176. https://doi.org/10.1186/s40168-022-01365-1.\n\n\nPuente-Sánchez, Fernando, Jacobo Aguirre, and Vı́ctor Parro. 2016. “A Novel Conceptual Approach to Read-Filtering in High-Throughput Amplicon Sequencing Studies.” Nucleic Acids Research 44 (4): e40–40. https://doi.org/10.1093/nar/gkv1113.\n\n\nRognes, Torbjørn, Tomáš Flouri, Ben Nichols, Christopher Quince, and Frédéric Mahé. 2016. “VSEARCH: A Versatile Open Source Tool for Metagenomics.” PeerJ 4: e2584. https://doi.org/10.7717/peerj.2584.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#required-packages",
    "href": "lotus3_ssu.html#required-packages",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Required Packages",
    "text": "Required Packages\n\nClick here for required R packages.set.seed(919191)\nlibrary(microeco)\nlibrary(mia)\nlibrary(phyloseq)\nlibrary(microbiome)\npacman::p_load(magrittr, tidyverse, janitor, \n               install = FALSE, update = FALSE)",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#lotus3-workflow",
    "href": "lotus3_ssu.html#lotus3-workflow",
    "title": "SWELTR Depth 16S rRNA",
    "section": "LotuS3 Workflow",
    "text": "LotuS3 Workflow\nThe lotus3 command requires a few user provided files and and inputs. There are numerous options for tweaking the analysis. Here is the command used to analyze the dataset.\n\nlotus3 -i . \\\n       -map ssu_miSeqMap.sm.txt \\\n       -o LOTUS3_ASV \\\n       -sdmopt sdm_miSeq.txt \\\n       -p miSeq \\\n       -amplicon_type SSU  \\\n       -forwardPrimer GTGCCAGCMGCCGCGGTAA \\\n       -reversePrimer GGACTACHVGGGTWTCTAAT \\\n       -clustering dada2 \\\n       -refDB SLV \\\n       -taxAligner lambda \\\n       -threads 20\n\nBelow you will find a more detailed explaination of each parameter and, where applicable, specific choices are presented in bold font.\n\n-i input directory (uses mapping file).\n-map mapping file  Download the mapping file for this study .\n\n\n\n\n\n\n\nWarning\n\n\n\nThe structure and content of the mapping file is very important. Please consult the LotuS documentation for more details.\n\n\n\n-o output directory\n-sdmopt SDM option file, defaults to “configs/sdm_miSeq.txt” in current dir. This file is installed with the LotuS3 package.\n\n-p sequencing platform: PacBio, PacBio_GA, 454, AVITI, miSeq or hiSeq.\n-amplicon_type &lt;SSU|LSU|ITS|ITS1|ITS2|custom&gt;\n-forwardPrimer forward primer\n-reversePrimer reverse primer\n-clustering sequence clustering algorithm: (1) UPARSE, (2) swarm, (3) cd-hit, (6) unoise3, (7) dada2, (8) VSEARCH.\n-refDB &lt;KSGP|SLV|GG2|HITdb|PR2|UNITE|beetax&gt;\n-taxAligner &lt;0|blast|lambda|utax|sintax|vsearch|usearch&gt;\n-threads number of threads to be used.\n\n\nClick here to see the output of LotuS3 pipeline\n00:00:01 LotuS 3.03\n          COMMAND\n          perl ~/miniconda3/envs/lotus3/bin/lotus3 \n          -i . -m ssu_miSeqMap.sm.txt -o LOTUS3_ASV \n          -s ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\n          -p miSeq -amplicon_type SSU \n          -forwardPrimer GTGCCAGCMGCCGCGGTAA\n          -reversePrimer GGACTACHVGGGTWTCTAAT \n          -CL dada2 -refDB KSGP\n          -taxAligner lambda -t 20\n------------ I/O configuration --------------\nInput       .\nOutput      LOTUS3_ASV\nSDM options ~/miniconda3/envs/lotus3/share/lotus3-3.03-1/configs/sdm_miSeq.txt\nTempDir     LOTUS3_ASV/tmpFiles/\nNumCores    20\n------------ Pipeline config   --------------\nSequencing platform     miseq\nAmplicon target         bacteria, SSU\nDereplication filter    -derepMin 8:1,4:2,3:3\nClustering algorithm    DADA2 -&gt; ASV's\nRead mapping to ASV     minimap2, at 0.99 %id cutoff\nASV clustering based on sequence error profiles (-dada2seed 0)\nPrecluster read merging No\nRef Chimera checking    Yes (DB=~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//rdp_gold.fa, -chim_skew 2)\ndeNovo Chimera check    Yes\nTax assignment          Lambda (-LCA_frac 0.8, -LCA_cover 0.5, -LCA_idthresh 97,95,88,83,81,78,0)\nReferenceDatabase       KSGP\nRefDB location          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\nASV phylogeny           Yes (mafft, fasttree2)\nUnclassified ASV's      Kept in matrix\n00:00:01 Reading mapping file\n          Sequence files are indicated in mapping file.\n          Switching to paired end read mode\n          Found \"SequencingRun\" column, with 1 categories (ssu_1)\n00:00:01 Demultiplexing, filtering, dereplicating input files, this\n          might take some time..\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n00:01:02 Finished primary read processing with sdm:\n          Reads processed: 4,009,780; 4,009,311 (pair 1;pair 2)\n          Accepted (High qual): 2,542,654; 2,816,397 (80; 253,740 end-trimmed)\n          Accepted (Mid qual): 15;28\n          Rejected: 1,467,242; 1,193,478\n          Dereplication block 0: 67,634 unique sequences (avg size\n          28; 1,908,091 counts)\n          For an extensive report see LOTUS3_ASV/LotuSLogS//demulti.log\n00:01:02 DADA2 ASV clustering\n          check progress at LOTUS3_ASV/LotuSLogS/LotuS_progout.log\n\n00:07:45 Found 14840 ASVs, summing to 1700760 reads (dada2)\n00:07:45 Starting backmapping of \n            - dereplicated reads\n            - low-abundant dereplicated Reads\n            - mid-quality reads\n          to ASV's using minimap2 at &gt;= 0.99 identity.\n00:08:18 Backmapping dereplicated reads via minimap2: \n          Backmapping  mid qual reads: \n          Backmapping  mid qual reads: \n00:08:18 Extending and merging pairs of ASV Seeds\n00:08:24 Found 16152 fasta seed sequences based on seed extension\n          and read merging\n00:08:24 Found contaminated 0 ASV's using minimap2 \n       (phiX.0: ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//phiX.fasta)\n00:09:55 2749 ASV's removed with LULU\n 00:09:55 Postfilter:\n          Extended logs active, contaminant and chimeric matrix will be created.\n          After filtering 12091 ASV's (2300907 reads) remaining in matrix.\n00:13:31 Assigning taxonomy against \n          ~/miniconda3/envs/lotus3/share/lotus3-3.03-1//DB//KSGP_v3.1.fasta\n          using LAMBDA3\n00:15:03 Calculating Taxonomic Abundance Tables from KSGP assignments\nCalculating higher abundance levels\nAdding 23 unclassified ASV's to output matrices\nTotal reads in matrix: 2300907\nTaxLvl  %Assigned_Reads %Assigned_ASVs\nPhylum  95  94\nClass   92  79\nOrder   88  77\nFamily  77  71\nGenus   40  60\nSpecies 17  32\n00:15:04 Building tree (fasttree) and aligning (mafft) OTUs\n 00:26:02 LotuS3 finished. Output in:\n          LOTUS3_ASV\n                    Next steps:          \n          - Phyloseq: load LOTUS3_ASV/phyloseq.Rdata directly with\n          the phyloseq package in R\n          - Phylogeny: ASV phylogentic tree available in LOTUS3_ASV/OTUphylo.nwk\n          - .biom: LOTUS3_ASV/OTU.biom contains biom formatted output\n          - Alpha diversity/rarefaction curves: rtk (available as\n          R package or in bin/rtk)\n          - LotuSLogS/ contains run statistics (useful for describing\n          data/amount of reads/quality and citations to programs used\n          - Tutorial: Visit http://lotus2.earlham.ac.uk for a numerical\n          ecology tutorial",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#load-results",
    "href": "lotus3_ssu.html#load-results",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Load Results",
    "text": "Load Results\nThe LotuS3 pipeline produces numerous output files but for our purposes there are four specific files we are interested in:\n\nphyloseq.Rdata\nOTU.fna\nOTU.txt\nhiera_BLAST.txt\n\nWe will mainly work from the phyloseq.Rdata (renamed phyloseq_ssu.Rdata) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis.\n\nload(\"files/LOTUS3/ssu/phyloseq_ssu.Rdata\")\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 12091 taxa and 41 samples ]\nsample_data() Sample Data:       [ 41 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 12091 taxa by 7 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 12091 tips and 12089 internal nodes ]\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n2\n\n\nMax. number of reads\n196476\n\n\nTotal number of reads\n2300907\n\n\nAverage number of reads\n56120\n\n\nMedian number of reads\n16754\n\n\nTotal ASVs\n12091\n\n\nNumber of singleton ASVs\n39\n\n\nAny ASVs sum to 1 or less?\nTRUE\n\n\nPercent of ASVs that are singletons\n0.323\n\n\nSparsity\n0.725\n\n\n\nIn this next part of the workflow the main goal is to create a microtable object using the R package microeco (Liu et al. 2021). The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.\nWe will also:\n\nRemove any ASVs without kingdom level classification.\n\nRevome any contaminants (chloroplast, mitochondria, etc.).\n\nRemove Negative Control (NC) samples.\n\nRemove any low-count samples.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#sample-summary",
    "href": "lotus3_ssu.html#sample-summary",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Sample Summary",
    "text": "Sample Summary\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nNew sample ID based on plot, depth, treatment, & pair\n\n\nPlot\nExperimental plot ID\n\n\nTreatment\nControl vs warming\n\n\nTemp\nWarming treatment temperature\n\n\nDepth\nSampling depth in soil\n\n\nPairing\nSample pairings\n\n\nraw_rc\nNo. of raw reads\n\n\ncutadapt_rc\nRead count after cutdapt\n\n\nfinal_rc\nRead count after LotuS3 pipeline\n\n\nno_asv\nNo. of ASVs\n\n\n\n\nSample data\n\ntmp_otu &lt;- data.frame(otu_table(physeq))\ntmp_otu_sum &lt;- data.frame(colSums(tmp_otu &gt; 0)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_otu_sum)[2] &lt;- \"no_asv\"\ntmp_read_sum &lt;- data.frame(colSums(tmp_otu)) %&gt;%\n  tibble::rownames_to_column(\"SampleID\")\nnames(tmp_read_sum)[2] &lt;- \"final_rc\"\ntmp_track_reads &lt;- read.table(\n    \"files/CUTADAPT/ssu/ssu_cutadapt_track.txt\",\n    header = TRUE, sep = \"\\t\"\n)\n\ntmp_track_reads$SampleID &lt;- stringr::str_replace_all(\n  tmp_track_reads$SampleID, \"-\", \"_\")\nnames(tmp_track_reads)[2] &lt;- \"raw_rc\"\nnames(tmp_track_reads)[3] &lt;- \"cutadapt_rc\"\n#----------------/-------------------------#\ntmp_samp &lt;- data.frame(sample_data(physeq)) %&gt;% \n  tibble::rownames_to_column(\"SampleID\")\n#----------------/-------------------------#\nsample_tab &lt;- dplyr::left_join(tmp_samp, tmp_track_reads, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_read_sum, by = \"SampleID\") %&gt;% \n  dplyr::left_join(., tmp_otu_sum, by = \"SampleID\") \n\nsample_tab &lt;- sample_tab[, -c(2, 3, 9:15)]\n\nsample_tab$per_reads_kept &lt;- round(sample_tab$final_rc/sample_tab$raw_rc, digits = 3)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(per_reads_kept, .before = no_asv)\n\n#rm(list = ls(pattern = \"tmp_\"))\n\n\n\nSample metadata including read changes at start and end of workflow.\n\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#prep-data-for-microeco",
    "href": "lotus3_ssu.html#prep-data-for-microeco",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Prep Data for microeco\n",
    "text": "Prep Data for microeco\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section, microeco tutorial.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the mock data.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))\ntmp_tax[1:6, 1:4]\n\n           Domain   Phylum      Class      Order\nASV11944   Bacteria Bacillota   Bacilli    RES148\nASV8741    Bacteria Bacillota_A Clostridia Christensenellales\nASV11355   Bacteria Bacillota_A Clostridia Christensenellales\nASV6303    Bacteria Bacillota_A Clostridia Christensenellales\nASV9863    Bacteria Bacillota_A Clostridia Christensenellales\nASV11366   Bacteria Bacillota_A Clostridia Christensenellales\nNext we need to add rank definitions to each classification.\n\ntmp_tax &lt;- data.frame(tax_table(physeq))   \ntmp_tax &lt;- tmp_tax %&gt;% dplyr::rename(\"Kingdom\" = \"Domain\")\ntmp_tax &lt;- tmp_tax %&gt;%\n  dplyr::mutate_all(~stringr::str_replace_all(., \"\\\\?\", \"\"))\n\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\ntmp_tax$Species &lt;- paste(\"s\", tmp_tax$Species, sep = \"__\")\n\nAnd now the final, modified taxonomy table.\n\ntmp_tax[1:6, 1:4]\n\n            Kingdom     Phylum          Class         Order\nASV11944      k__Bacteria   p__Bacillota      c__Bacilli      o__RES148\nASV8741     k__Bacteria p__Bacillota_A  c__Clostridia   o__Christensenellales\nASV11355      k__Bacteria   p__Bacillota_A  c__Clostridia   o__Christensenellales\nASV6303     k__Bacteria p__Bacillota_A  c__Clostridia   o__Christensenellales\nASV9863     k__Bacteria p__Bacillota_A  c__Clostridia   o__Christensenellales\nASV11366      k__Bacteria   p__Bacillota_A  c__Clostridia   o__Christensenellales\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\ntmp_otu[1:6, 1:3]\n\n          P00_D00_000_NNN P01_D00_010_W4A P01_D10_020_W4A\nASV11944                0               1               0   \nASV8741                 0               3               1   \nASV11355                0               2               0   \nASV6303                 0               5               0   \nASV9863                 0               8               1   \nASV11366                0               0               0   \n\nidentical(row.names(tmp_otu), row.names(tmp_tax))\n\n[1] TRUE\nC. Sample Table\nHere is what the sample table looks like.\n\nhead(sample_tab)\n\n                       SampleID Plot Treatment Temp  Depth Pairing raw_rc\nP00_D00_000_NNN P00_D00_000_NNN  P00  Negative    N 00_000       N    910\nP01_D00_010_W4A P01_D00_010_W4A  P01      Warm    4 00_010       A 258854\nP01_D10_020_W4A P01_D10_020_W4A  P01      Warm    4 10_020       A 203065\nP01_D20_050_W4A P01_D20_050_W4A  P01      Warm    4 20_050       A  21946\nP01_D50_100_W4A P01_D50_100_W4A  P01      Warm    4 50_100       A  14208\nP02_D00_010_C0A P02_D00_010_C0A  P02   Control    0 00_010       A 252475\n                cutadapt_rc final_rc per_reads_kept no_asv\nP00_D00_000_NNN         668      359          0.395     27\nP01_D00_010_W4A      243806   138823          0.536   7310\nP01_D10_020_W4A      191122   115924          0.571   6747\nP01_D20_050_W4A       20663    13581          0.619   1145\nP01_D50_100_W4A       13257     9358          0.659    341\nP02_D00_010_C0A      237494   143134          0.567   7425\n\n\n\nsample_tab &lt;- sample_tab %&gt;% tibble::column_to_rownames(\"SampleID\")\nsample_tab$SampleID &lt;- rownames(sample_tab)\nsample_tab &lt;- sample_tab %&gt;% dplyr::relocate(SampleID)\n\nAnd now the final, modified sample table.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#create-a-microtable-object",
    "href": "lotus3_ssu.html#create-a-microtable-object",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nNote\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- sample_tab\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_otu\n\n\nlibrary(microeco)\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12091 rows and 41 columns\ntax_table have 12091 rows and 7 columns\nAdd Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"files/LOTUS3/ssu/OTU.fna\")\nidentical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))\n\n[1] TRUE\n\ntmp_fasta_names &lt;- names(rep_fasta)\ntmp_rep_fasta &lt;- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]\nrep_fasta &lt;- tmp_rep_fasta\ntmp_me$rep_fasta &lt;- rep_fasta\nidentical(row.names(tmp_me$tax_table), names(rep_fasta))\ntmp_me$tidy_dataset()\n\n[1] TRUE\n\nme_asv_raw &lt;- microeco::clone(tmp_me)\nme_asv_raw\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12091 rows and 41 columns\ntax_table have 12091 rows and 7 columns\n\n\nLoading required namespace: Biostrings\n\n\nrep_fasta have 12091 sequences\n\n\n\nThe microeco object me_asv_raw contains all data from the LotuS3 pipeline, before any dataset curation.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#curate-the-data-set",
    "href": "lotus3_ssu.html#curate-the-data-set",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove any Kingdom NAs\nHere we can just use the straight up subset command since we do not need to worry about any ranks above Kingdom also being removed.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% \n  base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\ntmp_no_na$tidy_dataset()\n\n\nme_asv_no_na &lt;- microeco::clone(tmp_no_na)\nme_asv_no_na\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12018 rows and 41 columns\ntax_table have 12018 rows and 7 columns\nrep_fasta have 12018 sequences\n\n\n\nThe microeco object me_asv_no_na conatins only ASVs calssified as Archaea or Bacteria.\n\nRemove Contaminants\nNow we can remove any potential contaminants like mitochondria or chloroplasts.\n\ntmp_no_cont &lt;- microeco::clone(tmp_no_na)\ntmp_no_cont$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\ntmp_no_cont$tidy_dataset()\n\nTotal 17 features are removed from tax_table ...\n\nme_asv_no_cont &lt;- microeco::clone(tmp_no_cont)\nme_asv_no_cont\n\n\n\nmicrotable-class object:\nsample_table have 41 rows and 11 columns\notu_table have 12001 rows and 41 columns\ntax_table have 12001 rows and 7 columns\nrep_fasta have 12001 sequences\n\n\n\nThe microeco object me_asv_no_cont does not conatins contain ASVs calssified as mitochondria or chloroplast.\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, Treatment == \"Negative\")\ntmp_nc$tidy_dataset()\n\n11974 taxa with 0 abundance are removed from the otu_table ...\n\nme_asv_nc &lt;- microeco::clone(tmp_nc)\nme_asv_nc\n\n\n\nmicrotable-class object:\nsample_table have 1 rows and 11 columns\notu_table have 27 rows and 1 columns\ntax_table have 27 rows and 7 columns\nrep_fasta have 27 sequences\n\n\n\nThe microeco object me_asv_nc conatins only the negative control sample(s) and associated ASVs.\n\nLooks like there are 27 ASVs in the NC samples from a total of 359 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\nnc_asvs\n\n\n\n [1] \"ASV9992\"  \"ASV3670\"  \"ASV2401\"  \"ASV7644\"  \"ASV9669\"  \"ASV4853\" \n [7] \"ASV6060\"  \"ASV3886\"  \"ASV1\"     \"ASV8245\"  \"ASV2651\"  \"ASV2615\" \n[13] \"ASV4053\"  \"ASV2791\"  \"ASV4425\"  \"ASV4582\"  \"ASV8443\"  \"ASV4651\" \n[19] \"ASV7994\"  \"ASV5320\"  \"ASV5932\"  \"ASV6596\"  \"ASV4883\"  \"ASV4302\" \n[25] \"ASV1155\"  \"ASV1663\"  \"ASV10187\"\n\n\nASVs are numbered in order by total abundance in the data set so we know that many of the ASVs in the NC samples are not particularly abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% \n                 dplyr::filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  dplyr::mutate(total_reads_NC = rowSums(dplyr::select(., contains(\"NNN\"))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(\"NNN\"))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0))\ntmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\ntmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(\"NNN\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\"))\ntmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(\"NNN\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\ntmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n\n\nnc_check &lt;- tmp_rem_nc_df\n\n\n\nSummary of ASVs detected in Negative Control (NC) samples.\n\n\n\n\n Download summary of ASVs detected in Negative Control (NC) samples \nLooking at these data we can see that ASVs like ASV1 are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV6060, ASV9669, and ASV9992 are very abundant in NC samples. We decided to remove ASVs if:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  dplyr::filter(perc_in_neg &gt; 10 | perc_in_neg_samp &gt; 10)\n#tmp_rem_asv &lt;- nc_remove$ASV_ID %&gt;% \n#  unlist(strsplit(., split = \", \")) \n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n17\n295\n580\n33.714\n\n\nRetained\n10\n64\n65084\n0.098\n\n\n\nWe identified a total of 27 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 17 ASVs from the data set, which represented 295 total reads in NC samples and 580 total reads in non-NC samples. Of the total reads removed 33.714% came from NC samples. Of all ASVs identified in NC samples,10 were retained because the fell below the threshhold criteria. These ASVs accounted for 64 reads in NC samples and 65084 reads in non-NC samples. NC samples accounted for 0.098% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_cont)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  dplyr::filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 Treatment != \"Negative\")\ntmp_no_nc$tidy_dataset()\n\n\nme_asv_no_nc &lt;- microeco::clone(tmp_no_nc)\nme_asv_no_nc\n\n\n\nmicrotable-class object:\nsample_table have 40 rows and 11 columns\notu_table have 11984 rows and 40 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences\n\n\n\nThe microeco object me_asv_no_nc does not conatins the negative control sample(s).\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts—here we set the threshold to 1000 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\n\nCheck if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).\n\ntmp_row_sums &lt;- rowSums(tmp_no_low$otu_table)\nany(tmp_row_sums == 0)\n\n[1] FALSE\n\nme_asv_no_low &lt;- microeco::clone(tmp_no_low)\nme_asv_no_low\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 11 columns\notu_table have 11984 rows and 39 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences\n\n\n\nThe microeco object me_asv_no_low does not conatins the low abundance samples.\n\nGiving us the final microtable object.\n\nme_asv &lt;- microeco::clone(me_asv_no_low)\n\n\n\nmicrotable-class object:\nsample_table have 39 rows and 11 columns\notu_table have 11984 rows and 39 columns\ntax_table have 11984 rows and 7 columns\nrep_fasta have 11984 sequences",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  },
  {
    "objectID": "lotus3_ssu.html#summary",
    "href": "lotus3_ssu.html#summary",
    "title": "SWELTR Depth 16S rRNA",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Borman et al. 2024).\nFirst we do a little formatting to get our data compatible with mia.\n\n# https://github.com/microbiome/OMA/issues/202\ntmp_counts &lt;- as.matrix(me_asv_raw$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_asv_raw &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_asv_raw$sample_table),\n                                rowData = DataFrame(me_asv_raw$tax_table))\nrm(list = ls(pattern = \"tmp_\"))\ntmp_counts &lt;- as.matrix(me_asv$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_asv &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_asv$sample_table),\n                                rowData = DataFrame(me_asv$tax_table))\nmia_me_asv_raw_summ &lt;- summary(mia_me_asv_raw, assay.type = \"counts\")\nmia_me_asv_summ &lt;- summary(mia_me_asv, assay.type = \"counts\")\nrm(list = ls(pattern = \"tmp_\"))\n\n\n\n          me_dataset total_asvs total_reads total_samples\n1           original      12091     2300907            41\n2      No NA kingdom      12018     2299105            41\n3    no contaminants      12001     2297992            41\n4     no neg control      11984     2297053            40\n5 no low count samps      11984     2297051            39\n\n\n\n\n\n\n\n\n\nMetric\nStart\nEnd\n\n\n\nMin. number reads\n2\n1004\n\n\nMax. number reads\n196476\n196357\n\n\nTotal number reads\n2300907\n2297051\n\n\nAvg number reads\n56120\n58899\n\n\nMedian no. reads\n16754\n20330\n\n\nTotal ASVs\n12091\n11984\n\n\nNo. singleton ASVs\n0\n0\n\n\nAvg ASVs per sample.\n3321\n3475\n\n\n\nWe started off with 12091 ASVs and 41 samples. Screening for NA kingdom assignment removed an additional 73 ASVs. Screening for Mitochondria and Chloroplast removed 17 ASVs. After removing the negative controls there were 11984 ASVs and 40. After removing low-count samples, there were 11984 ASVs and 39 samples remaining.",
    "crumbs": [
      "A. Processing",
      "SWELTR Depth 16S rRNA"
    ]
  }
]