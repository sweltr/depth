---
title: "SWELTR Depth 16S rRNA"
subtitle: "Scripts for OTU processing using Mothur"
format:
  html: 
    code-copy: true
    code-overflow: wrap
    code-link: false
    css: styles.css
    embed-resources: true
    code-tools:
      source: true
      toggle: false
      caption: none
      
    theme: cosmo
---

```{r}
#| message: false
#| results: hide
#| echo: false
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(magrittr)
library(reactable)
library(downloadthis)
library(reactablefmtr)
library(readr)
```


::: {.callout-note appearance="simple" collapse="true"}

### Expand to see environment variables

We are running mothur in [Batch Mode](https://mothur.org/wiki/batch_mode/) and using Environment Variables to generalize the batch commands for reuse. The format of environmental variables is `[tag]=[value]`. mothur will automatically pull in the systems environment variable, which we can set in bash and then run the batch file:

```         
$ export DATA=../RAW_DATA/16S/2019/cutadapt/
$ export TYPE=fastq
$ export PROC=30
$ export REF_LOC=reference_dbs
$ export TAXREF_FASTA=gsrdb.fasta
$ export TAXREF_TAX=gsrdb.tax
$ export ALIGNREF=silva.v4.fasta

$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota
```
:::

::: {.callout-note appearance="simple" collapse="true"}
### Expand for the MOTHUR batchfile

{{< include include/_mothur_batchfile.qmd>}}
:::

```{verbatim}
set.dir(output=pipelineFiles/)
```

```
Mothur's directories:
outputDir=pipelineFiles/
```


```{verbatim}
make.file(inputdir=$DATA, type=$TYPE, prefix=ssu)
```

```
Setting input directories to: 
	../RAW_DATA/16S/2019/cutadapt/
Output File Names: 
  ssu.files
```

## Reducing Sequencing & PCR Errors

```{verbatim}
make.contigs(file=ssu.files, processors=30)
```

```
[WARNING]: group P00-D00-000-NNN contains illegal characters in the name. 
Group names should not include :, -, or / characters.  The ':' character 
is a special character used in trees. Using ':' will result in your tree 
being unreadable by tree reading software.  The '-' character is a special 
character used by mothur to parse group names.  Using the '-' character 
will prevent you from selecting groups. The '/' character will created 
unreadable filenames when mothur includes the group in an output filename.

[NOTE] Updating P00-D00-000-NNN to P00_D00_000_NNN to avoid downstream issues.

...

Total of all groups is 3774200

It took 470 secs to process 3774200 sequences.

Output File Names: 
ssu.trim.contigs.fasta
ssu.scrap.contigs.fasta
ssu.contigs_report
ssu.contigs.count_table
```


```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 200 |  200   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    4    | 94356    |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 943551   |
| Median:     |   1   | 253 |  253   |   0    |    5    | 1887101  |
| 75%-tile:   |   1   | 253 |  253   |   1    |    5    | 2830651  |
| 97.5%-tile: |   1   | 254 |  254   |   14   |    8    | 3679846  |
| Maximum:    |   1   | 479 |  479   |   77   |   213   | 3774200  |
| Mean:       |   1   | 253 |  253   |   1    |    4    |          |

```         
# of unique seqs:   3774200
total # of seqs:    3774200

It took 103 secs to summarize  sequences.

Output File Names:
ssu.trim.contigs.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 20.

Total seqs: 3774200.

Output File Names: 
ssu.contigs.count.summary
```


```{verbatim}
screen.seqs(fasta=current, count=current, maxambig=0, 
            minlength=252, maxlength=254, maxhomop=6, 
            processors=$PROC)
```

```
It took 12 secs to screen 3774200 sequences, removed 1614711.

/******************************************/
Running command: remove.seqs(accnos=ssu.trim.contigs.bad.accnos.temp, count=ssu.contigs.count_table)
Removed 1614711 sequences from ssu.contigs.count_table.

Output File Names:
ssu.contigs.pick.count_table

/******************************************/

Output File Names:
ssu.trim.contigs.good.fasta
ssu.trim.contigs.bad.accnos
ssu.contigs.good.count_table

It took 55 secs to screen 3774200 sequences.
```


```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 253 |  253   |   0    |    4    |  53988   |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 539873   |
| Median:     |   1   | 253 |  253   |   0    |    5    | 1079745  |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 1619617  |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 2105502  |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 2159489  |
| Mean:       |   1   | 253 |  253   |   0    |    4    |          |

```         
# of unique seqs:   2159489
total # of seqs:    2159489

It took 58 secs to summarize 2159489 sequences.

Output File Names:
ssu.trim.contigs.good.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 4.

Total seqs: 2159489.

Output File Names: 
ssu.contigs.good.count.summary
```

## Processing Improved Reads

```{verbatim}
unique.seqs(fasta=current, count=current)
```

```
2159489	677517

Output File Names: 
ssu.trim.contigs.good.unique.fasta
ssu.trim.contigs.good.count_table
```



```{verbatim}
summary.seqs(count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 253 |  253   |   0    |    4    |  53988   |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 539873   |
| Median:     |   1   | 253 |  253   |   0    |    5    | 1079745  |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 1619617  |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 2105502  |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 2159489  |
| Mean:       |   1   | 253 |  253   |   0    |    4    |          |

```         
# of unique seqs:   677517
total # of seqs:    2159489

It took 19 secs to summarize 2159489 sequences.

Output File Names:
ssu.trim.contigs.good.unique.summary
```
:::

## Aligning Reads



```{verbatim}
pcr.seqs(fasta=$REF_LOC/silva.nr_v132.align, 
         oligos=ssu.oligos, keepdots=F, processors=30)
```

```
It took 152 secs to screen 213119 sequences.

Output File Names: 
silva.nr_v132.pcr.align
silva.nr_v132.bad.accnos
silva.nr_v132.scrap.pcr.align
```

```{verbatim}
rename.file(input=$REF_LOC/silva.nr_v132.pcr.align, new=$REF_LOC/$ALIGNREF)
summary.seqs(fasta=$REF_LOC/$ALIGNREF, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:-------:|
| Minimum     |   2   | 21432 |  179   |   0    |    3    |    1    |
| 2.5%-tile:  | 12749 | 22331 |  252   |   0    |    3    |  4207   |
| 25%-tile:   | 12749 | 22331 |  253   |   0    |    4    |  42069  |
| Median:     | 12749 | 22331 |  253   |   0    |    5    |  84137  |
| 75%-tile:   | 12749 | 22331 |  253   |   0    |    5    | 126205  |
| 97.5%-tile: | 12749 | 22331 |  254   |   1    |    6    | 164067  |
| Maximum:    | 12762 | 40319 |  799   |   5    |   16    | 168273  |
| Mean:       | 12748 | 22331 |  253   |   0    |    4    |         |

```         
# of Seqs:  168273

It took 8 secs to summarize 168273 sequences.

Output File Names:
silva.v4.summary
```
:::

```{verbatim}
align.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)
```

```
Using 30 processors.

Reading in the silva.v4.fasta template sequences...	DONE.
It took 96 to read  168273 sequences.

Aligning sequences from ssu.trim.contigs.good.unique.fasta ...
It took 307 secs to align 677517 sequences.

[WARNING]: 1 of your sequences generated alignments that 
eliminated too many bases, a list is provided in 
ssu.trim.contigs.good.unique.flip.accnos.
[NOTE]: 1 of your sequences were reversed to produce a better alignment.

It took 308 seconds to align 677517 sequences.

Output File Names: 
ssu.trim.contigs.good.unique.align
ssu.trim.contigs.good.unique.align_report
ssu.trim.contigs.good.unique.flip.accnos
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:--------:|
| Minimum     |   2   | 5093  |  19    |   0    |    2    |    1     |
| 2.5%-tile:  | 12749 | 22331 |  253   |   0    |    4    |  53988   |
| 25%-tile:   | 12749 | 22331 |  253   |   0    |    4    | 539873   |
| Median:     | 12749 | 22331 |  253   |   0    |    4    | 1079745  |
| 75%-tile:   | 12749 | 22331 |  253   |   0    |    5    | 1619617  |
| 97.5%-tile: | 12749 | 22331 |  254   |   0    |    6    | 2105502  |
| Maximum:    | 14853 | 22332 |  254   |   0    |    6    | 2159489  |
| Mean:       | 12748 | 253   |  252   |   0    |    4    |          |

```         
# of unique seqs:   677517
total # of seqs:    2159489

It took 50 secs to summarize 2159489 sequences.

Output File Names:
ssu.trim.contigs.good.unique.summary
```
:::

```{verbatim}
screen.seqs(fasta=current, count=current, start=12749, end=22331, processors=$PROC)
```

```
It took 46 secs to screen 677517 sequences, removed 1117.

/******************************************/
Running command: 
remove.seqs(accnos=ssu.trim.contigs.good.unique.bad.accnos.temp,
count=ssu.trim.contigs.good.count_table)
Removed 1667 sequences from ssu.trim.contigs.good.count_table.

Output File Names:
ssu.trim.contigs.good.pick.count_table

/******************************************/

Output File Names:
ssu.trim.contigs.good.unique.good.align
ssu.trim.contigs.good.unique.bad.accnos
ssu.trim.contigs.good.good.count_table

It took 70 secs to screen 677517 sequences.
```


```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary


|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:--------:|
| Minimum     | 12749 | 22331 |  224   |   0    |    3    |    1     |
| 2.5%-tile:  | 12749 | 22331 |  253   |   0    |    4    |  53946   |
| 25%-tile:   | 12749 | 22331 |  253   |   0    |    4    | 539456   |
| Median:     | 12749 | 22331 |  253   |   0    |    5    | 1078912  |
| 75%-tile:   | 12749 | 22331 |  253   |   0    |    5    | 1618367  |
| 97.5%-tile: | 12749 | 22331 |  254   |   0    |    6    | 2103877  |
| Maximum:    | 12749 | 22332 |  254   |   0    |    6    | 2157822  |
| Mean:       | 12749 | 22331 |  253   |   0    |    4    |          |

```         
# of unique seqs:   676400
total # of seqs:    2157822

It took 45 secs to summarize 2157822 sequences.

Output File Names:
ssu.trim.contigs.good.unique.good.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 4.

Total seqs: 2157822.

Output File Names: 
ssu.trim.contigs.good.good.count.summary
```

```{verbatim}
filter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)
```

```
Creating Filter...
It took 13 secs to filter 676400 sequences.

Length of filtered alignment: 565
Number of columns removed: 39755
Length of the original alignment: 40320
Number of sequences used to construct filter: 676400

Output File Names: 
ssu.filter
ssu.trim.contigs.good.unique.good.filter.fasta
```

```{verbatim}
unique.seqs(fasta=current, count=current)
```

```
676400	676390

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.fasta
ssu.trim.contigs.good.unique.good.filter.count_table
```


```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 4.

Total seqs: 2157822.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.count.summary
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 565 |  224   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 565 |  253   |   0    |    4    |  53946  |
| 25%-tile:   |   1   | 565 |  253   |   0    |    4    | 539456  |
| Median:     |   1   | 565 |  253   |   0    |    5    | 1078912 |
| 75%-tile:   |   1   | 565 |  253   |   0    |    5    | 1618367 |
| 97.5%-tile: |   1   | 565 |  254   |   0    |    6    | 2103877 |
| Maximum:    |   4   | 565 |  254   |   0    |    6    | 2157822 |
| Mean:       |   1   | 565 |  253   |   0    |    4    |          |

```         
# of unique seqs:   676390
total # of seqs:    2157822

It took 24 secs to summarize 2157822 sequences.

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.summary
```
:::

## Precluster

```{verbatim}
pre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)
```

```         
/******************************************/
Splitting by sample: 

Using 30 processors.

Selecting sequences for groups ...
....

Selected 4 sequences from P02_D50_100_C0A.
Selected 188 sequences from P00_D00_000_NNN.
Selected 1417 sequences from P04_D50_100_C0B.
Selected 1850 sequences from P04_D20_050_C0B.

It took 51 seconds to split the dataset by sample.
/******************************************/

Total number of sequences before pre.cluster was 69707.
pre.cluster removed 35359 sequences.

It took 65 secs to cluster 69707 sequences.

Deconvoluting count table results...

It took 2 secs to merge 494436 sequences group data.

/******************************************/
Running get.seqs: 
Selected 392143 sequences from 
ssu.trim.contigs.good.unique.good.filter.unique.fasta.
/******************************************/
It took 221 secs to run pre.cluster.

```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 565 |  224   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 565 |  253   |   0    |    4    |  53946   |
| 25%-tile:   |   1   | 565 |  253   |   0    |    4    | 539456   |
| Median:     |   1   | 565 |  253   |   0    |    5    | 1078912  |
| 75%-tile:   |   1   | 565 |  253   |   0    |    5    | 1618367  |
| 97.5%-tile: |   1   | 565 |  254   |   0    |    6    | 2103877  |
| Maximum:    |   4   | 565 |  254   |   0    |    6    | 2157822  |
| Mean:       |   1   | 565 |  253   |   0    |    4    |          |

```         
# of unique seqs:   392143
total # of seqs:    2157822

It took 12 secs to summarize 2157822 sequences.

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.precluster.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 4.

Total seqs: 2157822.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.count.summary
```

## Remove Negative Controls

```{verbatim}

```

```

```

## Remove Chimeras

```{verbatim}
chimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)
```

```
Using vsearch version v2.15.2.
Checking sequences from ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...

/******************************************/
Splitting by sample: 

...

Removing chimeras from your input files:
/******************************************/
Running command: remove.seqs(fasta=ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, accnos=ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)
Removed 102824 sequences from ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta

/******************************************/

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta

```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 565 |  224   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 565 |  253   |   0    |    4    |  50736   |
| 25%-tile:   |   1   | 565 |  253   |   0    |    4    | 507355   |
| Median:     |   1   | 565 |  253   |   0    |    5    | 1014710  |
| 75%-tile:   |   1   | 565 |  253   |   0    |    5    | 1522065  |
| 97.5%-tile: |   1   | 565 |  254   |   0    |    6    | 1978684  |
| Maximum:    |   4   | 565 |  254   |   0    |    6    | 2029419  |
| Mean:       |   1   | 565 |  253   |   0    |    4    |          |

```         
# of unique seqs:   289267
total # of seqs:    2029419

It took 9 secs to summarize 2029419 sequences.

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 4.

Total seqs: 2029419.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary
```

## Assign Taxonomy

The `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.

::: callout-note
You can download an appropriate version of the GSR database [here](https://manichanh.vhir.org/gsrdb/download_db_links2.php).
:::

To create a mothur formatted version GSR-DB[^_merge_runs-1], we perform the following steps.

[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.

#### Download a data base

Here we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). 

```{zsh}
#| echo: true
#| eval: false
wget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz
tar -xvzf GSR-DB_V4_cluster-1.tar.gz
```

First (in the command line) we remove first line of the taxonomy file. 

```{zsh}
cp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt
sed '1d' tmp0.txt > tmp1.txt
```

Next, delete species and remove leading \[a-z\]\_\_ from taxa names

```{zsh}
sed -E 's/s__.*//g' tmp1.txt > tmp2.txt
sed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax
cp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta
```

```{verbatim}
classify.seqs(fasta=current, count=current, reference=$REF_LOC/$TAXREF_FASTA, taxonomy=$REF_LOC/$TAXREF_TAX, processors=$PROC)
```


```
Using 30 processors.
Reading template taxonomy...     DONE.
Reading template probabilities...     DONE.
It took 5 seconds get probabilities.
Classifying sequences from 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...
[WARNING]: M06508_30_000000000-CWJV7_1_1107_17148_15216 could not be classified. 
You can use the remove.lineage command with taxon=unknown; to remove 
such sequences.

...
It took 211 secs to classify 289267 sequences.

It took 8 secs to create the summary file for 289267 sequences.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary
```

## Remove Contaminants

```{verbatim}
remove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)
```

```         
Running command: 
remove.seqs(
accnos=ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos,
count=ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table,
fasta=ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)

Removed 48 sequences from 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.

Removed 84 sequences from 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.

/******************************************/

Output File Names:
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta
```

```{verbatim}
summary.tax(taxonomy=current, count=current)
```

```
Using 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table 
as input file for the count parameter.

Using 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy 
as input file for the taxonomy parameter.

It took 7 secs to create the summary file for 2029335 sequences.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary

```


```{verbatim}
count.groups(count=current)
```

```
Size of smallest group: 4.

Total seqs: 2029335.

Output File Names: 
ssu.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary
```

## Track Reads through Workflow

At this point we can look at the number of reads that made it through each step of the workflow for every sample.

```{r}
#| echo: false
#| eval: true
read_change <- read_tsv(
  "16S/OTU/all_sample_otu_read_changes.txt",
  col_names = TRUE
)
```

```{r}
#| echo: false
#| eval: true
reactable(read_change,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    screen_seqs = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    align_screen = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    remove_nc = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    nochim = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    no_contam = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_change)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Tracking read changes at each step of the mothur workflow.", 
                              font_size = 15)
```

{{< downloadthis 16S/OTU/all_sample_otu_read_changes.txt dname=all_sample_otu_read_changes label="Download read changes for mothur pipeline" icon=table type=info class=data-button id=all_sample_otu_read_changes >}}

## Preparing for analysis

```{verbatim}
rename.file(fasta=current, count=current, taxonomy=current, prefix=final)
```

```         
Current files saved by mothur:
fasta=final.fasta
taxonomy=final.taxonomy
count=final.count_table
```

## Clustering

```{verbatim}
cluster.split(fasta=final.fasta, count=final.count_table, 
              taxonomy=final.taxonomy, 
              taxlevel=4, cluster=f, processors=$PROC)
cluster.split(file=final.file, count=final.count_table, processors=$PROC)
```

```         
Splitting the file...
/******************************************/
Selecting sequences for group Proteobacteria_unclassified (1 of 277)
Number of unique sequences: 7273

Selected 24265 sequences from final.count_table.

Calculating distances for group Proteobacteria_unclassified (1 of 277):

Sequence	Time	Num_Dists_Below_Cutoff
0	0	0
2300	0	9
100	0	9
...
  ...
    ...
    
It took 4 secs to find distances for 7273 sequences. 
60765 distances below cutoff 0.03.

It took 224 seconds to cluster
Merging the clustered files...
It took 6 seconds to merge.
[WARNING]: Cannot run sens.spec analysis without a column file, skipping.
Output File Names: 
final.opti_mcc.list
```

```{verbatim}
system(mkdir cluster.split.gsrdb)
system(mv final.opti_mcc.list cluster.split.gsrdb/)
system(mv final.file cluster.split.gsrdb/)
system(mv final.dist cluster.split.gsrdb/)
```

```{verbatim}
dist.seqs(fasta=final.fasta, cutoff=0.03, processors=$PROC) 
cluster(column=final.dist, count=final.count_table)
```

```         
Sequence    Time    Num_Dists_Below_Cutoff

It took 7687 secs to find distances for 289219 sequences. 
48697445 distances below cutoff 0.03.

Output File Names: 
final.dist

You did not set a cutoff, using 0.03.

Clustering final.dist

iter	time	label	num_otus	cutoff	tp	tn	fp	fn	sensitivity	specificity	ppv	npv	fdr	accuracy	mcc	f1score

0.03
0	0	0.03	289219	0.03	0	4.1775e+10	0	4.86974e+07	0	1	0	0.998836	1	0.998836	0	0	
1	83	0.03	51356	0.03	3.96337e+07	4.17676e+10	7.33898e+06	9.06371e+06	0.813877	0.999824	0.843761	0.999783	0.843761	0.999608	0.828488	0.82855	
2	90	0.03	47348	0.03	4.00304e+07	4.17675e+10	7.50079e+06	8.66701e+06	0.822023	0.99982	0.842192	0.999793	0.842192	0.999613	0.831853	0.831986	
3	89	0.03	47187	0.03	4.005e+07	4.17675e+10	7.5084e+06	8.6474e+06	0.822426	0.99982	0.842123	0.999793	0.842123	0.999614	0.832023	0.832158	
4	90	0.03	47143	0.03	4.00577e+07	4.17675e+10	7.51272e+06	8.63974e+06	0.822583	0.99982	0.842072	0.999793	0.842072	0.999614	0.832077	0.832213	


It took 725 seconds to cluster

Output File Names: 
final.opti_mcc.list
final.opti_mcc.steps
final.opti_mcc.sensspec

```