---
title: "ITS Data Curation"
subtitle: "Workflow for curating the ASV dataset--generating microeco & phyloseq objects, removing spurious ASVs and low-count/negative control samples. "
format:
  html:
    other-links: 
      - text: Data
        href: https://doi.org/10.25573/data.16828063
---

{{< include include/code/_setup.qmd >}}

{{< include include/code/_curate.qmd >}}

# Overview

On this page we use the phyloseq output from LotuS3 to format the otu, taxonomy, and sample data tables so we can create a microeco object. Once that is complete we curate the dataset by removing possible contaminants, negative control samples, NA kingdoms, and low-count samples.  

## Data 

All you need to run this workflow is the phyloseq object generated by the LotuS3 pipeline.

::: {.callout-note appearance="default" icon=false}

### {{< fa download >}} &nbsp; phyloseq_its.Rdata

{{< downloadthis files/CURATE/phyloseq_its.Rdata dname="phyloseq_its" label="LotuS3 phyloseq data" icon="code-slash" type="link" >}}

:::

</br>

## Citable resources 

#### Packages used in this workflow:

1. R microeco package [@liu2021microeco]. 
2. phyloseq R package [@mcmurdie2013phyloseq].
3. miaverse [@felix2024mia]

</br>

## References

::: {#refs}
:::

# Workflow

## Required Packages

```{r}
#| message: false
#| results: hide
#| code-fold: true
#| code-summary: "Click here for required R packages."

set.seed(919191)
library(microeco)
library(mia)
library(phyloseq)
library(microbiome)
```

```{r}
#| echo: false
#| eval: true
#rm(list = ls())
load("page_build/its_curate.rdata")
```

## Review Results

The LotuS3 pipeline produces numerous output files  but for our purposes there are four specific files we are interested in:

1. `phyloseq.Rdata`
2. `OTU.fna`
3. `OTU.txt`
4. `hiera_BLAST.txt`

We will mainly work from the `phyloseq.Rdata` (renamed `phyloseq_its.Rdata`) since this contains the OTU Table, Sample Data, Taxonomy Table, and Phylogenetic Tree. Note: a phylogenetic tree of microbial ASVs from short reads seems less than useful. Therefore, we will exclude this from the initial analysis. 

```{r}
#| echo: true
#| eval: false
load("working_files/LOTUS3/its/phyloseq_its.Rdata")
physeq
```

```
phyloseq-class experiment-level object
otu_table()   OTU Table:         [ 12091 taxa and 41 samples ]
sample_data() Sample Data:       [ 41 samples by 14 sample variables ]
tax_table()   Taxonomy Table:    [ 12091 taxa by 7 taxonomic ranks ]
phy_tree()    Phylogenetic Tree: [ 12091 tips and 12089 internal nodes ]
```

```{bash}
#| echo: false
#| eval: true
cp working_files/LOTUS3/its/phyloseq_its.Rdata files/CURATE
```

```{r}
#| echo: false
#| eval: false
#| comment: this is for fixing two column names in the sample_data rather than rerunning the pipeline. 
library(speedyseq)
physeq <- physeq %>% 
      rename_sample_data(I7_Index = index) %>%
      rename_sample_data(I5_Index = index2) 
```

```{r}
#| echo: false
#| eval: false
min_read_ps <- min(readcount(physeq))
max_read_ps <- max(readcount(physeq))
total_reads_ps <- sum(readcount(physeq))
mean_reads_ps <- round(mean(readcount(physeq)), digits = 0)
median_reads_ps <- median(readcount(physeq))
total_asvs_ps <- ntaxa(physeq)
singleton_ps <- tryCatch(ntaxa(rare(physeq, detection = 1, 
                                    prevalence = 0)), 
                         error=function(err) NA)
singleton_ps_perc <- 
  tryCatch(round((100*(ntaxa(rare(physeq, detection = 1, prevalence = 0)) /
  ntaxa(physeq))), digits = 3), error=function(err) NA)
sparsity_ps <- 
  round(length(which(abundances(physeq) == 0))/
          length(abundances(physeq)), digits = 3)

ds_metrics <- data.frame(rbind(min_read_ps, max_read_ps, total_reads_ps, 
                               mean_reads_ps, median_reads_ps, total_asvs_ps, 
                               singleton_ps, singleton_ps_perc, sparsity_ps))
```

| Metric                              | Results                             |
|-------------------------------------|-------------------------------------|
| Min. number of reads                | `r ds_metrics[1, ]`                 |
| Max. number of reads                | `r ds_metrics[2, ]`                 |
| Total number of reads               | `r ds_metrics[3, ]`                 |
| Average number of reads             | `r ds_metrics[4, ]`                 |
| Median number of reads              | `r ds_metrics[5, ]`                 |
| Total ASVs                          | `r ds_metrics[6, ]`                 |
| Number of singleton ASVs            | `r ds_metrics[7, ]`                 |
| Any ASVs sum to 1 or less?          | `r isTRUE(ds_metrics[7, ] >= 1)`    |
| Percent of ASVs that are singletons | `r ds_metrics[8, ]`                 |
| Sparsity                            | `r ds_metrics[9, ]`                 |

In this next part of the workflow the goal is to create a *microtable object* using the R package [microeco](https://chiliubio.github.io/microeco_tutorial/) [@liu2021microeco]. The microtable will be used to store the ASV by sample data as well the taxonomic, fasta, and sample data in a single object. More on that in a moment.

We will also:

-   Remove any ASVs without kingdom level classification.\
-   Revome any contaminants (chloroplast, mitochondria, etc.).\
-   Remove Negative Control (NC) samples.\
-   Remove any low-count samples.

## Sample Data

Before we begin, let's create a summary table containing some basic sample metadata and the read count data. We want to inspect how total reads changed through the workflow. Table headers are as follows:

| Header         | Description                                             |
|----------------|---------------------------------------------------------|
| `SampleID`     | New sample ID based on plot, depth, treatment, & pair   |
| `Plot`         | Experimental plot ID                                    |
| `Treatment`    | Control vs warming                                      |
| `Temp`         | Warming treatment temperature                           |
| `Depth`        | Sampling depth in soil                                  |
| `Pairing`      | Sample pairings                                         |
| `raw_rc`       | No. of raw reads                                        |
| `cutadapt_rc`  | Read count after cutdapt                                |
| `final_rc`     | Read count after LotuS3 pipeline                        |
| `no_asv`       | No. of ASVs                                             |

<br/>

Now I do some wrangling of the sample data to format it for downstream analysis. WAhat I want to do is first add read counts from different point in the workflow. 

```{r}
#| echo: true
tmp_otu <- data.frame(otu_table(physeq))
tmp_otu_sum <- data.frame(colSums(tmp_otu > 0)) %>%
  tibble::rownames_to_column("SampleID")
names(tmp_otu_sum)[2] <- "no_asv"
tmp_read_sum <- data.frame(colSums(tmp_otu)) %>%
  tibble::rownames_to_column("SampleID")
names(tmp_read_sum)[2] <- "final_rc"
tmp_track_reads <- read.table(
    "files/CUTADAPT/its_cutadapt_track.txt",
    header = TRUE, sep = "\t"
)

tmp_track_reads$SampleID <- stringr::str_replace_all(
  tmp_track_reads$SampleID, "-", "_")
names(tmp_track_reads)[2] <- "raw_rc"
names(tmp_track_reads)[3] <- "cutadapt_rc"
```

Then I want to merge this data with the sample metadata. 

```{r}
#| echo: true
tmp_samp <- data.frame(sample_data(physeq)) %>% 
  tibble::rownames_to_column("SampleID")

sample_tab <- dplyr::left_join(tmp_samp, tmp_track_reads, by = "SampleID") %>% 
  dplyr::left_join(., tmp_read_sum, by = "SampleID") %>% 
  dplyr::left_join(., tmp_otu_sum, by = "SampleID") 
sample_tab <- sample_tab %>%
  dplyr::select(1, 4, 7, 5, 6, 8, 16, 17, 18, 
                19, 9, 14, 15, 3, 10, 11, 12, 13, 2)
```

```{r}
readr::write_delim(sample_tab, "files/CURATE/its_sample_data_full.txt",
    delim = "\t")

sample_tab_trim <- sample_tab[, -c(11:19)]
sample_tab_trim$per_reads_kept <- 
  round(sample_tab_trim$final_rc/sample_tab_trim$raw_rc, digits = 3)
sample_tab_trim <- sample_tab_trim %>% 
  dplyr::relocate(per_reads_kept, .before = no_asv)
```
</br>

```{r}
#| echo: false
#| eval: true
#| label: tbl-make-metadata-table-its
#| tbl-cap: "Sample metadata including read changes at start and end of workflow."
make_metadata_table(sample_tab_trim)
```

```{r}
#| echo: false
#| eval: false
readr::write_delim(sample_tab_trim, "files/CURATE/its_sample_data_with_rc.txt",
    delim = "\t")
```

</br>

{{< downloadthis files/CURATE/its_sample_data_with_rc.txt dname=its_sample_data_with_rc label="Download sample metadata" icon=table type=primary class=data-button id=shrimp_md >}}

## Prep Data for microeco

Like any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in this section,  [microeco](https://chiliubio.github.io/microeco_tutorial/basic-class.html#prepare-the-example-data) tutorial.

### A. Taxonomy Table

Here is what the taxonomy table looks like in the dataset.

```{r}
#| eval: false
#| echo: true
tmp_tax <- data.frame(tax_table(physeq))
tmp_tax[1:6, 1:4]

```

```         
        Domain Phylum     Class           Order
ASV1495 Fungi  Ascomycota ?               ?
ASV1740 Fungi  Ascomycota Sordariomycetes Sordariales
ASV697  Fungi  Ascomycota Sordariomycetes Sordariales
ASV1118 Fungi  Ascomycota Sordariomycetes Sordariales
ASV796  Fungi  Ascomycota Sordariomycetes Hypocreales
ASV118	Fungi  Ascomycota Sordariomycetes ?
```

Next we need to add rank definitions (e.g., `k__`, `p__`, etc.) to each classification. 

```{r}
#| echo: true
#| eval: false
tmp_tax <- data.frame(tax_table(physeq))   
tmp_tax <- tmp_tax %>% dplyr::rename("Kingdom" = "Domain")
tmp_tax <- tmp_tax %>%
  dplyr::mutate_all(~stringr::str_replace_all(., "\\?", ""))

tmp_tax$Kingdom <- paste("k", tmp_tax$Kingdom, sep = "__")
tmp_tax$Phylum <- paste("p", tmp_tax$Phylum, sep = "__")
tmp_tax$Class <- paste("c", tmp_tax$Class, sep = "__")
tmp_tax$Order <- paste("o", tmp_tax$Order, sep = "__")
tmp_tax$Family <- paste("f", tmp_tax$Family, sep = "__")
tmp_tax$Genus <- paste("g", tmp_tax$Genus, sep = "__")
tmp_tax$Species <- paste("s", tmp_tax$Species, sep = "__")
```

And now the final, modified taxonomy table.

```{r}
#| echo: true
#| eval: false
tmp_tax[1:6, 1:4]
write.table(tmp_tax[1:13, 1:4], "tmp_tax.txt", sep = " ", row.names = TRUE, quote = FALSE)

```

```
        Kingdom  Phylum        Class              Order
ASV1495 k__Fungi p__Ascomycota c__                o__
ASV1740 k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales
ASV697  k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales
ASV1118 k__Fungi p__Ascomycota c__Sordariomycetes o__Sordariales
ASV796  k__Fungi p__Ascomycota c__Sordariomycetes o__Hypocreales
ASV118	k__Fungi p__Ascomycota c__Sordariomycetes o__
```

### B. Sequence Table

Here is what the sequence table looks like in the dataset.

```{r}
#| eval: false
#| echo: true
tmp_otu[1:6, 1:3]
```

```
        P01_D00_010_W4A P01_D10_020_W4A P01_D20_050_W4A
ASV1059               0               0               0 
ASV1495               0               0               0 
ASV1740               2               0               0 
ASV697                0               0               0 
ASV1118               2               0               0 
ASV796                0               0               0 
```

```{r}
#| echo: true
#| eval: false
identical(row.names(tmp_otu), row.names(tmp_tax))
```

```
[1] TRUE
```

### C. Sample Table

Here is what the sample table looks like.

```{r}
#| eval: true
#| echo: true
sample_tab[1:6, 1:6]
```

```{r}
#| echo: true
#| eval: false
sample_tab <- sample_tab %>% tibble::column_to_rownames("SampleID")
sample_tab$SampleID <- rownames(sample_tab)
sample_tab <- sample_tab %>% dplyr::relocate(SampleID)
```

## Create a Microtable Object

With these three files in hand we are now ready to create a [microtable object](https://chiliubio.github.io/microeco_tutorial/basic-class.html).

::: callout-tip
A [microtable object](https://chiliubio.github.io/microeco_tutorial/basic-class.html) contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications). It can also contain a phylogenetic tree of ASVs as well as representative sequences of each ASV. 
:::

```{r}
#| echo: true
#| eval: false
sample_info <- sample_tab
tax_tab <- tmp_tax
otu_tab <- tmp_otu
```

```{r}
#| echo: true
#| eval: false
tmp_me <- microtable$new(sample_table = sample_info, 
                         otu_table = otu_tab, 
                         tax_table = tax_tab)
tmp_me
```

```         
microtable-class object:
sample_table have 40 rows and 19 columns
otu_table have 2168 rows and 40 columns
tax_table have 2168 rows and 7 columns
```

### D. Add Representative Sequence

We can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.

```{r}
#| echo: true
#| eval: false
rep_fasta <- Biostrings::readDNAStringSet("working_files/LOTUS3/its/OTU.fna")
identical(row.names(tmp_me$tax_table), row.names(tmp_me$otu_table))
```

```
[1] TRUE
```

```{r}
#| echo: true
#| eval: false
tmp_fasta_names <- names(rep_fasta)
tmp_rep_fasta <- rep_fasta[match(row.names(tmp_me$tax_table), tmp_fasta_names)]
rep_fasta <- tmp_rep_fasta
tmp_me$rep_fasta <- rep_fasta
identical(row.names(tmp_me$tax_table), names(rep_fasta))
tmp_me$tidy_dataset()
```

```
[1] TRUE
```

```{r}
#| echo: true
#| eval: false
me_asv_raw <- microeco::clone(tmp_me)
me_asv_raw
```

```{r}
#| echo: false
#| eval: true
me_asv_raw
```

> The microeco object `me_asv_raw` contains all data from the LotuS3 pipeline, before any dataset curation. 

## Curate the Data Set

Pretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.

### Remove any Kingdom NAs

Here we can just use the straight up `subset` command since we do not need to worry about any ranks above Kingdom also being removed.

```{r}
#| echo: true
#| eval: false
tmp_no_na <- microeco::clone(tmp_me)
tmp_no_na$tax_table %<>% 
  base::subset(Kingdom == "k__Fungi")
tmp_no_na$tidy_dataset()
```

```{r}
#| echo: true
#| eval: false
me_asv_no_na <- microeco::clone(tmp_no_na)
me_asv_no_na
```

```{r}
#| echo: false
#| eval: true
me_asv_no_na
```

> The microeco object `me_asv_no_na` conatins only ASVs calssified as Archaea or Bacteria. 

Since there no contaminants (mitochondria and/or chloroplasts) and no negative control samples in this dataset we can skip to the process of removing low-count samples.

### Remove Low-Count Samples

```{r}
#| echo: true
#| eval: true
tmp_df <- data.frame(me_asv_no_na$sample_sums())
tmp_df[order(tmp_df[, 1], decreasing = FALSE), ]
```

We can see that almost all samples have more to than 1000 reads. Two are slightly below but we can keep those and just remove samples with really low read counts--here we set the threshold to `500` reads.

```{r}
#| echo: true
#| eval: false
tmp_no_low <- microeco::clone(tmp_no_na)
tmp_no_low$otu_table <- tmp_no_na$otu_table %>%
          dplyr::select(where(~ is.numeric(.) && sum(.) >= 500))
tmp_no_low$tidy_dataset()
```

Check if any row sum (i.e., ASVs) is equal to 0 after removing the negative control sample(s).

```{r}
tmp_row_sums <- rowSums(tmp_no_low$otu_table)
any(tmp_row_sums == 0)
```

```
[1] FALSE
```

```{r}
#| echo: true
#| eval: false
me_asv_no_low <- microeco::clone(tmp_no_low)
me_asv_no_low
```

```{r}
#| echo: false
#| eval: true
me_asv_no_low
```

> The microeco object `me_asv_no_low` does not conatins the low abundance samples. 

Giving us the final microtable object.

```{r}
#| echo: true
#| eval: false
me_asv <- microeco::clone(me_asv_no_low)
```

```{r}
#| echo: false
#| eval: true
me_asv
```

# Summary

Now time to summarize the data. For this we use the R package [miaverse](https://microbiome.github.io) [@felix2024mia]. First we do a little formatting to get our data compatible with mia.

```{r}
#| echo: false
#| eval: false
identical(rownames(me_asv_raw$sample_table), colnames(me_asv_raw$otu_table))
identical(rownames(me_asv$sample_table), colnames(me_asv$otu_table))
```


```{r}
#| echo: true
#| eval: false
# https://github.com/microbiome/OMA/issues/202
tmp_counts <- as.matrix(me_asv_raw$otu_table)
tmp_assays <-  SimpleList(counts = tmp_counts)
mia_me_asv_raw <- TreeSummarizedExperiment(assays = tmp_assays,
                                colData = DataFrame(me_asv_raw$sample_table),
                                rowData = DataFrame(me_asv_raw$tax_table))
rm(list = ls(pattern = "tmp_"))
tmp_counts <- as.matrix(me_asv$otu_table)
tmp_assays <-  SimpleList(counts = tmp_counts)
mia_me_asv <- TreeSummarizedExperiment(assays = tmp_assays,
                                colData = DataFrame(me_asv$sample_table),
                                rowData = DataFrame(me_asv$tax_table))
mia_me_asv_raw_summ <- summary(mia_me_asv_raw, assay.type = "counts")
mia_me_asv_summ <- summary(mia_me_asv, assay.type = "counts")
rm(list = ls(pattern = "tmp_"))
```

```{r}
objs <- c("me_asv_raw", "me_asv_no_na", "me_asv_no_cont", 
          "me_asv_no_nc", "me_asv_no_low")

pipe_summary <- summarize_objs(objs)
print(pipe_summary)
```

Here we can see how the total number of ASVs, reads, and samples changed during the curation process. 

```{r}
#| echo: false
#| eval: true
knitr::kable(pipe_summary)
```

And some additional metrics describing the changes. 

| Metric                | Start                                      | End                                    |
|-----------------------|--------------------------------------------|----------------------------------------|
| Min. number reads     | `r mia_me_asv_raw_summ$samples[2]`         | `r mia_me_asv_summ$samples[2]`         |
| Max. number reads     | `r mia_me_asv_raw_summ$samples[3]`         | `r mia_me_asv_summ$samples[3]`         |
| Total number reads    | `r mia_me_asv_raw_summ$samples[1]`         | `r mia_me_asv_summ$samples[1]`         |
| Avg number reads      | `r round(mia_me_asv_raw_summ$samples[5])`  | `r round(mia_me_asv_summ$samples[5])`  |
| Median no. reads      | `r mia_me_asv_raw_summ$samples[4]`         | `r mia_me_asv_summ$samples[4]`         |
| Total ASVs            | `r mia_me_asv_raw_summ$features[1]`        | `r mia_me_asv_summ$features[1]`        |
| No. singleton ASVs    | `r mia_me_asv_raw_summ$features[2]`        | `r mia_me_asv_summ$features[2]`        |
| Avg ASVs per sample.  | `r round(mia_me_asv_raw_summ$features[3])` | `r round(mia_me_asv_summ$features[3])` |

We started off with `r nrow(me_asv_raw$tax_table)` ASVs and `r nrow(me_asv_raw$sample_table)` samples. Screening for `NA` kingdom assignment removed an additional `r nrow(me_asv_raw$tax_table) - nrow(me_asv_no_na$tax_table)` ASVs. After removing low-count samples, there were `r nrow(me_asv$tax_table)` ASVs and `r nrow(me_asv$sample_table)` samples remaining.

```{r}
#| echo: false
#| eval: false
#---------Starting data-------------#
tmp_start <- microeco::clone(me_asv_raw)
tmp_start_rc <- data.frame(tmp_start$sample_sums()) %>% 
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("start_rc" = 2)

tmp_start_asv <- data.frame(t(tmp_start$otu_table))
tmp_start_asv <- data.frame(rowSums(tmp_start_asv > 0)) %>%
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("start_asv" = 2)

#---------Final data-------------#
tmp_final <- microeco::clone(me_asv)
tmp_final_rc <- data.frame(tmp_final$sample_sums()) %>% 
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("final_rc" = 2)

tmp_final_asv <- data.frame(t(tmp_final$otu_table))
tmp_final_asv <- data.frame(rowSums(tmp_final_asv > 0)) %>%
  tibble::rownames_to_column("SampleID") %>% 
  dplyr::rename("final_asv" = 2)

#---------Summary data-------------#
tmp_summary <- dplyr::full_join(tmp_start_rc, tmp_start_asv) %>%
  dplyr::left_join(., tmp_final_rc) %>%
  dplyr::left_join(., tmp_final_asv)

readr::write_delim(tmp_summary, 
                   "files/CURATE/oo_final_summary.txt", 
                   delim = "\t"
)

curate_summary <- tmp_summary
removed_samples <- curate_summary$SampleID[is.na(curate_summary$final_rc)]
```

```{r}
#| message: false
#| echo: false
#| eval: true
#| label: tbl-make-final-rc-asv-table-its
#| tbl-cap: "Beginning and ending read counts & total ASVs by sample."
make_final_asv_rc_table(curate_summary)
```


</br>

We lost `r length(removed_samples)` samples after curating the dataset. 

```{r}
#| message: false
#| echo: false
#| eval: true
print(removed_samples)
```

```{r}
#| echo: false
#| eval: false
#| comment: the following code blocks are for exporting data
```

```{bash}
#| echo: false
#| eval: false
mkdir -p files/SHARE/its_curated_data/
```

```{r}
#| echo: false
#| eval: false
tmp_path <- "files/SHARE/its_curated_data/"
tmp_otu_n <- "its_otu_table.txt"
tmp_tax_n <- "its_tax_table.txt"
tmp_sam_n <- "its_sample_table.txt"
tmp_rep_n <- "its_rep_fasta.fasta"
tmp_me_n <- "its_me_asv.rds"
tmp_ps_n <- "its_ps_asv.rds"
```

```{r}
#| echo: false
#| eval: false
library(seqinr)
tmp_otu <- me_asv$otu_table %>% tibble::rownames_to_column("ASV_ID")
write_delim(tmp_otu, paste(tmp_path, tmp_otu_n), delim = "\t")

tmp_tax <- me_asv$tax_table %>% tibble::rownames_to_column("ASV_ID")
write_delim(tmp_tax, paste(tmp_path, tmp_tax_n), delim = "\t")

write_delim(sample_tab, paste(tmp_path, tmp_sam_n), delim = "\t")

write.fasta(
  sequences = as.list(as.character(me_asv$rep_fasta)),
  names = names(me_asv$rep_fasta),
  file.out = paste(tmp_path, tmp_rep_n)
)
saveRDS(me_asv, paste(tmp_path, tmp_me_n))
ps_asv <- file2meco::meco2phyloseq(me_asv)
saveRDS(ps_asv, paste(tmp_path, tmp_ps_n))
```

```{bash}
#| echo: false
#| eval: false
cd files/SHARE
zip -r its_curated_data.zip its_curated_data/
rm -r its_curated_data/
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(ds_metrics, sample_tab, sample_info, tax_tab, otu_tab, 
            me_asv_raw, me_asv_no_na, sample_tab_trim, 
            me_asv_no_low, me_asv, mia_me_asv_raw, ps_asv,
            mia_me_asv, mia_me_asv_raw_summ, mia_me_asv_summ, pipe_summary,
            curate_summary, removed_samples, 
            sure = TRUE)
rm(list = ls(pattern = "tmp_"))
save.image("page_build/its_curate.rdata")
saveRDS(me_asv, "files/CURATE/its_me_asv.rds")
saveRDS(ps_asv, "files/CURATE/its_ps_asv.rds")
objects()
```


{{< include include/code/_footer.qmd >}}
